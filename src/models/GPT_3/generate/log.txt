NCCL_DEBUG=info NCCL_IB_DISABLE=0 NCCL_SOCKET_IFNAME=bond0 NCCL_IB_GID_INDEX=3 NCCL_NET_GDR_LEVEL=2 deepspeed --num_nodes 2 --num_gpus 8 --hostfile /mnt/config/hostfile pretrain_gpt2.py --model-parallel-size 1 --num-layers 40 --hidden-size 1408 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --save checkpoints --train-iters 50000 --resume-dataloader --train-data wikipedia --lazy-loader --tokenizer-type GPT2BPETokenizer --split 949,50,1 --distributed-backend nccl --no-load-optim --lr-decay-style cosine --warmup .01 --checkpoint-activations --deepspeed-activation-checkpointing --fp16 --deepspeed --deepspeed_config /mnt/Megatron-LM/scripts/ds_config.json
[2020-11-11 06:41:49,777] [INFO] [runner.py:285:main] Using IP address of 10.16.0.14 for node Tencent01
[2020-11-11 06:41:49,778] [INFO] [multinode_runner.py:51:get_cmd] Running on the following workers: Tencent01,Tencent02
[2020-11-11 06:41:49,778] [INFO] [runner.py:355:main] cmd = pdsh -f 1024 -w Tencent01,Tencent02 export NCCL_DEBUG=info; export NCCL_IB_GID_INDEX=3; export NCCL_NET_GDR_LEVEL=2; export NCCL_SOCKET_IFNAME=bond0; export NCCL_IB_DISABLE=0; export NCCL_INCLUDE_DIR=/usr/include; export NCCL_VERSION=2.6.4; export NCCL_LIBRARY=/usr/lib/x86_64-linux-gnu; export PYTHONPATH=/mnt/Megatron-LM;  cd /mnt/Megatron-LM; /opt/conda/bin/python -u -m deepspeed.launcher.launch --world_info=eyJUZW5jZW50MDEiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN10sICJUZW5jZW50MDIiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --node_rank=%n --master_addr=10.16.0.14 --master_port=29500 pretrain_gpt2.py --model-parallel-size '1' --num-layers '40' --hidden-size '1408' --num-attention-heads '16' --seq-length '1024' --max-position-embeddings '1024' --save 'checkpoints' --train-iters '50000' --resume-dataloader --train-data 'wikipedia' --lazy-loader --tokenizer-type 'GPT2BPETokenizer' --split '949,50,1' --distributed-backend 'nccl' --no-load-optim --lr-decay-style 'cosine' --warmup '.01' --checkpoint-activations --deepspeed-activation-checkpointing --fp16 --deepspeed --deepspeed_config '/mnt/Megatron-LM/scripts/ds_config.json'
Tencent01: [2020-11-11 06:41:51,235] [INFO] [launch.py:71:main] 0 NCCL_INCLUDE_DIR /usr/include
Tencent01: [2020-11-11 06:41:51,235] [INFO] [launch.py:71:main] 0 NCCL_IB_GID_INDEX 3
Tencent01: [2020-11-11 06:41:51,235] [INFO] [launch.py:71:main] 0 NCCL_IB_DISABLE 0
Tencent01: [2020-11-11 06:41:51,235] [INFO] [launch.py:71:main] 0 NCCL_VERSION 2.6.4
Tencent01: [2020-11-11 06:41:51,235] [INFO] [launch.py:71:main] 0 NCCL_NET_GDR_LEVEL 2
Tencent01: [2020-11-11 06:41:51,235] [INFO] [launch.py:71:main] 0 NCCL_LIBRARY /usr/lib/x86_64-linux-gnu
Tencent01: [2020-11-11 06:41:51,235] [INFO] [launch.py:71:main] 0 NCCL_DEBUG info
Tencent01: [2020-11-11 06:41:51,235] [INFO] [launch.py:71:main] 0 NCCL_SOCKET_IFNAME bond0
Tencent01: [2020-11-11 06:41:51,235] [INFO] [launch.py:78:main] WORLD INFO DICT: {'Tencent01': [0, 1, 2, 3, 4, 5, 6, 7], 'Tencent02': [0, 1, 2, 3, 4, 5, 6, 7]}
Tencent01: [2020-11-11 06:41:51,235] [INFO] [launch.py:87:main] nnodes=2, num_local_procs=8, node_rank=0
Tencent01: [2020-11-11 06:41:51,235] [INFO] [launch.py:99:main] global_rank_mapping=defaultdict(<class 'list'>, {'Tencent01': [0, 1, 2, 3, 4, 5, 6, 7], 'Tencent02': [8, 9, 10, 11, 12, 13, 14, 15]})
Tencent01: [2020-11-11 06:41:51,235] [INFO] [launch.py:100:main] dist_world_size=16
Tencent01: [2020-11-11 06:41:51,235] [INFO] [launch.py:103:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
Tencent02: [2020-11-11 06:41:51,315] [INFO] [launch.py:71:main] 1 NCCL_INCLUDE_DIR /usr/include
Tencent02: [2020-11-11 06:41:51,315] [INFO] [launch.py:71:main] 1 NCCL_IB_GID_INDEX 3
Tencent02: [2020-11-11 06:41:51,315] [INFO] [launch.py:71:main] 1 NCCL_IB_DISABLE 0
Tencent02: [2020-11-11 06:41:51,315] [INFO] [launch.py:71:main] 1 NCCL_VERSION 2.6.4
Tencent02: [2020-11-11 06:41:51,315] [INFO] [launch.py:71:main] 1 NCCL_NET_GDR_LEVEL 2
Tencent02: [2020-11-11 06:41:51,315] [INFO] [launch.py:71:main] 1 NCCL_LIBRARY /usr/lib/x86_64-linux-gnu
Tencent02: [2020-11-11 06:41:51,315] [INFO] [launch.py:71:main] 1 NCCL_DEBUG info
Tencent02: [2020-11-11 06:41:51,315] [INFO] [launch.py:71:main] 1 NCCL_SOCKET_IFNAME bond0
Tencent02: [2020-11-11 06:41:51,315] [INFO] [launch.py:78:main] WORLD INFO DICT: {'Tencent01': [0, 1, 2, 3, 4, 5, 6, 7], 'Tencent02': [0, 1, 2, 3, 4, 5, 6, 7]}
Tencent02: [2020-11-11 06:41:51,315] [INFO] [launch.py:87:main] nnodes=2, num_local_procs=8, node_rank=1
Tencent02: [2020-11-11 06:41:51,315] [INFO] [launch.py:99:main] global_rank_mapping=defaultdict(<class 'list'>, {'Tencent01': [0, 1, 2, 3, 4, 5, 6, 7], 'Tencent02': [8, 9, 10, 11, 12, 13, 14, 15]})
Tencent02: [2020-11-11 06:41:51,315] [INFO] [launch.py:100:main] dist_world_size=16
Tencent02: [2020-11-11 06:41:51,315] [INFO] [launch.py:103:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
Tencent01: using world size: 16 and model-parallel size: 1 
Tencent01:  > using dynamic loss scaling
Tencent01: [2020-11-11 06:41:54,605] [INFO] [checkpointing.py:629:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
Tencent01: [2020-11-11 06:41:54,605] [INFO] [checkpointing.py:629:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
Tencent01: [2020-11-11 06:41:54,605] [INFO] [checkpointing.py:629:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
Tencent01: [2020-11-11 06:41:54,605] [INFO] [checkpointing.py:629:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
Tencent01: [2020-11-11 06:41:54,606] [INFO] [checkpointing.py:629:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
Tencent01: [2020-11-11 06:41:54,607] [INFO] [checkpointing.py:629:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
Tencent01: [2020-11-11 06:41:54,607] [INFO] [checkpointing.py:629:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
Tencent02: [2020-11-11 06:41:55,418] [INFO] [checkpointing.py:629:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
Tencent02: [2020-11-11 06:41:55,419] [INFO] [checkpointing.py:629:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
Tencent02: [2020-11-11 06:41:55,419] [INFO] [checkpointing.py:629:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
Tencent02: [2020-11-11 06:41:55,419] [INFO] [checkpointing.py:629:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
Tencent02: [2020-11-11 06:41:55,419] [INFO] [checkpointing.py:629:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
Tencent02: [2020-11-11 06:41:55,439] [INFO] [checkpointing.py:629:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
Tencent02: [2020-11-11 06:41:55,442] [INFO] [checkpointing.py:629:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
Tencent02: [2020-11-11 06:41:55,458] [INFO] [checkpointing.py:629:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
Tencent01: > initializing model parallel with size 1
Tencent01: [2020-11-11 06:41:55,460] [INFO] [checkpointing.py:629:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
Tencent01: Pretrain GPT2 model
Tencent01: arguments:
Tencent01:   transformer_xl ............... False
Tencent01:   pretrained_bert .............. False
Tencent01:   attention_dropout ............ 0.1
Tencent01:   num_attention_heads .......... 16
Tencent01:   hidden_size .................. 1408
Tencent01:   intermediate_size ............ None
Tencent01:   num_layers ................... 40
Tencent01:   layernorm_epsilon ............ 1e-05
Tencent01:   hidden_dropout ............... 0.1
Tencent01:   max_position_embeddings ...... 1024
Tencent01:   vocab_size ................... 30522
Tencent01:   deep_init .................... False
Tencent01:   make_vocab_size_divisible_by . 128
Tencent01:   cpu_optimizer ................ False
Tencent01:   cpu_torch_adam ............... False
Tencent01:   fp16 ......................... True
Tencent01:   fp32_embedding ............... False
Tencent01:   fp32_layernorm ............... False
Tencent01:   fp32_tokentypes .............. False
Tencent01:   fp32_allreduce ............... False
Tencent01:   hysteresis ................... 2
Tencent01:   loss_scale ................... None
Tencent01:   loss_scale_window ............ 1000
Tencent01:   min_scale .................... 1
Tencent01:   experiment_name .............. gpt-345M11-11-06-41
Tencent01:   batch_size ................... 8
Tencent01:   weight_decay ................. 0.01
Tencent01:   checkpoint_activations ....... True
Tencent01:   checkpoint_num_layers ........ 1
Tencent01:   deepspeed_activation_checkpointing  True
Tencent01:   clip_grad .................... 1.0
Tencent01:   train_iters .................. 50000
Tencent01:   log_interval ................. 100
Tencent01:   exit_interval ................ None
Tencent01:   summary_dir .................. 
Tencent01:   seed ......................... 1234
Tencent01:   reset_position_ids ........... False
Tencent01:   reset_attention_mask ......... False
Tencent01:   lr_decay_iters ............... None
Tencent01:   lr_decay_style ............... cosine
Tencent01:   lr ........................... 0.00015
Tencent01:   warmup ....................... 0.01
Tencent01:   save ......................... checkpoints/gpt-345M11-11-06-41
Tencent01:   save_interval ................ 5000
Tencent01:   no_save_optim ................ False
Tencent01:   no_save_rng .................. False
Tencent01:   load ......................... None
Tencent01:   no_load_optim ................ True
Tencent01:   no_load_rng .................. False
Tencent01:   finetune ..................... False
Tencent01:   resume_dataloader ............ True
Tencent01:   distributed_backend .......... nccl
Tencent01:   local_rank ................... 0
Tencent01:   eval_batch_size .............. None
Tencent01:   eval_iters ................... 100
Tencent01:   eval_interval ................ 1000
Tencent01:   eval_seq_length .............. None
Tencent01:   eval_max_preds_per_seq ....... None
Tencent01:   overlapping_eval ............. 32
Tencent01:   cloze_eval ................... False
Tencent01:   eval_hf ...................... False
Tencent01:   load_openai .................. False
Tencent01:   temperature .................. 1.0
Tencent01:   top_p ........................ 0.0
Tencent01:   top_k ........................ 0
Tencent01:   out_seq_length ............... 256
Tencent01:   model_parallel_size .......... 1
Tencent01:   shuffle ...................... False
Tencent01:   train_data ................... ['wikipedia']
Tencent01:   use_npy_data_loader .......... False
Tencent01:   train_data_path .............. 
Tencent01:   val_data_path ................ 
Tencent01:   test_data_path ............... 
Tencent01:   input_data_sizes_file ........ sizes.txt
Tencent01:   delim ........................ ,
Tencent01:   text_key ..................... sentence
Tencent01:   eval_text_key ................ None
Tencent01:   valid_data ................... None
Tencent01:   split ........................ 949,50,1
Tencent01:   test_data .................... None
Tencent01:   lazy_loader .................. True
Tencent01:   loose_json ................... False
Tencent01:   presplit_sentences ........... False
Tencent01:   num_workers .................. 2
Tencent01:   tokenizer_model_type ......... bert-large-uncased
Tencent01:   tokenizer_path ............... tokenizer.model
Tencent01:   tokenizer_type ............... GPT2BPETokenizer
Tencent01:   cache_dir .................... None
Tencent01:   use_tfrecords ................ False
Tencent01:   seq_length ................... 1024
Tencent01:   mem_length ................... 512
Tencent01:   max_preds_per_seq ............ None
Tencent01:   sample_one_document .......... False
Tencent01:   deepspeed .................... True
Tencent01:   deepspeed_config ............. /mnt/Megatron-LM/scripts/ds_config.json
Tencent01:   deepscale .................... False
Tencent01:   deepscale_config ............. None
Tencent01:   deepspeed_mpi ................ False
Tencent01:   cuda ......................... True
Tencent01:   rank ......................... 0
Tencent01:   world_size ................... 16
Tencent01:   dynamic_loss_scale ........... True
Tencent01: [2020-11-11 06:41:55,462] [INFO] [checkpointing.py:256:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
Tencent01: configuring data
Tencent01: > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Tencent01: > found end-of-document token: 50256
Tencent01: VM-0-14-centos:108:108 [2] NCCL INFO Bootstrap : Using [0]bond0:9.216.102.34<0>
Tencent01: VM-0-14-centos:108:108 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
Tencent01: VM-0-14-centos:108:108 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
Tencent01: VM-0-14-centos:108:108 [2] NCCL INFO NET/IB : Using [0]mlx5_bond_0:1/RoCE ; OOB bond0:9.216.102.34<0>
Tencent01: VM-0-14-centos:108:108 [2] NCCL INFO Using network IB
Tencent01: NCCL version 2.6.4+cuda10.0
Tencent01: VM-0-14-centos:112:112 [6] NCCL INFO Bootstrap : Using [0]bond0:9.216.102.34<0>
Tencent01: VM-0-14-centos:112:112 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
Tencent01: VM-0-14-centos:112:112 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
Tencent01: VM-0-14-centos:109:109 [3] NCCL INFO Bootstrap : Using [0]bond0:9.216.102.34<0>
Tencent01: VM-0-14-centos:109:109 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
Tencent01: VM-0-14-centos:109:109 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
Tencent01: VM-0-14-centos:107:107 [1] NCCL INFO Bootstrap : Using [0]bond0:9.216.102.34<0>
Tencent01: VM-0-14-centos:107:107 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
Tencent01: VM-0-14-centos:107:107 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
Tencent01: VM-0-14-centos:109:109 [3] NCCL INFO NET/IB : Using [0]mlx5_bond_0:1/RoCE ; OOB bond0:9.216.102.34<0>
Tencent01: VM-0-14-centos:109:109 [3] NCCL INFO Using network IB
Tencent01: NCCL version 2.6.4+cuda10.0
Tencent01: VM-0-14-centos:112:112 [6] NCCL INFO NET/IB : Using [0]mlx5_bond_0:1/RoCE ; OOB bond0:9.216.102.34<0>
Tencent01: VM-0-14-centos:112:112 [6] NCCL INFO Using network IB
Tencent01: NCCL version 2.6.4+cuda10.0
Tencent01: VM-0-14-centos:107:107 [1] NCCL INFO NET/IB : Using [0]mlx5_bond_0:1/RoCE ; OOB bond0:9.216.102.34<0>
Tencent01: VM-0-14-centos:107:107 [1] NCCL INFO Using network IB
Tencent01: NCCL version 2.6.4+cuda10.0
Tencent01: VM-0-14-centos:110:110 [4] NCCL INFO Bootstrap : Using [0]bond0:9.216.102.34<0>
Tencent01: VM-0-14-centos:110:110 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
Tencent01: VM-0-14-centos:110:110 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
Tencent01: VM-0-14-centos:110:110 [4] NCCL INFO NET/IB : Using [0]mlx5_bond_0:1/RoCE ; OOB bond0:9.216.102.34<0>
Tencent01: VM-0-14-centos:110:110 [4] NCCL INFO Using network IB
Tencent01: NCCL version 2.6.4+cuda10.0
Tencent01: VM-0-14-centos:106:106 [0] NCCL INFO Bootstrap : Using [0]bond0:9.216.102.34<0>
Tencent01: VM-0-14-centos:106:106 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
Tencent01: VM-0-14-centos:106:106 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
Tencent01: VM-0-14-centos:106:106 [0] NCCL INFO NET/IB : Using [0]mlx5_bond_0:1/RoCE ; OOB bond0:9.216.102.34<0>
Tencent01: VM-0-14-centos:106:106 [0] NCCL INFO Using network IB
Tencent01: NCCL version 2.6.4+cuda10.0
Tencent01: VM-0-14-centos:113:113 [7] NCCL INFO Bootstrap : Using [0]bond0:9.216.102.34<0>
Tencent01: VM-0-14-centos:113:113 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
Tencent01: VM-0-14-centos:113:113 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
Tencent01: VM-0-14-centos:113:113 [7] NCCL INFO NET/IB : Using [0]mlx5_bond_0:1/RoCE ; OOB bond0:9.216.102.34<0>
Tencent01: VM-0-14-centos:113:113 [7] NCCL INFO Using network IB
Tencent01: NCCL version 2.6.4+cuda10.0
Tencent01: VM-0-14-centos:108:184 [2] NCCL INFO Channel 00/32 :    0
Tencent01: VM-0-14-centos:108:184 [2] NCCL INFO Channel 01/32 :    0
Tencent01: VM-0-14-centos:108:184 [2] NCCL INFO Channel 02/32 :    0
Tencent01: VM-0-14-centos:108:184 [2] NCCL INFO Channel 03/32 :    0
Tencent01: VM-0-14-centos:108:184 [2] NCCL INFO Channel 04/32 :    0
Tencent01: VM-0-14-centos:108:184 [2] NCCL INFO Channel 05/32 :    0
Tencent01: VM-0-14-centos:108:184 [2] NCCL INFO Channel 06/32 :    0
Tencent01: VM-0-14-centos:108:184 [2] NCCL INFO Channel 07/32 :    0
Tencent01: VM-0-14-centos:108:184 [2] NCCL INFO Channel 08/32 :    0
Tencent01: VM-0-14-centos:108:184 [2] NCCL INFO Channel 09/32 :    0
Tencent01: VM-0-14-centos:108:184 [2] NCCL INFO Channel 10/32 :    0
Tencent01: VM-0-14-centos:108:184 [2] NCCL INFO Channel 11/32 :    0
Tencent01: VM-0-14-centos:108:184 [2] NCCL INFO Channel 12/32 :    0
Tencent01: VM-0-14-centos:108:184 [2] NCCL INFO Channel 13/32 :    0
Tencent01: VM-0-14-centos:108:184 [2] NCCL INFO Channel 14/32 :    0
Tencent01: VM-0-14-centos:108:184 [2] NCCL INFO Channel 15/32 :    0
Tencent01: VM-0-14-centos:108:184 [2] NCCL INFO Channel 16/32 :    0
Tencent01: VM-0-14-centos:108:184 [2] NCCL INFO Channel 17/32 :    0
Tencent01: VM-0-14-centos:108:184 [2] NCCL INFO Channel 18/32 :    0
Tencent01: VM-0-14-centos:108:184 [2] NCCL INFO Channel 19/32 :    0
Tencent01: VM-0-14-centos:108:184 [2] NCCL INFO Channel 20/32 :    0
Tencent01: VM-0-14-centos:108:184 [2] NCCL INFO Channel 21/32 :    0
Tencent01: VM-0-14-centos:108:184 [2] NCCL INFO Channel 22/32 :    0
Tencent01: VM-0-14-centos:108:184 [2] NCCL INFO Channel 23/32 :    0
Tencent01: VM-0-14-centos:108:184 [2] NCCL INFO Channel 24/32 :    0
Tencent01: VM-0-14-centos:108:184 [2] NCCL INFO Channel 25/32 :    0
Tencent01: VM-0-14-centos:108:184 [2] NCCL INFO Channel 26/32 :    0
Tencent01: VM-0-14-centos:108:184 [2] NCCL INFO Channel 27/32 :    0
Tencent01: VM-0-14-centos:108:184 [2] NCCL INFO Channel 28/32 :    0
Tencent01: VM-0-14-centos:108:184 [2] NCCL INFO Channel 29/32 :    0
Tencent01: VM-0-14-centos:108:184 [2] NCCL INFO Channel 30/32 :    0
Tencent01: VM-0-14-centos:108:184 [2] NCCL INFO Channel 31/32 :    0
Tencent01: VM-0-14-centos:108:184 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0->
Tencent01: VM-0-14-centos:108:184 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff
Tencent01: VM-0-14-centos:108:184 [2] NCCL INFO comm 0x7f68a8001020 rank 0 nranks 1 cudaDev 2 busId 3d000 - Init COMPLETE
Tencent01: VM-0-14-centos:107:188 [1] NCCL INFO Channel 00/32 :    0
Tencent01: VM-0-14-centos:107:188 [1] NCCL INFO Channel 01/32 :    0
Tencent01: VM-0-14-centos:107:188 [1] NCCL INFO Channel 02/32 :    0
Tencent01: VM-0-14-centos:107:188 [1] NCCL INFO Channel 03/32 :    0
Tencent01: VM-0-14-centos:107:188 [1] NCCL INFO Channel 04/32 :    0
Tencent01: VM-0-14-centos:107:188 [1] NCCL INFO Channel 05/32 :    0
Tencent01: VM-0-14-centos:107:188 [1] NCCL INFO Channel 06/32 :    0
Tencent01: VM-0-14-centos:107:188 [1] NCCL INFO Channel 07/32 :    0
Tencent01: VM-0-14-centos:107:188 [1] NCCL INFO Channel 08/32 :    0
Tencent01: VM-0-14-centos:107:188 [1] NCCL INFO Channel 09/32 :    0
Tencent01: VM-0-14-centos:107:188 [1] NCCL INFO Channel 10/32 :    0
Tencent01: VM-0-14-centos:107:188 [1] NCCL INFO Channel 11/32 :    0
Tencent01: VM-0-14-centos:107:188 [1] NCCL INFO Channel 12/32 :    0
Tencent01: VM-0-14-centos:107:188 [1] NCCL INFO Channel 13/32 :    0
Tencent01: VM-0-14-centos:107:188 [1] NCCL INFO Channel 14/32 :    0
Tencent01: VM-0-14-centos:107:188 [1] NCCL INFO Channel 15/32 :    0
Tencent01: VM-0-14-centos:107:188 [1] NCCL INFO Channel 16/32 :    0
Tencent01: VM-0-14-centos:107:188 [1] NCCL INFO Channel 17/32 :    0
Tencent01: VM-0-14-centos:107:188 [1] NCCL INFO Channel 18/32 :    0
Tencent01: VM-0-14-centos:107:188 [1] NCCL INFO Channel 19/32 :    0
Tencent01: VM-0-14-centos:107:188 [1] NCCL INFO Channel 20/32 :    0
Tencent01: VM-0-14-centos:107:188 [1] NCCL INFO Channel 21/32 :    0
Tencent01: VM-0-14-centos:107:188 [1] NCCL INFO Channel 22/32 :    0
Tencent01: VM-0-14-centos:107:188 [1] NCCL INFO Channel 23/32 :    0
Tencent01: VM-0-14-centos:107:188 [1] NCCL INFO Channel 24/32 :    0
Tencent01: VM-0-14-centos:107:188 [1] NCCL INFO Channel 25/32 :    0
Tencent01: VM-0-14-centos:107:188 [1] NCCL INFO Channel 26/32 :    0
Tencent01: VM-0-14-centos:107:188 [1] NCCL INFO Channel 27/32 :    0
Tencent01: VM-0-14-centos:107:188 [1] NCCL INFO Channel 28/32 :    0
Tencent01: VM-0-14-centos:107:188 [1] NCCL INFO Channel 29/32 :    0
Tencent01: VM-0-14-centos:107:188 [1] NCCL INFO Channel 30/32 :    0
Tencent01: VM-0-14-centos:107:188 [1] NCCL INFO Channel 31/32 :    0
Tencent01: VM-0-14-centos:107:188 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0->
Tencent01: VM-0-14-centos:107:188 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff
Tencent01: VM-0-14-centos:107:188 [1] NCCL INFO comm 0x7f2d84001020 rank 0 nranks 1 cudaDev 1 busId 1b000 - Init COMPLETE
Tencent01: VM-0-14-centos:109:187 [3] NCCL INFO Channel 00/32 :    0
Tencent01: VM-0-14-centos:109:187 [3] NCCL INFO Channel 01/32 :    0
Tencent01: VM-0-14-centos:109:187 [3] NCCL INFO Channel 02/32 :    0
Tencent01: VM-0-14-centos:109:187 [3] NCCL INFO Channel 03/32 :    0
Tencent01: VM-0-14-centos:109:187 [3] NCCL INFO Channel 04/32 :    0
Tencent01: VM-0-14-centos:109:187 [3] NCCL INFO Channel 05/32 :    0
Tencent01: VM-0-14-centos:109:187 [3] NCCL INFO Channel 06/32 :    0
Tencent01: VM-0-14-centos:109:187 [3] NCCL INFO Channel 07/32 :    0
Tencent01: VM-0-14-centos:109:187 [3] NCCL INFO Channel 08/32 :    0
Tencent01: VM-0-14-centos:109:187 [3] NCCL INFO Channel 09/32 :    0
Tencent01: VM-0-14-centos:109:187 [3] NCCL INFO Channel 10/32 :    0
Tencent01: VM-0-14-centos:109:187 [3] NCCL INFO Channel 11/32 :    0
Tencent01: VM-0-14-centos:109:187 [3] NCCL INFO Channel 12/32 :    0
Tencent01: VM-0-14-centos:109:187 [3] NCCL INFO Channel 13/32 :    0
Tencent01: VM-0-14-centos:109:187 [3] NCCL INFO Channel 14/32 :    0
Tencent01: VM-0-14-centos:109:187 [3] NCCL INFO Channel 15/32 :    0
Tencent01: VM-0-14-centos:109:187 [3] NCCL INFO Channel 16/32 :    0
Tencent01: VM-0-14-centos:109:187 [3] NCCL INFO Channel 17/32 :    0
Tencent01: VM-0-14-centos:109:187 [3] NCCL INFO Channel 18/32 :    0
Tencent01: VM-0-14-centos:109:187 [3] NCCL INFO Channel 19/32 :    0
Tencent01: VM-0-14-centos:109:187 [3] NCCL INFO Channel 20/32 :    0
Tencent01: VM-0-14-centos:109:187 [3] NCCL INFO Channel 21/32 :    0
Tencent01: VM-0-14-centos:109:187 [3] NCCL INFO Channel 22/32 :    0
Tencent01: VM-0-14-centos:109:187 [3] NCCL INFO Channel 23/32 :    0
Tencent01: VM-0-14-centos:109:187 [3] NCCL INFO Channel 24/32 :    0
Tencent01: VM-0-14-centos:109:187 [3] NCCL INFO Channel 25/32 :    0
Tencent01: VM-0-14-centos:109:187 [3] NCCL INFO Channel 26/32 :    0
Tencent01: VM-0-14-centos:109:187 [3] NCCL INFO Channel 27/32 :    0
Tencent01: VM-0-14-centos:109:187 [3] NCCL INFO Channel 28/32 :    0
Tencent01: VM-0-14-centos:109:187 [3] NCCL INFO Channel 29/32 :    0
Tencent01: VM-0-14-centos:109:187 [3] NCCL INFO Channel 30/32 :    0
Tencent01: VM-0-14-centos:109:187 [3] NCCL INFO Channel 31/32 :    0
Tencent01: VM-0-14-centos:109:187 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0->
Tencent01: VM-0-14-centos:109:187 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff
Tencent01: VM-0-14-centos:109:187 [3] NCCL INFO comm 0x7f88e0001020 rank 0 nranks 1 cudaDev 3 busId 3e000 - Init COMPLETE
Tencent01: VM-0-14-centos:112:189 [6] NCCL INFO Channel 00/32 :    0
Tencent01: VM-0-14-centos:112:189 [6] NCCL INFO Channel 01/32 :    0
Tencent01: VM-0-14-centos:112:189 [6] NCCL INFO Channel 02/32 :    0
Tencent01: VM-0-14-centos:112:189 [6] NCCL INFO Channel 03/32 :    0
Tencent01: VM-0-14-centos:112:189 [6] NCCL INFO Channel 04/32 :    0
Tencent01: VM-0-14-centos:112:189 [6] NCCL INFO Channel 05/32 :    0
Tencent01: VM-0-14-centos:112:189 [6] NCCL INFO Channel 06/32 :    0
Tencent01: VM-0-14-centos:112:189 [6] NCCL INFO Channel 07/32 :    0
Tencent01: VM-0-14-centos:112:189 [6] NCCL INFO Channel 08/32 :    0
Tencent01: VM-0-14-centos:112:189 [6] NCCL INFO Channel 09/32 :    0
Tencent01: VM-0-14-centos:112:189 [6] NCCL INFO Channel 10/32 :    0
Tencent01: VM-0-14-centos:112:189 [6] NCCL INFO Channel 11/32 :    0
Tencent01: VM-0-14-centos:112:189 [6] NCCL INFO Channel 12/32 :    0
Tencent01: VM-0-14-centos:112:189 [6] NCCL INFO Channel 13/32 :    0
Tencent01: VM-0-14-centos:112:189 [6] NCCL INFO Channel 14/32 :    0
Tencent01: VM-0-14-centos:112:189 [6] NCCL INFO Channel 15/32 :    0
Tencent01: VM-0-14-centos:112:189 [6] NCCL INFO Channel 16/32 :    0
Tencent01: VM-0-14-centos:112:189 [6] NCCL INFO Channel 17/32 :    0
Tencent01: VM-0-14-centos:112:189 [6] NCCL INFO Channel 18/32 :    0
Tencent01: VM-0-14-centos:112:189 [6] NCCL INFO Channel 19/32 :    0
Tencent01: VM-0-14-centos:112:189 [6] NCCL INFO Channel 20/32 :    0
Tencent01: VM-0-14-centos:112:189 [6] NCCL INFO Channel 21/32 :    0
Tencent01: VM-0-14-centos:112:189 [6] NCCL INFO Channel 22/32 :    0
Tencent01: VM-0-14-centos:112:189 [6] NCCL INFO Channel 23/32 :    0
Tencent01: VM-0-14-centos:112:189 [6] NCCL INFO Channel 24/32 :    0
Tencent01: VM-0-14-centos:112:189 [6] NCCL INFO Channel 25/32 :    0
Tencent01: VM-0-14-centos:112:189 [6] NCCL INFO Channel 26/32 :    0
Tencent01: VM-0-14-centos:112:189 [6] NCCL INFO Channel 27/32 :    0
Tencent01: VM-0-14-centos:112:189 [6] NCCL INFO Channel 28/32 :    0
Tencent01: VM-0-14-centos:112:189 [6] NCCL INFO Channel 29/32 :    0
Tencent01: VM-0-14-centos:112:189 [6] NCCL INFO Channel 30/32 :    0
Tencent01: VM-0-14-centos:112:189 [6] NCCL INFO Channel 31/32 :    0
Tencent01: VM-0-14-centos:112:189 [6] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0->
Tencent01: VM-0-14-centos:112:189 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000
Tencent01: VM-0-14-centos:112:189 [6] NCCL INFO comm 0x7f59b0001020 rank 0 nranks 1 cudaDev 6 busId b1000 - Init COMPLETE
Tencent01: VM-0-14-centos:106:191 [0] NCCL INFO Channel 00/32 :    0
Tencent01: VM-0-14-centos:106:191 [0] NCCL INFO Channel 01/32 :    0
Tencent01: VM-0-14-centos:106:191 [0] NCCL INFO Channel 02/32 :    0
Tencent01: VM-0-14-centos:106:191 [0] NCCL INFO Channel 03/32 :    0
Tencent01: VM-0-14-centos:106:191 [0] NCCL INFO Channel 04/32 :    0
Tencent01: VM-0-14-centos:106:191 [0] NCCL INFO Channel 05/32 :    0
Tencent01: VM-0-14-centos:106:191 [0] NCCL INFO Channel 06/32 :    0
Tencent01: VM-0-14-centos:106:191 [0] NCCL INFO Channel 07/32 :    0
Tencent01: VM-0-14-centos:106:191 [0] NCCL INFO Channel 08/32 :    0
Tencent01: VM-0-14-centos:106:191 [0] NCCL INFO Channel 09/32 :    0
Tencent01: VM-0-14-centos:106:191 [0] NCCL INFO Channel 10/32 :    0
Tencent01: VM-0-14-centos:106:191 [0] NCCL INFO Channel 11/32 :    0
Tencent01: VM-0-14-centos:106:191 [0] NCCL INFO Channel 12/32 :    0
Tencent01: VM-0-14-centos:106:191 [0] NCCL INFO Channel 13/32 :    0
Tencent01: VM-0-14-centos:106:191 [0] NCCL INFO Channel 14/32 :    0
Tencent01: VM-0-14-centos:106:191 [0] NCCL INFO Channel 15/32 :    0
Tencent01: VM-0-14-centos:106:191 [0] NCCL INFO Channel 16/32 :    0
Tencent01: VM-0-14-centos:106:191 [0] NCCL INFO Channel 17/32 :    0
Tencent01: VM-0-14-centos:106:191 [0] NCCL INFO Channel 18/32 :    0
Tencent01: VM-0-14-centos:106:191 [0] NCCL INFO Channel 19/32 :    0
Tencent01: VM-0-14-centos:106:191 [0] NCCL INFO Channel 20/32 :    0
Tencent01: VM-0-14-centos:106:191 [0] NCCL INFO Channel 21/32 :    0
Tencent01: VM-0-14-centos:106:191 [0] NCCL INFO Channel 22/32 :    0
Tencent01: VM-0-14-centos:106:191 [0] NCCL INFO Channel 23/32 :    0
Tencent01: VM-0-14-centos:106:191 [0] NCCL INFO Channel 24/32 :    0
Tencent01: VM-0-14-centos:106:191 [0] NCCL INFO Channel 25/32 :    0
Tencent01: VM-0-14-centos:106:191 [0] NCCL INFO Channel 26/32 :    0
Tencent01: VM-0-14-centos:106:191 [0] NCCL INFO Channel 27/32 :    0
Tencent01: VM-0-14-centos:106:191 [0] NCCL INFO Channel 28/32 :    0
Tencent01: VM-0-14-centos:106:191 [0] NCCL INFO Channel 29/32 :    0
Tencent01: VM-0-14-centos:106:191 [0] NCCL INFO Channel 30/32 :    0
Tencent01: VM-0-14-centos:106:191 [0] NCCL INFO Channel 31/32 :    0
Tencent01: VM-0-14-centos:106:191 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0->
Tencent01: VM-0-14-centos:106:191 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff
Tencent01: VM-0-14-centos:106:191 [0] NCCL INFO comm 0x7f0144001020 rank 0 nranks 1 cudaDev 0 busId 1a000 - Init COMPLETE
Tencent01: building GPT2 model ...
Tencent01: VM-0-14-centos:110:190 [4] NCCL INFO Channel 00/32 :    0
Tencent01: VM-0-14-centos:110:190 [4] NCCL INFO Channel 01/32 :    0
Tencent01: VM-0-14-centos:110:190 [4] NCCL INFO Channel 02/32 :    0
Tencent01: VM-0-14-centos:110:190 [4] NCCL INFO Channel 03/32 :    0
Tencent01: VM-0-14-centos:110:190 [4] NCCL INFO Channel 04/32 :    0
Tencent01: VM-0-14-centos:110:190 [4] NCCL INFO Channel 05/32 :    0
Tencent01: VM-0-14-centos:110:190 [4] NCCL INFO Channel 06/32 :    0
Tencent01: VM-0-14-centos:110:190 [4] NCCL INFO Channel 07/32 :    0
Tencent01: VM-0-14-centos:110:190 [4] NCCL INFO Channel 08/32 :    0
Tencent01: VM-0-14-centos:110:190 [4] NCCL INFO Channel 09/32 :    0
Tencent01: VM-0-14-centos:110:190 [4] NCCL INFO Channel 10/32 :    0
Tencent01: VM-0-14-centos:110:190 [4] NCCL INFO Channel 11/32 :    0
Tencent01: VM-0-14-centos:110:190 [4] NCCL INFO Channel 12/32 :    0
Tencent01: VM-0-14-centos:110:190 [4] NCCL INFO Channel 13/32 :    0
Tencent01: VM-0-14-centos:110:190 [4] NCCL INFO Channel 14/32 :    0
Tencent01: VM-0-14-centos:110:190 [4] NCCL INFO Channel 15/32 :    0
Tencent01: VM-0-14-centos:110:190 [4] NCCL INFO Channel 16/32 :    0
Tencent01: VM-0-14-centos:110:190 [4] NCCL INFO Channel 17/32 :    0
Tencent01: VM-0-14-centos:110:190 [4] NCCL INFO Channel 18/32 :    0
Tencent01: VM-0-14-centos:110:190 [4] NCCL INFO Channel 19/32 :    0
Tencent01: VM-0-14-centos:110:190 [4] NCCL INFO Channel 20/32 :    0
Tencent01: VM-0-14-centos:110:190 [4] NCCL INFO Channel 21/32 :    0
Tencent01: VM-0-14-centos:110:190 [4] NCCL INFO Channel 22/32 :    0
Tencent01: VM-0-14-centos:110:190 [4] NCCL INFO Channel 23/32 :    0
Tencent01: VM-0-14-centos:110:190 [4] NCCL INFO Channel 24/32 :    0
Tencent01: VM-0-14-centos:110:190 [4] NCCL INFO Channel 25/32 :    0
Tencent01: VM-0-14-centos:110:190 [4] NCCL INFO Channel 26/32 :    0
Tencent01: VM-0-14-centos:110:190 [4] NCCL INFO Channel 27/32 :    0
Tencent01: VM-0-14-centos:110:190 [4] NCCL INFO Channel 28/32 :    0
Tencent01: VM-0-14-centos:110:190 [4] NCCL INFO Channel 29/32 :    0
Tencent01: VM-0-14-centos:110:190 [4] NCCL INFO Channel 30/32 :    0
Tencent01: VM-0-14-centos:110:190 [4] NCCL INFO Channel 31/32 :    0
Tencent01: VM-0-14-centos:110:190 [4] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0->
Tencent01: VM-0-14-centos:110:190 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000
Tencent01: VM-0-14-centos:110:190 [4] NCCL INFO comm 0x7f17b4001020 rank 0 nranks 1 cudaDev 4 busId 88000 - Init COMPLETE
Tencent01: VM-0-14-centos:113:192 [7] NCCL INFO Channel 00/32 :    0
Tencent01: VM-0-14-centos:113:192 [7] NCCL INFO Channel 01/32 :    0
Tencent01: VM-0-14-centos:113:192 [7] NCCL INFO Channel 02/32 :    0
Tencent01: VM-0-14-centos:113:192 [7] NCCL INFO Channel 03/32 :    0
Tencent01: VM-0-14-centos:113:192 [7] NCCL INFO Channel 04/32 :    0
Tencent01: VM-0-14-centos:113:192 [7] NCCL INFO Channel 05/32 :    0
Tencent01: VM-0-14-centos:113:192 [7] NCCL INFO Channel 06/32 :    0
Tencent01: VM-0-14-centos:113:192 [7] NCCL INFO Channel 07/32 :    0
Tencent01: VM-0-14-centos:113:192 [7] NCCL INFO Channel 08/32 :    0
Tencent01: VM-0-14-centos:113:192 [7] NCCL INFO Channel 09/32 :    0
Tencent01: VM-0-14-centos:113:192 [7] NCCL INFO Channel 10/32 :    0
Tencent01: VM-0-14-centos:113:192 [7] NCCL INFO Channel 11/32 :    0
Tencent01: VM-0-14-centos:113:192 [7] NCCL INFO Channel 12/32 :    0
Tencent01: VM-0-14-centos:113:192 [7] NCCL INFO Channel 13/32 :    0
Tencent01: VM-0-14-centos:113:192 [7] NCCL INFO Channel 14/32 :    0
Tencent01: VM-0-14-centos:113:192 [7] NCCL INFO Channel 15/32 :    0
Tencent01: VM-0-14-centos:113:192 [7] NCCL INFO Channel 16/32 :    0
Tencent01: VM-0-14-centos:113:192 [7] NCCL INFO Channel 17/32 :    0
Tencent01: VM-0-14-centos:113:192 [7] NCCL INFO Channel 18/32 :    0
Tencent01: VM-0-14-centos:113:192 [7] NCCL INFO Channel 19/32 :    0
Tencent01: VM-0-14-centos:113:192 [7] NCCL INFO Channel 20/32 :    0
Tencent01: VM-0-14-centos:113:192 [7] NCCL INFO Channel 21/32 :    0
Tencent01: VM-0-14-centos:113:192 [7] NCCL INFO Channel 22/32 :    0
Tencent01: VM-0-14-centos:113:192 [7] NCCL INFO Channel 23/32 :    0
Tencent01: VM-0-14-centos:113:192 [7] NCCL INFO Channel 24/32 :    0
Tencent01: VM-0-14-centos:113:192 [7] NCCL INFO Channel 25/32 :    0
Tencent01: VM-0-14-centos:113:192 [7] NCCL INFO Channel 26/32 :    0
Tencent01: VM-0-14-centos:113:192 [7] NCCL INFO Channel 27/32 :    0
Tencent01: VM-0-14-centos:113:192 [7] NCCL INFO Channel 28/32 :    0
Tencent01: VM-0-14-centos:113:192 [7] NCCL INFO Channel 29/32 :    0
Tencent01: VM-0-14-centos:113:192 [7] NCCL INFO Channel 30/32 :    0
Tencent01: VM-0-14-centos:113:192 [7] NCCL INFO Channel 31/32 :    0
Tencent01: VM-0-14-centos:113:192 [7] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0->
Tencent01: VM-0-14-centos:113:192 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000
Tencent01: VM-0-14-centos:113:192 [7] NCCL INFO comm 0x7f12dc001020 rank 0 nranks 1 cudaDev 7 busId b2000 - Init COMPLETE
Tencent02: VM-0-9-centos:78:78 [3] NCCL INFO Bootstrap : Using [0]bond0:9.216.102.2<0>
Tencent02: VM-0-9-centos:78:78 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
Tencent02: VM-0-9-centos:78:78 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
Tencent02: VM-0-9-centos:78:78 [3] NCCL INFO NET/IB : Using [0]mlx5_bond_0:1/RoCE ; OOB bond0:9.216.102.2<0>
Tencent02: VM-0-9-centos:78:78 [3] NCCL INFO Using network IB
Tencent02: NCCL version 2.6.4+cuda10.0
Tencent02: VM-0-9-centos:76:76 [1] NCCL INFO Bootstrap : Using [0]bond0:9.216.102.2<0>
Tencent02: VM-0-9-centos:76:76 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
Tencent02: VM-0-9-centos:76:76 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
Tencent02: VM-0-9-centos:76:76 [1] NCCL INFO NET/IB : Using [0]mlx5_bond_0:1/RoCE ; OOB bond0:9.216.102.2<0>
Tencent02: VM-0-9-centos:76:76 [1] NCCL INFO Using network IB
Tencent02: NCCL version 2.6.4+cuda10.0
Tencent02: VM-0-9-centos:82:82 [7] NCCL INFO Bootstrap : Using [0]bond0:9.216.102.2<0>
Tencent02: VM-0-9-centos:82:82 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
Tencent02: VM-0-9-centos:82:82 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
Tencent02: VM-0-9-centos:82:82 [7] NCCL INFO NET/IB : Using [0]mlx5_bond_0:1/RoCE ; OOB bond0:9.216.102.2<0>
Tencent02: VM-0-9-centos:82:82 [7] NCCL INFO Using network IB
Tencent02: NCCL version 2.6.4+cuda10.0
Tencent02: VM-0-9-centos:81:81 [6] NCCL INFO Bootstrap : Using [0]bond0:9.216.102.2<0>
Tencent02: VM-0-9-centos:81:81 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
Tencent02: VM-0-9-centos:81:81 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
Tencent02: VM-0-9-centos:81:81 [6] NCCL INFO NET/IB : Using [0]mlx5_bond_0:1/RoCE ; OOB bond0:9.216.102.2<0>
Tencent02: VM-0-9-centos:81:81 [6] NCCL INFO Using network IB
Tencent02: NCCL version 2.6.4+cuda10.0
Tencent02: VM-0-9-centos:75:75 [0] NCCL INFO Bootstrap : Using [0]bond0:9.216.102.2<0>
Tencent02: VM-0-9-centos:75:75 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
Tencent02: VM-0-9-centos:75:75 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
Tencent02: VM-0-9-centos:75:75 [0] NCCL INFO NET/IB : Using [0]mlx5_bond_0:1/RoCE ; OOB bond0:9.216.102.2<0>
Tencent02: VM-0-9-centos:75:75 [0] NCCL INFO Using network IB
Tencent02: NCCL version 2.6.4+cuda10.0
Tencent02: VM-0-9-centos:79:79 [4] NCCL INFO Bootstrap : Using [0]bond0:9.216.102.2<0>
Tencent02: VM-0-9-centos:79:79 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
Tencent02: VM-0-9-centos:79:79 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
Tencent02: VM-0-9-centos:79:79 [4] NCCL INFO NET/IB : Using [0]mlx5_bond_0:1/RoCE ; OOB bond0:9.216.102.2<0>
Tencent02: VM-0-9-centos:79:79 [4] NCCL INFO Using network IB
Tencent02: NCCL version 2.6.4+cuda10.0
Tencent02: VM-0-9-centos:78:140 [3] NCCL INFO Channel 00/32 :    0
Tencent02: VM-0-9-centos:78:140 [3] NCCL INFO Channel 01/32 :    0
Tencent02: VM-0-9-centos:78:140 [3] NCCL INFO Channel 02/32 :    0
Tencent02: VM-0-9-centos:78:140 [3] NCCL INFO Channel 03/32 :    0
Tencent02: VM-0-9-centos:78:140 [3] NCCL INFO Channel 04/32 :    0
Tencent02: VM-0-9-centos:78:140 [3] NCCL INFO Channel 05/32 :    0
Tencent02: VM-0-9-centos:78:140 [3] NCCL INFO Channel 06/32 :    0
Tencent02: VM-0-9-centos:78:140 [3] NCCL INFO Channel 07/32 :    0
Tencent02: VM-0-9-centos:78:140 [3] NCCL INFO Channel 08/32 :    0
Tencent02: VM-0-9-centos:78:140 [3] NCCL INFO Channel 09/32 :    0
Tencent02: VM-0-9-centos:78:140 [3] NCCL INFO Channel 10/32 :    0
Tencent02: VM-0-9-centos:78:140 [3] NCCL INFO Channel 11/32 :    0
Tencent02: VM-0-9-centos:78:140 [3] NCCL INFO Channel 12/32 :    0
Tencent02: VM-0-9-centos:78:140 [3] NCCL INFO Channel 13/32 :    0
Tencent02: VM-0-9-centos:78:140 [3] NCCL INFO Channel 14/32 :    0
Tencent02: VM-0-9-centos:78:140 [3] NCCL INFO Channel 15/32 :    0
Tencent02: VM-0-9-centos:78:140 [3] NCCL INFO Channel 16/32 :    0
Tencent02: VM-0-9-centos:78:140 [3] NCCL INFO Channel 17/32 :    0
Tencent02: VM-0-9-centos:78:140 [3] NCCL INFO Channel 18/32 :    0
Tencent02: VM-0-9-centos:78:140 [3] NCCL INFO Channel 19/32 :    0
Tencent02: VM-0-9-centos:78:140 [3] NCCL INFO Channel 20/32 :    0
Tencent02: VM-0-9-centos:78:140 [3] NCCL INFO Channel 21/32 :    0
Tencent02: VM-0-9-centos:78:140 [3] NCCL INFO Channel 22/32 :    0
Tencent02: VM-0-9-centos:78:140 [3] NCCL INFO Channel 23/32 :    0
Tencent02: VM-0-9-centos:78:140 [3] NCCL INFO Channel 24/32 :    0
Tencent02: VM-0-9-centos:78:140 [3] NCCL INFO Channel 25/32 :    0
Tencent02: VM-0-9-centos:78:140 [3] NCCL INFO Channel 26/32 :    0
Tencent02: VM-0-9-centos:78:140 [3] NCCL INFO Channel 27/32 :    0
Tencent02: VM-0-9-centos:78:140 [3] NCCL INFO Channel 28/32 :    0
Tencent02: VM-0-9-centos:78:140 [3] NCCL INFO Channel 29/32 :    0
Tencent02: VM-0-9-centos:78:140 [3] NCCL INFO Channel 30/32 :    0
Tencent02: VM-0-9-centos:78:140 [3] NCCL INFO Channel 31/32 :    0
Tencent02: VM-0-9-centos:78:140 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0->-1
Tencent02: VM-0-9-centos:78:140 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff
Tencent02: VM-0-9-centos:78:140 [3] NCCL INFO comm 0x7fa3ec001020 rank 0 nranks 1 cudaDev 3 busId 3e000 - Init COMPLETE
Tencent02: VM-0-9-centos:76:151 [1] NCCL INFO Channel 00/32 :    0
Tencent02: VM-0-9-centos:76:151 [1] NCCL INFO Channel 01/32 :    0
Tencent02: VM-0-9-centos:76:151 [1] NCCL INFO Channel 02/32 :    0
Tencent02: VM-0-9-centos:76:151 [1] NCCL INFO Channel 03/32 :    0
Tencent02: VM-0-9-centos:76:151 [1] NCCL INFO Channel 04/32 :    0
Tencent02: VM-0-9-centos:76:151 [1] NCCL INFO Channel 05/32 :    0
Tencent02: VM-0-9-centos:76:151 [1] NCCL INFO Channel 06/32 :    0
Tencent02: VM-0-9-centos:76:151 [1] NCCL INFO Channel 07/32 :    0
Tencent02: VM-0-9-centos:76:151 [1] NCCL INFO Channel 08/32 :    0
Tencent02: VM-0-9-centos:76:151 [1] NCCL INFO Channel 09/32 :    0
Tencent02: VM-0-9-centos:76:151 [1] NCCL INFO Channel 10/32 :    0
Tencent02: VM-0-9-centos:76:151 [1] NCCL INFO Channel 11/32 :    0
Tencent02: VM-0-9-centos:76:151 [1] NCCL INFO Channel 12/32 :    0
Tencent02: VM-0-9-centos:76:151 [1] NCCL INFO Channel 13/32 :    0
Tencent02: VM-0-9-centos:76:151 [1] NCCL INFO Channel 14/32 :    0
Tencent02: VM-0-9-centos:76:151 [1] NCCL INFO Channel 15/32 :    0
Tencent02: VM-0-9-centos:76:151 [1] NCCL INFO Channel 16/32 :    0
Tencent02: VM-0-9-centos:76:151 [1] NCCL INFO Channel 17/32 :    0
Tencent02: VM-0-9-centos:76:151 [1] NCCL INFO Channel 18/32 :    0
Tencent02: VM-0-9-centos:76:151 [1] NCCL INFO Channel 19/32 :    0
Tencent02: VM-0-9-centos:76:151 [1] NCCL INFO Channel 20/32 :    0
Tencent02: VM-0-9-centos:76:151 [1] NCCL INFO Channel 21/32 :    0
Tencent02: VM-0-9-centos:76:151 [1] NCCL INFO Channel 22/32 :    0
Tencent02: VM-0-9-centos:76:151 [1] NCCL INFO Channel 23/32 :    0
Tencent02: VM-0-9-centos:76:151 [1] NCCL INFO Channel 24/32 :    0
Tencent02: VM-0-9-centos:76:151 [1] NCCL INFO Channel 25/32 :    0
Tencent02: VM-0-9-centos:76:151 [1] NCCL INFO Channel 26/32 :    0
Tencent02: VM-0-9-centos:76:151 [1] NCCL INFO Channel 27/32 :    0
Tencent02: VM-0-9-centos:76:151 [1] NCCL INFO Channel 28/32 :    0
Tencent02: VM-0-9-centos:76:151 [1] NCCL INFO Channel 29/32 :    0
Tencent02: VM-0-9-centos:76:151 [1] NCCL INFO Channel 30/32 :    0
Tencent02: VM-0-9-centos:76:151 [1] NCCL INFO Channel 31/32 :    0
Tencent02: VM-0-9-centos:76:151 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0->-1
Tencent02: VM-0-9-centos:76:151 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff
Tencent02: VM-0-9-centos:82:152 [7] NCCL INFO Channel 00/32 :    0
Tencent02: VM-0-9-centos:82:152 [7] NCCL INFO Channel 01/32 :    0
Tencent02: VM-0-9-centos:82:152 [7] NCCL INFO Channel 02/32 :    0
Tencent02: VM-0-9-centos:82:152 [7] NCCL INFO Channel 03/32 :    0
Tencent02: VM-0-9-centos:82:152 [7] NCCL INFO Channel 04/32 :    0
Tencent02: VM-0-9-centos:82:152 [7] NCCL INFO Channel 05/32 :    0
Tencent02: VM-0-9-centos:82:152 [7] NCCL INFO Channel 06/32 :    0
Tencent02: VM-0-9-centos:82:152 [7] NCCL INFO Channel 07/32 :    0
Tencent02: VM-0-9-centos:82:152 [7] NCCL INFO Channel 08/32 :    0
Tencent02: VM-0-9-centos:82:152 [7] NCCL INFO Channel 09/32 :    0
Tencent02: VM-0-9-centos:82:152 [7] NCCL INFO Channel 10/32 :    0
Tencent02: VM-0-9-centos:82:152 [7] NCCL INFO Channel 11/32 :    0
Tencent02: VM-0-9-centos:82:152 [7] NCCL INFO Channel 12/32 :    0
Tencent02: VM-0-9-centos:82:152 [7] NCCL INFO Channel 13/32 :    0
Tencent02: VM-0-9-centos:82:152 [7] NCCL INFO Channel 14/32 :    0
Tencent02: VM-0-9-centos:82:152 [7] NCCL INFO Channel 15/32 :    0
Tencent02: VM-0-9-centos:82:152 [7] NCCL INFO Channel 16/32 :    0
Tencent02: VM-0-9-centos:82:152 [7] NCCL INFO Channel 17/32 :    0
Tencent02: VM-0-9-centos:82:152 [7] NCCL INFO Channel 18/32 :    0
Tencent02: VM-0-9-centos:82:152 [7] NCCL INFO Channel 19/32 :    0
Tencent02: VM-0-9-centos:82:152 [7] NCCL INFO Channel 20/32 :    0
Tencent02: VM-0-9-centos:82:152 [7] NCCL INFO Channel 21/32 :    0
Tencent02: VM-0-9-centos:82:152 [7] NCCL INFO Channel 22/32 :    0
Tencent02: VM-0-9-centos:82:152 [7] NCCL INFO Channel 23/32 :    0
Tencent02: VM-0-9-centos:82:152 [7] NCCL INFO Channel 24/32 :    0
Tencent02: VM-0-9-centos:82:152 [7] NCCL INFO Channel 25/32 :    0
Tencent02: VM-0-9-centos:82:152 [7] NCCL INFO Channel 26/32 :    0
Tencent02: VM-0-9-centos:82:152 [7] NCCL INFO Channel 27/32 :    0
Tencent02: VM-0-9-centos:82:152 [7] NCCL INFO Channel 28/32 :    0
Tencent02: VM-0-9-centos:82:152 [7] NCCL INFO Channel 29/32 :    0
Tencent02: VM-0-9-centos:82:152 [7] NCCL INFO Channel 30/32 :    0
Tencent02: VM-0-9-centos:82:152 [7] NCCL INFO Channel 31/32 :    0
Tencent02: VM-0-9-centos:82:152 [7] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0->-1
Tencent02: VM-0-9-centos:82:152 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000
Tencent02: VM-0-9-centos:76:151 [1] NCCL INFO comm 0x7ff47c001020 rank 0 nranks 1 cudaDev 1 busId 1b000 - Init COMPLETE
Tencent02: VM-0-9-centos:82:152 [7] NCCL INFO comm 0x7f1584001020 rank 0 nranks 1 cudaDev 7 busId b2000 - Init COMPLETE
Tencent02: VM-0-9-centos:75:154 [0] NCCL INFO Channel 00/32 :    0
Tencent02: VM-0-9-centos:75:154 [0] NCCL INFO Channel 01/32 :    0
Tencent02: VM-0-9-centos:75:154 [0] NCCL INFO Channel 02/32 :    0
Tencent02: VM-0-9-centos:75:154 [0] NCCL INFO Channel 03/32 :    0
Tencent02: VM-0-9-centos:75:154 [0] NCCL INFO Channel 04/32 :    0
Tencent02: VM-0-9-centos:75:154 [0] NCCL INFO Channel 05/32 :    0
Tencent02: VM-0-9-centos:75:154 [0] NCCL INFO Channel 06/32 :    0
Tencent02: VM-0-9-centos:75:154 [0] NCCL INFO Channel 07/32 :    0
Tencent02: VM-0-9-centos:75:154 [0] NCCL INFO Channel 08/32 :    0
Tencent02: VM-0-9-centos:75:154 [0] NCCL INFO Channel 09/32 :    0
Tencent02: VM-0-9-centos:75:154 [0] NCCL INFO Channel 10/32 :    0
Tencent02: VM-0-9-centos:75:154 [0] NCCL INFO Channel 11/32 :    0
Tencent02: VM-0-9-centos:75:154 [0] NCCL INFO Channel 12/32 :    0
Tencent02: VM-0-9-centos:75:154 [0] NCCL INFO Channel 13/32 :    0
Tencent02: VM-0-9-centos:75:154 [0] NCCL INFO Channel 14/32 :    0
Tencent02: VM-0-9-centos:75:154 [0] NCCL INFO Channel 15/32 :    0
Tencent02: VM-0-9-centos:75:154 [0] NCCL INFO Channel 16/32 :    0
Tencent02: VM-0-9-centos:75:154 [0] NCCL INFO Channel 17/32 :    0
Tencent02: VM-0-9-centos:75:154 [0] NCCL INFO Channel 18/32 :    0
Tencent02: VM-0-9-centos:75:154 [0] NCCL INFO Channel 19/32 :    0
Tencent02: VM-0-9-centos:75:154 [0] NCCL INFO Channel 20/32 :    0
Tencent02: VM-0-9-centos:75:154 [0] NCCL INFO Channel 21/32 :    0
Tencent02: VM-0-9-centos:75:154 [0] NCCL INFO Channel 22/32 :    0
Tencent02: VM-0-9-centos:75:154 [0] NCCL INFO Channel 23/32 :    0
Tencent02: VM-0-9-centos:75:154 [0] NCCL INFO Channel 24/32 :    0
Tencent02: VM-0-9-centos:75:154 [0] NCCL INFO Channel 25/32 :    0
Tencent02: VM-0-9-centos:75:154 [0] NCCL INFO Channel 26/32 :    0
Tencent02: VM-0-9-centos:75:154 [0] NCCL INFO Channel 27/32 :    0
Tencent02: VM-0-9-centos:75:154 [0] NCCL INFO Channel 28/32 :    0
Tencent02: VM-0-9-centos:75:154 [0] NCCL INFO Channel 29/32 :    0
Tencent02: VM-0-9-centos:75:154 [0] NCCL INFO Channel 30/32 :    0
Tencent02: VM-0-9-centos:75:154 [0] NCCL INFO Channel 31/32 :    0
Tencent02: VM-0-9-centos:75:154 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0->-1
Tencent02: VM-0-9-centos:75:154 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff
Tencent02: VM-0-9-centos:75:154 [0] NCCL INFO comm 0x7f9be0001020 rank 0 nranks 1 cudaDev 0 busId 1a000 - Init COMPLETE
Tencent02: VM-0-9-centos:81:153 [6] NCCL INFO Channel 00/32 :    0
Tencent02: VM-0-9-centos:81:153 [6] NCCL INFO Channel 01/32 :    0
Tencent02: VM-0-9-centos:81:153 [6] NCCL INFO Channel 02/32 :    0
Tencent02: VM-0-9-centos:81:153 [6] NCCL INFO Channel 03/32 :    0
Tencent02: VM-0-9-centos:81:153 [6] NCCL INFO Channel 04/32 :    0
Tencent02: VM-0-9-centos:81:153 [6] NCCL INFO Channel 05/32 :    0
Tencent02: VM-0-9-centos:81:153 [6] NCCL INFO Channel 06/32 :    0
Tencent02: VM-0-9-centos:81:153 [6] NCCL INFO Channel 07/32 :    0
Tencent02: VM-0-9-centos:81:153 [6] NCCL INFO Channel 08/32 :    0
Tencent02: VM-0-9-centos:81:153 [6] NCCL INFO Channel 09/32 :    0
Tencent02: VM-0-9-centos:81:153 [6] NCCL INFO Channel 10/32 :    0
Tencent02: VM-0-9-centos:81:153 [6] NCCL INFO Channel 11/32 :    0
Tencent02: VM-0-9-centos:81:153 [6] NCCL INFO Channel 12/32 :    0
Tencent02: VM-0-9-centos:81:153 [6] NCCL INFO Channel 13/32 :    0
Tencent02: VM-0-9-centos:81:153 [6] NCCL INFO Channel 14/32 :    0
Tencent02: VM-0-9-centos:81:153 [6] NCCL INFO Channel 15/32 :    0
Tencent02: VM-0-9-centos:81:153 [6] NCCL INFO Channel 16/32 :    0
Tencent02: VM-0-9-centos:81:153 [6] NCCL INFO Channel 17/32 :    0
Tencent02: VM-0-9-centos:81:153 [6] NCCL INFO Channel 18/32 :    0
Tencent02: VM-0-9-centos:81:153 [6] NCCL INFO Channel 19/32 :    0
Tencent02: VM-0-9-centos:81:153 [6] NCCL INFO Channel 20/32 :    0
Tencent02: VM-0-9-centos:81:153 [6] NCCL INFO Channel 21/32 :    0
Tencent02: VM-0-9-centos:81:153 [6] NCCL INFO Channel 22/32 :    0
Tencent02: VM-0-9-centos:81:153 [6] NCCL INFO Channel 23/32 :    0
Tencent02: VM-0-9-centos:81:153 [6] NCCL INFO Channel 24/32 :    0
Tencent02: VM-0-9-centos:81:153 [6] NCCL INFO Channel 25/32 :    0
Tencent02: VM-0-9-centos:81:153 [6] NCCL INFO Channel 26/32 :    0
Tencent02: VM-0-9-centos:81:153 [6] NCCL INFO Channel 27/32 :    0
Tencent02: VM-0-9-centos:81:153 [6] NCCL INFO Channel 28/32 :    0
Tencent02: VM-0-9-centos:81:153 [6] NCCL INFO Channel 29/32 :    0
Tencent02: VM-0-9-centos:81:153 [6] NCCL INFO Channel 30/32 :    0
Tencent02: VM-0-9-centos:81:153 [6] NCCL INFO Channel 31/32 :    0
Tencent02: VM-0-9-centos:81:153 [6] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0->-1
Tencent02: VM-0-9-centos:81:153 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000
Tencent02: VM-0-9-centos:81:153 [6] NCCL INFO comm 0x7fa85c001020 rank 0 nranks 1 cudaDev 6 busId b1000 - Init COMPLETE
Tencent02: VM-0-9-centos:79:155 [4] NCCL INFO Channel 00/32 :    0
Tencent02: VM-0-9-centos:79:155 [4] NCCL INFO Channel 01/32 :    0
Tencent02: VM-0-9-centos:79:155 [4] NCCL INFO Channel 02/32 :    0
Tencent02: VM-0-9-centos:79:155 [4] NCCL INFO Channel 03/32 :    0
Tencent02: VM-0-9-centos:79:155 [4] NCCL INFO Channel 04/32 :    0
Tencent02: VM-0-9-centos:79:155 [4] NCCL INFO Channel 05/32 :    0
Tencent02: VM-0-9-centos:79:155 [4] NCCL INFO Channel 06/32 :    0
Tencent02: VM-0-9-centos:79:155 [4] NCCL INFO Channel 07/32 :    0
Tencent02: VM-0-9-centos:79:155 [4] NCCL INFO Channel 08/32 :    0
Tencent02: VM-0-9-centos:79:155 [4] NCCL INFO Channel 09/32 :    0
Tencent02: VM-0-9-centos:79:155 [4] NCCL INFO Channel 10/32 :    0
Tencent02: VM-0-9-centos:79:155 [4] NCCL INFO Channel 11/32 :    0
Tencent02: VM-0-9-centos:79:155 [4] NCCL INFO Channel 12/32 :    0
Tencent02: VM-0-9-centos:79:155 [4] NCCL INFO Channel 13/32 :    0
Tencent02: VM-0-9-centos:79:155 [4] NCCL INFO Channel 14/32 :    0
Tencent02: VM-0-9-centos:79:155 [4] NCCL INFO Channel 15/32 :    0
Tencent02: VM-0-9-centos:79:155 [4] NCCL INFO Channel 16/32 :    0
Tencent02: VM-0-9-centos:79:155 [4] NCCL INFO Channel 17/32 :    0
Tencent02: VM-0-9-centos:79:155 [4] NCCL INFO Channel 18/32 :    0
Tencent02: VM-0-9-centos:79:155 [4] NCCL INFO Channel 19/32 :    0
Tencent02: VM-0-9-centos:79:155 [4] NCCL INFO Channel 20/32 :    0
Tencent02: VM-0-9-centos:79:155 [4] NCCL INFO Channel 21/32 :    0
Tencent02: VM-0-9-centos:79:155 [4] NCCL INFO Channel 22/32 :    0
Tencent02: VM-0-9-centos:79:155 [4] NCCL INFO Channel 23/32 :    0
Tencent02: VM-0-9-centos:79:155 [4] NCCL INFO Channel 24/32 :    0
Tencent02: VM-0-9-centos:79:155 [4] NCCL INFO Channel 25/32 :    0
Tencent02: VM-0-9-centos:79:155 [4] NCCL INFO Channel 26/32 :    0
Tencent02: VM-0-9-centos:79:155 [4] NCCL INFO Channel 27/32 :    0
Tencent02: VM-0-9-centos:79:155 [4] NCCL INFO Channel 28/32 :    0
Tencent02: VM-0-9-centos:79:155 [4] NCCL INFO Channel 29/32 :    0
Tencent02: VM-0-9-centos:79:155 [4] NCCL INFO Channel 30/32 :    0
Tencent02: VM-0-9-centos:79:155 [4] NCCL INFO Channel 31/32 :    0
Tencent02: VM-0-9-centos:79:155 [4] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0->-1
Tencent02: VM-0-9-centos:79:155 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000
Tencent02: VM-0-9-centos:79:155 [4] NCCL INFO comm 0x7efaa4001020 rank 0 nranks 1 cudaDev 4 busId 88000 - Init COMPLETE
Tencent01: VM-0-14-centos:111:111 [5] NCCL INFO Bootstrap : Using [0]bond0:9.216.102.34<0>
Tencent01: VM-0-14-centos:111:111 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
Tencent01: VM-0-14-centos:111:111 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
Tencent01: VM-0-14-centos:111:111 [5] NCCL INFO NET/IB : Using [0]mlx5_bond_0:1/RoCE ; OOB bond0:9.216.102.34<0>
Tencent01: VM-0-14-centos:111:111 [5] NCCL INFO Using network IB
Tencent01: NCCL version 2.6.4+cuda10.0
Tencent02: VM-0-9-centos:77:77 [2] NCCL INFO Bootstrap : Using [0]bond0:9.216.102.2<0>
Tencent02: VM-0-9-centos:77:77 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
Tencent02: VM-0-9-centos:77:77 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
Tencent02: VM-0-9-centos:77:77 [2] NCCL INFO NET/IB : Using [0]mlx5_bond_0:1/RoCE ; OOB bond0:9.216.102.2<0>
Tencent02: VM-0-9-centos:77:77 [2] NCCL INFO Using network IB
Tencent02: NCCL version 2.6.4+cuda10.0
Tencent01: VM-0-14-centos:111:202 [5] NCCL INFO Channel 00/32 :    0
Tencent01: VM-0-14-centos:111:202 [5] NCCL INFO Channel 01/32 :    0
Tencent01: VM-0-14-centos:111:202 [5] NCCL INFO Channel 02/32 :    0
Tencent01: VM-0-14-centos:111:202 [5] NCCL INFO Channel 03/32 :    0
Tencent01: VM-0-14-centos:111:202 [5] NCCL INFO Channel 04/32 :    0
Tencent01: VM-0-14-centos:111:202 [5] NCCL INFO Channel 05/32 :    0
Tencent01: VM-0-14-centos:111:202 [5] NCCL INFO Channel 06/32 :    0
Tencent01: VM-0-14-centos:111:202 [5] NCCL INFO Channel 07/32 :    0
Tencent01: VM-0-14-centos:111:202 [5] NCCL INFO Channel 08/32 :    0
Tencent01: VM-0-14-centos:111:202 [5] NCCL INFO Channel 09/32 :    0
Tencent01: VM-0-14-centos:111:202 [5] NCCL INFO Channel 10/32 :    0
Tencent01: VM-0-14-centos:111:202 [5] NCCL INFO Channel 11/32 :    0
Tencent01: VM-0-14-centos:111:202 [5] NCCL INFO Channel 12/32 :    0
Tencent01: VM-0-14-centos:111:202 [5] NCCL INFO Channel 13/32 :    0
Tencent01: VM-0-14-centos:111:202 [5] NCCL INFO Channel 14/32 :    0
Tencent01: VM-0-14-centos:111:202 [5] NCCL INFO Channel 15/32 :    0
Tencent01: VM-0-14-centos:111:202 [5] NCCL INFO Channel 16/32 :    0
Tencent01: VM-0-14-centos:111:202 [5] NCCL INFO Channel 17/32 :    0
Tencent01: VM-0-14-centos:111:202 [5] NCCL INFO Channel 18/32 :    0
Tencent01: VM-0-14-centos:111:202 [5] NCCL INFO Channel 19/32 :    0
Tencent01: VM-0-14-centos:111:202 [5] NCCL INFO Channel 20/32 :    0
Tencent01: VM-0-14-centos:111:202 [5] NCCL INFO Channel 21/32 :    0
Tencent01: VM-0-14-centos:111:202 [5] NCCL INFO Channel 22/32 :    0
Tencent01: VM-0-14-centos:111:202 [5] NCCL INFO Channel 23/32 :    0
Tencent01: VM-0-14-centos:111:202 [5] NCCL INFO Channel 24/32 :    0
Tencent01: VM-0-14-centos:111:202 [5] NCCL INFO Channel 25/32 :    0
Tencent01: VM-0-14-centos:111:202 [5] NCCL INFO Channel 26/32 :    0
Tencent01: VM-0-14-centos:111:202 [5] NCCL INFO Channel 27/32 :    0
Tencent01: VM-0-14-centos:111:202 [5] NCCL INFO Channel 28/32 :    0
Tencent01: VM-0-14-centos:111:202 [5] NCCL INFO Channel 29/32 :    0
Tencent01: VM-0-14-centos:111:202 [5] NCCL INFO Channel 30/32 :    0
Tencent01: VM-0-14-centos:111:202 [5] NCCL INFO Channel 31/32 :    0
Tencent01: VM-0-14-centos:111:202 [5] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0->
Tencent01: VM-0-14-centos:111:202 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000
Tencent01: VM-0-14-centos:111:202 [5] NCCL INFO comm 0x7f08ec001020 rank 0 nranks 1 cudaDev 5 busId 89000 - Init COMPLETE
Tencent02: VM-0-9-centos:77:165 [2] NCCL INFO Channel 00/32 :    0
Tencent02: VM-0-9-centos:77:165 [2] NCCL INFO Channel 01/32 :    0
Tencent02: VM-0-9-centos:77:165 [2] NCCL INFO Channel 02/32 :    0
Tencent02: VM-0-9-centos:77:165 [2] NCCL INFO Channel 03/32 :    0
Tencent02: VM-0-9-centos:77:165 [2] NCCL INFO Channel 04/32 :    0
Tencent02: VM-0-9-centos:77:165 [2] NCCL INFO Channel 05/32 :    0
Tencent02: VM-0-9-centos:77:165 [2] NCCL INFO Channel 06/32 :    0
Tencent02: VM-0-9-centos:77:165 [2] NCCL INFO Channel 07/32 :    0
Tencent02: VM-0-9-centos:77:165 [2] NCCL INFO Channel 08/32 :    0
Tencent02: VM-0-9-centos:77:165 [2] NCCL INFO Channel 09/32 :    0
Tencent02: VM-0-9-centos:77:165 [2] NCCL INFO Channel 10/32 :    0
Tencent02: VM-0-9-centos:77:165 [2] NCCL INFO Channel 11/32 :    0
Tencent02: VM-0-9-centos:77:165 [2] NCCL INFO Channel 12/32 :    0
Tencent02: VM-0-9-centos:77:165 [2] NCCL INFO Channel 13/32 :    0
Tencent02: VM-0-9-centos:77:165 [2] NCCL INFO Channel 14/32 :    0
Tencent02: VM-0-9-centos:77:165 [2] NCCL INFO Channel 15/32 :    0
Tencent02: VM-0-9-centos:77:165 [2] NCCL INFO Channel 16/32 :    0
Tencent02: VM-0-9-centos:77:165 [2] NCCL INFO Channel 17/32 :    0
Tencent02: VM-0-9-centos:77:165 [2] NCCL INFO Channel 18/32 :    0
Tencent02: VM-0-9-centos:77:165 [2] NCCL INFO Channel 19/32 :    0
Tencent02: VM-0-9-centos:77:165 [2] NCCL INFO Channel 20/32 :    0
Tencent02: VM-0-9-centos:77:165 [2] NCCL INFO Channel 21/32 :    0
Tencent02: VM-0-9-centos:77:165 [2] NCCL INFO Channel 22/32 :    0
Tencent02: VM-0-9-centos:77:165 [2] NCCL INFO Channel 23/32 :    0
Tencent02: VM-0-9-centos:77:165 [2] NCCL INFO Channel 24/32 :    0
Tencent02: VM-0-9-centos:77:165 [2] NCCL INFO Channel 25/32 :    0
Tencent02: VM-0-9-centos:77:165 [2] NCCL INFO Channel 26/32 :    0
Tencent02: VM-0-9-centos:77:165 [2] NCCL INFO Channel 27/32 :    0
Tencent02: VM-0-9-centos:77:165 [2] NCCL INFO Channel 28/32 :    0
Tencent02: VM-0-9-centos:77:165 [2] NCCL INFO Channel 29/32 :    0
Tencent02: VM-0-9-centos:77:165 [2] NCCL INFO Channel 30/32 :    0
Tencent02: VM-0-9-centos:77:165 [2] NCCL INFO Channel 31/32 :    0
Tencent02: VM-0-9-centos:77:165 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0->-1
Tencent02: VM-0-9-centos:77:165 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff
Tencent02: VM-0-9-centos:77:165 [2] NCCL INFO comm 0x7ff694001020 rank 0 nranks 1 cudaDev 2 busId 3d000 - Init COMPLETE
Tencent02: VM-0-9-centos:80:80 [5] NCCL INFO Bootstrap : Using [0]bond0:9.216.102.2<0>
Tencent02: VM-0-9-centos:80:80 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
Tencent02: VM-0-9-centos:80:80 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
Tencent02: VM-0-9-centos:80:80 [5] NCCL INFO NET/IB : Using [0]mlx5_bond_0:1/RoCE ; OOB bond0:9.216.102.2<0>
Tencent02: VM-0-9-centos:80:80 [5] NCCL INFO Using network IB
Tencent02: NCCL version 2.6.4+cuda10.0
Tencent02: VM-0-9-centos:80:169 [5] NCCL INFO Channel 00/32 :    0
Tencent02: VM-0-9-centos:80:169 [5] NCCL INFO Channel 01/32 :    0
Tencent02: VM-0-9-centos:80:169 [5] NCCL INFO Channel 02/32 :    0
Tencent02: VM-0-9-centos:80:169 [5] NCCL INFO Channel 03/32 :    0
Tencent02: VM-0-9-centos:80:169 [5] NCCL INFO Channel 04/32 :    0
Tencent02: VM-0-9-centos:80:169 [5] NCCL INFO Channel 05/32 :    0
Tencent02: VM-0-9-centos:80:169 [5] NCCL INFO Channel 06/32 :    0
Tencent02: VM-0-9-centos:80:169 [5] NCCL INFO Channel 07/32 :    0
Tencent02: VM-0-9-centos:80:169 [5] NCCL INFO Channel 08/32 :    0
Tencent02: VM-0-9-centos:80:169 [5] NCCL INFO Channel 09/32 :    0
Tencent02: VM-0-9-centos:80:169 [5] NCCL INFO Channel 10/32 :    0
Tencent02: VM-0-9-centos:80:169 [5] NCCL INFO Channel 11/32 :    0
Tencent02: VM-0-9-centos:80:169 [5] NCCL INFO Channel 12/32 :    0
Tencent02: VM-0-9-centos:80:169 [5] NCCL INFO Channel 13/32 :    0
Tencent02: VM-0-9-centos:80:169 [5] NCCL INFO Channel 14/32 :    0
Tencent02: VM-0-9-centos:80:169 [5] NCCL INFO Channel 15/32 :    0
Tencent02: VM-0-9-centos:80:169 [5] NCCL INFO Channel 16/32 :    0
Tencent02: VM-0-9-centos:80:169 [5] NCCL INFO Channel 17/32 :    0
Tencent02: VM-0-9-centos:80:169 [5] NCCL INFO Channel 18/32 :    0
Tencent02: VM-0-9-centos:80:169 [5] NCCL INFO Channel 19/32 :    0
Tencent02: VM-0-9-centos:80:169 [5] NCCL INFO Channel 20/32 :    0
Tencent02: VM-0-9-centos:80:169 [5] NCCL INFO Channel 21/32 :    0
Tencent02: VM-0-9-centos:80:169 [5] NCCL INFO Channel 22/32 :    0
Tencent02: VM-0-9-centos:80:169 [5] NCCL INFO Channel 23/32 :    0
Tencent02: VM-0-9-centos:80:169 [5] NCCL INFO Channel 24/32 :    0
Tencent02: VM-0-9-centos:80:169 [5] NCCL INFO Channel 25/32 :    0
Tencent02: VM-0-9-centos:80:169 [5] NCCL INFO Channel 26/32 :    0
Tencent02: VM-0-9-centos:80:169 [5] NCCL INFO Channel 27/32 :    0
Tencent02: VM-0-9-centos:80:169 [5] NCCL INFO Channel 28/32 :    0
Tencent02: VM-0-9-centos:80:169 [5] NCCL INFO Channel 29/32 :    0
Tencent02: VM-0-9-centos:80:169 [5] NCCL INFO Channel 30/32 :    0
Tencent02: VM-0-9-centos:80:169 [5] NCCL INFO Channel 31/32 :    0
Tencent02: VM-0-9-centos:80:169 [5] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0->-1
Tencent02: VM-0-9-centos:80:169 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000
Tencent02: VM-0-9-centos:80:169 [5] NCCL INFO comm 0x7f6600001020 rank 0 nranks 1 cudaDev 5 busId 89000 - Init COMPLETE
Tencent01:  > number of parameters on model parallel rank 0: 1024587520
Tencent01: VM-0-14-centos:110:582 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent01: VM-0-14-centos:110:582 [4] NCCL INFO Trees [0] 5/-1/-1->4->-1|-1->4->5/-1/-1 [1] 5/-1/-1->4->13|13->4->5/-1/-1
Tencent01: VM-0-14-centos:110:582 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000
Tencent01: VM-0-14-centos:111:588 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent01: VM-0-14-centos:111:588 [5] NCCL INFO Trees [0] 7/12/-1->5->4|4->5->7/12/-1 [1] 7/-1/-1->5->4|4->5->7/-1/-1
Tencent01: VM-0-14-centos:112:585 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent01: VM-0-14-centos:113:584 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent01: VM-0-14-centos:111:588 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000
Tencent01: VM-0-14-centos:112:585 [6] NCCL INFO Trees [0] -1/-1/-1->6->1|1->6->-1/-1/-1 [1] -1/-1/-1->6->1|1->6->-1/-1/-1
Tencent01: VM-0-14-centos:113:584 [7] NCCL INFO Trees [0] 0/-1/-1->7->5|5->7->0/-1/-1 [1] 0/-1/-1->7->5|5->7->0/-1/-1
Tencent01: VM-0-14-centos:112:585 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000
Tencent01: VM-0-14-centos:113:584 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000
Tencent01: VM-0-14-centos:109:587 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent01: VM-0-14-centos:109:587 [3] NCCL INFO Trees [0] 1/-1/-1->3->2|2->3->1/-1/-1 [1] 1/-1/-1->3->2|2->3->1/-1/-1
Tencent01: VM-0-14-centos:109:587 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff
Tencent01: VM-0-14-centos:108:586 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent01: VM-0-14-centos:108:586 [2] NCCL INFO Trees [0] 3/-1/-1->2->0|0->2->3/-1/-1 [1] 3/-1/-1->2->0|0->2->3/-1/-1
Tencent01: VM-0-14-centos:108:586 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff
Tencent01: VM-0-14-centos:107:583 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent01: VM-0-14-centos:107:583 [1] NCCL INFO Trees [0] 6/-1/-1->1->3|3->1->6/-1/-1 [1] 6/-1/-1->1->3|3->1->6/-1/-1
Tencent01: VM-0-14-centos:107:583 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff
Tencent01: VM-0-14-centos:106:581 [0] NCCL INFO Channel 00/02 :    0   1   3   2   5  12  14  15   8   9  11  10  13   4   6   7
Tencent01: VM-0-14-centos:106:581 [0] NCCL INFO Channel 01/02 :    0   1   3   2   5  12  14  15   8   9  11  10  13   4   6   7
Tencent02: VM-0-9-centos:80:554 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent02: VM-0-9-centos:77:506 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent02: VM-0-9-centos:75:505 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent02: VM-0-9-centos:82:454 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent02: VM-0-9-centos:78:453 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent02: VM-0-9-centos:76:457 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent02: VM-0-9-centos:80:554 [5] NCCL INFO Trees [0] 15/-1/-1->13->12|12->13->15/-1/-1 [1] 15/4/-1->13->12|12->13->15/4/-1
Tencent02: VM-0-9-centos:77:506 [2] NCCL INFO Trees [0] 11/-1/-1->10->8|8->10->11/-1/-1 [1] 11/-1/-1->10->8|8->10->11/-1/-1
Tencent02: VM-0-9-centos:79:456 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent02: VM-0-9-centos:82:454 [7] NCCL INFO Trees [0] 8/-1/-1->15->13|13->15->8/-1/-1 [1] 8/-1/-1->15->13|13->15->8/-1/-1
Tencent02: VM-0-9-centos:78:453 [3] NCCL INFO Trees [0] 9/-1/-1->11->10|10->11->9/-1/-1 [1] 9/-1/-1->11->10|10->11->9/-1/-1
Tencent02: VM-0-9-centos:81:455 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent02: VM-0-9-centos:75:505 [0] NCCL INFO Trees [0] 10/-1/-1->8->15|15->8->10/-1/-1 [1] 10/-1/-1->8->15|15->8->10/-1/-1
Tencent02: VM-0-9-centos:76:457 [1] NCCL INFO Trees [0] 14/-1/-1->9->11|11->9->14/-1/-1 [1] 14/-1/-1->9->11|11->9->14/-1/-1
Tencent02: VM-0-9-centos:77:506 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff
Tencent02: VM-0-9-centos:80:554 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000
Tencent02: VM-0-9-centos:79:456 [4] NCCL INFO Trees [0] 13/-1/-1->12->5|5->12->13/-1/-1 [1] 13/-1/-1->12->-1|-1->12->13/-1/-1
Tencent01: VM-0-14-centos:106:581 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent02: VM-0-9-centos:78:453 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff
Tencent01: VM-0-14-centos:106:581 [0] NCCL INFO Trees [0] 2/-1/-1->0->7|7->0->2/-1/-1 [1] 2/-1/-1->0->7|7->0->2/-1/-1
Tencent01: VM-0-14-centos:106:581 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff
Tencent02: VM-0-9-centos:82:454 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000
Tencent02: VM-0-9-centos:76:457 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff
Tencent02: VM-0-9-centos:81:455 [6] NCCL INFO Trees [0] -1/-1/-1->14->9|9->14->-1/-1/-1 [1] -1/-1/-1->14->9|9->14->-1/-1/-1
Tencent02: VM-0-9-centos:79:456 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000
Tencent02: VM-0-9-centos:75:505 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff
Tencent02: VM-0-9-centos:81:455 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000
Tencent01: VM-0-14-centos:108:586 [2] NCCL INFO Ring 00 : 2[3d000] -> 5[89000] via P2P/IPC
Tencent02: VM-0-9-centos:82:454 [7] NCCL INFO Ring 00 : 15[b2000] -> 8[1a000] via P2P/IPC
Tencent01: VM-0-14-centos:107:583 [1] NCCL INFO Ring 00 : 1[1b000] -> 3[3e000] via P2P/IPC
Tencent02: VM-0-9-centos:76:457 [1] NCCL INFO Ring 00 : 9[1b000] -> 11[3e000] via P2P/IPC
Tencent01: VM-0-14-centos:112:585 [6] NCCL INFO Ring 00 : 6[b1000] -> 7[b2000] via P2P/IPC
Tencent02: VM-0-9-centos:78:453 [3] NCCL INFO Ring 00 : 11[3e000] -> 10[3d000] via P2P/IPC
Tencent02: VM-0-9-centos:75:505 [0] NCCL INFO Ring 00 : 8[1a000] -> 9[1b000] via P2P/IPC
Tencent02: VM-0-9-centos:81:455 [6] NCCL INFO Ring 00 : 14[b1000] -> 15[b2000] via P2P/IPC
Tencent02: VM-0-9-centos:77:506 [2] NCCL INFO Ring 00 : 10[3d000] -> 13[89000] via P2P/IPC
Tencent01: VM-0-14-centos:113:584 [7] NCCL INFO Ring 00 : 7[b2000] -> 0[1a000] via P2P/IPC
Tencent01: VM-0-14-centos:106:581 [0] NCCL INFO Ring 00 : 0[1a000] -> 1[1b000] via P2P/IPC
Tencent01: VM-0-14-centos:109:587 [3] NCCL INFO Ring 00 : 3[3e000] -> 2[3d000] via P2P/IPC
Tencent02: VM-0-9-centos:79:456 [4] NCCL INFO Ring 00 : 5[89000] -> 12[88000] [receive] via NET/IB/0
Tencent01: VM-0-14-centos:110:582 [4] NCCL INFO Ring 00 : 13[89000] -> 4[88000] [receive] via NET/IB/0
Tencent02: VM-0-9-centos:79:456 [4] NCCL INFO Ring 00 : 12[88000] -> 14[b1000] via P2P/IPC
Tencent02: VM-0-9-centos:80:554 [5] NCCL INFO Ring 00 : 13[89000] -> 4[88000] [send] via NET/IB/0
Tencent01: VM-0-14-centos:111:588 [5] NCCL INFO Ring 00 : 5[89000] -> 12[88000] [send] via NET/IB/0
Tencent02: VM-0-9-centos:80:554 [5] NCCL INFO NCCL_IB_GID_INDEX set by environment to 3.
Tencent01: VM-0-14-centos:110:582 [4] NCCL INFO Ring 00 : 4[88000] -> 6[b1000] via P2P/IPC
Tencent01: VM-0-14-centos:111:588 [5] NCCL INFO NCCL_IB_GID_INDEX set by environment to 3.
Tencent02: VM-0-9-centos:79:456 [4] NCCL INFO NCCL_IB_GID_INDEX set by environment to 3.
Tencent01: VM-0-14-centos:108:586 [2] NCCL INFO Ring 00 : 2[3d000] -> 0[1a000] via P2P/IPC
Tencent02: VM-0-9-centos:77:506 [2] NCCL INFO Ring 00 : 10[3d000] -> 8[1a000] via P2P/IPC
Tencent01: VM-0-14-centos:110:582 [4] NCCL INFO NCCL_IB_GID_INDEX set by environment to 3.
Tencent02: VM-0-9-centos:81:455 [6] NCCL INFO Ring 00 : 14[b1000] -> 9[1b000] via P2P/IPC
Tencent02: VM-0-9-centos:82:454 [7] NCCL INFO Ring 00 : 15[b2000] -> 13[89000] via P2P/IPC
Tencent02: VM-0-9-centos:78:453 [3] NCCL INFO Ring 00 : 11[3e000] -> 9[1b000] via P2P/IPC
Tencent02: VM-0-9-centos:75:505 [0] NCCL INFO Ring 00 : 8[1a000] -> 15[b2000] via P2P/IPC
Tencent01: VM-0-14-centos:113:584 [7] NCCL INFO Ring 00 : 7[b2000] -> 5[89000] via P2P/IPC
Tencent01: VM-0-14-centos:112:585 [6] NCCL INFO Ring 00 : 6[b1000] -> 1[1b000] via P2P/IPC
Tencent01: VM-0-14-centos:109:587 [3] NCCL INFO Ring 00 : 3[3e000] -> 1[1b000] via P2P/IPC
Tencent01: VM-0-14-centos:106:581 [0] NCCL INFO Ring 00 : 0[1a000] -> 7[b2000] via P2P/IPC
Tencent02: VM-0-9-centos:80:554 [5] NCCL INFO Ring 00 : 13[89000] -> 12[88000] via P2P/IPC
Tencent02: VM-0-9-centos:77:506 [2] NCCL INFO Ring 00 : 10[3d000] -> 11[3e000] via P2P/IPC
Tencent02: VM-0-9-centos:76:457 [1] NCCL INFO Ring 00 : 9[1b000] -> 14[b1000] via P2P/IPC
Tencent02: VM-0-9-centos:79:456 [4] NCCL INFO Ring 00 : 12[88000] -> 5[89000] [send] via NET/IB/0
Tencent01: VM-0-14-centos:108:586 [2] NCCL INFO Ring 00 : 2[3d000] -> 3[3e000] via P2P/IPC
Tencent02: VM-0-9-centos:75:505 [0] NCCL INFO Ring 00 : 8[1a000] -> 10[3d000] via P2P/IPC
Tencent01: VM-0-14-centos:106:581 [0] NCCL INFO Ring 00 : 0[1a000] -> 2[3d000] via P2P/IPC
Tencent01: VM-0-14-centos:107:583 [1] NCCL INFO Ring 00 : 1[1b000] -> 6[b1000] via P2P/IPC
Tencent01: VM-0-14-centos:111:588 [5] NCCL INFO Ring 00 : 12[88000] -> 5[89000] [receive] via NET/IB/0
Tencent02: VM-0-9-centos:78:453 [3] NCCL INFO Ring 01 : 11[3e000] -> 10[3d000] via P2P/IPC
Tencent01: VM-0-14-centos:111:588 [5] NCCL INFO Ring 00 : 5[89000] -> 4[88000] via P2P/IPC
Tencent02: VM-0-9-centos:80:554 [5] NCCL INFO Ring 00 : 13[89000] -> 15[b2000] via P2P/IPC
Tencent02: VM-0-9-centos:81:455 [6] NCCL INFO Ring 01 : 14[b1000] -> 15[b2000] via P2P/IPC
Tencent02: VM-0-9-centos:75:505 [0] NCCL INFO Ring 01 : 8[1a000] -> 9[1b000] via P2P/IPC
Tencent02: VM-0-9-centos:77:506 [2] NCCL INFO Ring 01 : 10[3d000] -> 13[89000] via P2P/IPC
Tencent02: VM-0-9-centos:76:457 [1] NCCL INFO Ring 01 : 9[1b000] -> 11[3e000] via P2P/IPC
Tencent01: VM-0-14-centos:109:587 [3] NCCL INFO Ring 01 : 3[3e000] -> 2[3d000] via P2P/IPC
Tencent02: VM-0-9-centos:79:456 [4] NCCL INFO Ring 00 : 12[88000] -> 13[89000] via P2P/IPC
Tencent01: VM-0-14-centos:108:586 [2] NCCL INFO Ring 01 : 2[3d000] -> 5[89000] via P2P/IPC
Tencent01: VM-0-14-centos:106:581 [0] NCCL INFO Ring 01 : 0[1a000] -> 1[1b000] via P2P/IPC
Tencent01: VM-0-14-centos:112:585 [6] NCCL INFO Ring 01 : 6[b1000] -> 7[b2000] via P2P/IPC
Tencent01: VM-0-14-centos:110:582 [4] NCCL INFO Ring 00 : 4[88000] -> 5[89000] via P2P/IPC
Tencent01: VM-0-14-centos:107:583 [1] NCCL INFO Ring 01 : 1[1b000] -> 3[3e000] via P2P/IPC
Tencent02: VM-0-9-centos:82:454 [7] NCCL INFO Ring 01 : 15[b2000] -> 8[1a000] via P2P/IPC
Tencent02: VM-0-9-centos:78:453 [3] NCCL INFO Ring 01 : 11[3e000] -> 9[1b000] via P2P/IPC
Tencent01: VM-0-14-centos:109:587 [3] NCCL INFO Ring 01 : 3[3e000] -> 1[1b000] via P2P/IPC
Tencent01: VM-0-14-centos:111:588 [5] NCCL INFO Ring 00 : 5[89000] -> 7[b2000] via P2P/IPC
Tencent02: VM-0-9-centos:79:456 [4] NCCL INFO Ring 01 : 5[89000] -> 12[88000] [receive] via NET/IB/0
Tencent02: VM-0-9-centos:77:506 [2] NCCL INFO Ring 01 : 10[3d000] -> 8[1a000] via P2P/IPC
Tencent02: VM-0-9-centos:75:505 [0] NCCL INFO Ring 01 : 8[1a000] -> 15[b2000] via P2P/IPC
Tencent02: VM-0-9-centos:82:454 [7] NCCL INFO Ring 01 : 15[b2000] -> 13[89000] via P2P/IPC
Tencent02: VM-0-9-centos:79:456 [4] NCCL INFO Ring 01 : 12[88000] -> 14[b1000] via P2P/IPC
Tencent01: VM-0-14-centos:113:584 [7] NCCL INFO Ring 01 : 7[b2000] -> 0[1a000] via P2P/IPC
Tencent02: VM-0-9-centos:80:554 [5] NCCL INFO Ring 01 : 13[89000] -> 4[88000] [send] via NET/IB/0
Tencent01: VM-0-14-centos:108:586 [2] NCCL INFO Ring 01 : 2[3d000] -> 0[1a000] via P2P/IPC
Tencent02: VM-0-9-centos:75:505 [0] NCCL INFO Ring 01 : 8[1a000] -> 10[3d000] via P2P/IPC
Tencent02: VM-0-9-centos:81:455 [6] NCCL INFO Ring 01 : 14[b1000] -> 9[1b000] via P2P/IPC
Tencent02: VM-0-9-centos:77:506 [2] NCCL INFO Ring 01 : 10[3d000] -> 11[3e000] via P2P/IPC
Tencent02: VM-0-9-centos:75:505 [0] NCCL INFO comm 0x7f9a50001020 rank 8 nranks 16 cudaDev 0 busId 1a000 - Init COMPLETE
Tencent01: VM-0-14-centos:110:582 [4] NCCL INFO Ring 01 : 13[89000] -> 4[88000] [receive] via NET/IB/0
Tencent01: VM-0-14-centos:111:588 [5] NCCL INFO Ring 01 : 5[89000] -> 12[88000] [send] via NET/IB/0
Tencent01: VM-0-14-centos:106:581 [0] NCCL INFO Ring 01 : 0[1a000] -> 7[b2000] via P2P/IPC
Tencent01: VM-0-14-centos:110:582 [4] NCCL INFO Ring 01 : 4[88000] -> 6[b1000] via P2P/IPC
Tencent02: VM-0-9-centos:77:506 [2] NCCL INFO comm 0x7ff5f4001020 rank 10 nranks 16 cudaDev 2 busId 3d000 - Init COMPLETE
Tencent01: VM-0-14-centos:113:584 [7] NCCL INFO Ring 01 : 7[b2000] -> 5[89000] via P2P/IPC
Tencent01: VM-0-14-centos:108:586 [2] NCCL INFO Ring 01 : 2[3d000] -> 3[3e000] via P2P/IPC
Tencent02: VM-0-9-centos:76:457 [1] NCCL INFO Ring 01 : 9[1b000] -> 14[b1000] via P2P/IPC
Tencent02: VM-0-9-centos:78:453 [3] NCCL INFO comm 0x7fa34c001020 rank 11 nranks 16 cudaDev 3 busId 3e000 - Init COMPLETE
Tencent01: VM-0-14-centos:112:585 [6] NCCL INFO Ring 01 : 6[b1000] -> 1[1b000] via P2P/IPC
Tencent01: VM-0-14-centos:106:581 [0] NCCL INFO Ring 01 : 0[1a000] -> 2[3d000] via P2P/IPC
Tencent02: VM-0-9-centos:81:455 [6] NCCL INFO comm 0x7fa6d0001020 rank 14 nranks 16 cudaDev 6 busId b1000 - Init COMPLETE
Tencent01: VM-0-14-centos:108:586 [2] NCCL INFO comm 0x7f6718001020 rank 2 nranks 16 cudaDev 2 busId 3d000 - Init COMPLETE
Tencent02: VM-0-9-centos:76:457 [1] NCCL INFO comm 0x7ff300001020 rank 9 nranks 16 cudaDev 1 busId 1b000 - Init COMPLETE
Tencent01: VM-0-14-centos:111:588 [5] NCCL INFO Ring 01 : 5[89000] -> 4[88000] via P2P/IPC
Tencent01: VM-0-14-centos:106:581 [0] NCCL INFO comm 0x7f009c001020 rank 0 nranks 16 cudaDev 0 busId 1a000 - Init COMPLETE
Tencent01: VM-0-14-centos:106:106 [0] NCCL INFO Launch mode Parallel
Tencent02: VM-0-9-centos:80:554 [5] NCCL INFO Ring 01 : 4[88000] -> 13[89000] [receive] via NET/IB/0
Tencent01: VM-0-14-centos:107:583 [1] NCCL INFO Ring 01 : 1[1b000] -> 6[b1000] via P2P/IPC
Tencent01: VM-0-14-centos:109:587 [3] NCCL INFO comm 0x7f8758001020 rank 3 nranks 16 cudaDev 3 busId 3e000 - Init COMPLETE
Tencent02: VM-0-9-centos:80:554 [5] NCCL INFO Ring 01 : 13[89000] -> 12[88000] via P2P/IPC
Tencent01: VM-0-14-centos:110:582 [4] NCCL INFO Ring 01 : 4[88000] -> 13[89000] [send] via NET/IB/0
Tencent01: VM-0-14-centos:112:585 [6] NCCL INFO comm 0x7f5910001020 rank 6 nranks 16 cudaDev 6 busId b1000 - Init COMPLETE
Tencent01: VM-0-14-centos:107:583 [1] NCCL INFO comm 0x7f2ce8001020 rank 1 nranks 16 cudaDev 1 busId 1b000 - Init COMPLETE
Tencent02: VM-0-9-centos:79:456 [4] NCCL INFO Ring 01 : 12[88000] -> 13[89000] via P2P/IPC
Tencent01: VM-0-14-centos:111:588 [5] NCCL INFO Ring 01 : 5[89000] -> 7[b2000] via P2P/IPC
Tencent01: VM-0-14-centos:110:582 [4] NCCL INFO Ring 01 : 4[88000] -> 5[89000] via P2P/IPC
Tencent02: VM-0-9-centos:80:554 [5] NCCL INFO Ring 01 : 13[89000] -> 15[b2000] via P2P/IPC
Tencent02: VM-0-9-centos:79:456 [4] NCCL INFO comm 0x7ef8f8001020 rank 12 nranks 16 cudaDev 4 busId 88000 - Init COMPLETE
Tencent01: VM-0-14-centos:113:584 [7] NCCL INFO comm 0x7f1158001020 rank 7 nranks 16 cudaDev 7 busId b2000 - Init COMPLETE
Tencent01: VM-0-14-centos:110:582 [4] NCCL INFO comm 0x7f162c001020 rank 4 nranks 16 cudaDev 4 busId 88000 - Init COMPLETE
Tencent01: VM-0-14-centos:111:588 [5] NCCL INFO comm 0x7f076c001020 rank 5 nranks 16 cudaDev 5 busId 89000 - Init COMPLETE
Tencent02: VM-0-9-centos:82:454 [7] NCCL INFO comm 0x7f1404001020 rank 15 nranks 16 cudaDev 7 busId b2000 - Init COMPLETE
Tencent02: VM-0-9-centos:80:554 [5] NCCL INFO comm 0x7f6560001020 rank 13 nranks 16 cudaDev 5 busId 89000 - Init COMPLETE
Tencent01: DeepSpeed is enabled.
Tencent01: [2020-11-11 06:42:28,378] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed info: version=0.3.0+1afca8f, git-hash=1afca8f, git-branch=HEAD
Tencent01: [2020-11-11 06:42:28,600] [INFO] [engine.py:514:_configure_optimizer] Using DeepSpeed Optimizer param name adam as basic optimizer
Tencent01: [2020-11-11 06:42:28,600] [INFO] [engine.py:517:_configure_optimizer] DeepSpeed Basic Optimizer = FusedAdam (
Tencent01: Parameter Group 0
Tencent01:     betas: (0.9, 0.999)
Tencent01:     bias_correction: True
Tencent01:     eps: 1e-08
Tencent01:     lr: 0.00015
Tencent01:     weight_decay: 0.01
Tencent01: 
Tencent01: Parameter Group 1
Tencent01:     betas: (0.9, 0.999)
Tencent01:     bias_correction: True
Tencent01:     eps: 1e-08
Tencent01:     lr: 0.00015
Tencent01:     weight_decay: 0.0
Tencent01: )
Tencent01: [2020-11-11 06:42:28,600] [INFO] [engine.py:579:_configure_fp16_optimizer] Creating fp16 optimizer with dynamic loss scale
Tencent01: [2020-11-11 06:42:28,600] [INFO] [engine.py:579:_configure_fp16_optimizer] Creating fp16 optimizer with dynamic loss scale
Tencent01: [2020-11-11 06:42:28,601] [INFO] [engine.py:579:_configure_fp16_optimizer] Creating fp16 optimizer with dynamic loss scale
Tencent01: [2020-11-11 06:42:28,602] [INFO] [engine.py:579:_configure_fp16_optimizer] Creating fp16 optimizer with dynamic loss scale
Tencent01: [2020-11-11 06:42:28,602] [INFO] [engine.py:579:_configure_fp16_optimizer] Creating fp16 optimizer with dynamic loss scale
Tencent02: [2020-11-11 06:42:28,603] [INFO] [engine.py:579:_configure_fp16_optimizer] Creating fp16 optimizer with dynamic loss scale
Tencent02: [2020-11-11 06:42:28,603] [INFO] [engine.py:579:_configure_fp16_optimizer] Creating fp16 optimizer with dynamic loss scale
Tencent02: [2020-11-11 06:42:28,603] [INFO] [engine.py:579:_configure_fp16_optimizer] Creating fp16 optimizer with dynamic loss scale
Tencent02: [2020-11-11 06:42:28,603] [INFO] [engine.py:579:_configure_fp16_optimizer] Creating fp16 optimizer with dynamic loss scale
Tencent02: [2020-11-11 06:42:28,603] [INFO] [engine.py:579:_configure_fp16_optimizer] Creating fp16 optimizer with dynamic loss scale
Tencent02: [2020-11-11 06:42:28,603] [INFO] [engine.py:579:_configure_fp16_optimizer] Creating fp16 optimizer with dynamic loss scale
Tencent02: [2020-11-11 06:42:28,603] [INFO] [engine.py:579:_configure_fp16_optimizer] Creating fp16 optimizer with dynamic loss scale
Tencent02: [2020-11-11 06:42:28,603] [INFO] [engine.py:579:_configure_fp16_optimizer] Creating fp16 optimizer with dynamic loss scale
Tencent01: [2020-11-11 06:42:28,605] [INFO] [engine.py:579:_configure_fp16_optimizer] Creating fp16 optimizer with dynamic loss scale
Tencent01: [2020-11-11 06:42:28,605] [INFO] [engine.py:579:_configure_fp16_optimizer] Creating fp16 optimizer with dynamic loss scale
Tencent01: [2020-11-11 06:42:28,605] [INFO] [engine.py:579:_configure_fp16_optimizer] Creating fp16 optimizer with dynamic loss scale
Tencent01: [2020-11-11 06:42:29,053] [INFO] [engine.py:541:_configure_optimizer] DeepSpeed Final Optimizer = FusedAdam (
Tencent01: Parameter Group 0
Tencent01:     betas: (0.9, 0.999)
Tencent01:     bias_correction: True
Tencent01:     eps: 1e-08
Tencent01:     lr: 0.00015
Tencent01:     step: 1
Tencent01:     weight_decay: 0.01
Tencent01: 
Tencent01: Parameter Group 1
Tencent01:     betas: (0.9, 0.999)
Tencent01:     bias_correction: True
Tencent01:     eps: 1e-08
Tencent01:     lr: 0.00015
Tencent01:     step: 1
Tencent01:     weight_decay: 0.0
Tencent01: )
Tencent01: [2020-11-11 06:42:29,059] [INFO] [engine.py:541:_configure_optimizer] DeepSpeed Final Optimizer = FusedAdam (
Tencent01: Parameter Group 0
Tencent01:     betas: (0.9, 0.999)
Tencent01:     bias_correction: True
Tencent01:     eps: 1e-08
Tencent01:     lr: 0.00015
Tencent01:     step: 1
Tencent01:     weight_decay: 0.01
Tencent01: 
Tencent01: Parameter Group 1
Tencent01:     betas: (0.9, 0.999)
Tencent01:     bias_correction: True
Tencent01:     eps: 1e-08
Tencent01:     lr: 0.00015
Tencent01:     step: 1
Tencent01:     weight_decay: 0.0
Tencent01: )
Tencent01: [2020-11-11 06:42:29,101] [INFO] [engine.py:542:_configure_optimizer] DeepSpeed Final Optimizer = {'dynamic_loss_scale': True, 'cur_scale': 4294967296, 'cur_iter': 0, 'last_overflow_iter': -1, 'scale_factor': 2, 'scale_window': 1000, 'optimizer_state_dict': {'state': {0: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:1'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:1')}, 1: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:1'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:1')}}, 'param_groups': [{'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'step': 1, 'params': [0]}, {'weight_decay': 0.0, 'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'step': 1, 'params': [1]}]}, 'fp32_groups_flat': [tensor([-0.0022, -0.0099,  0.0033,  ..., -0.0024,  0.0022, -0.0027],
Tencent01:        device='cuda:1', requires_grad=True), tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:1', requires_grad=True)], 'clip_grad': 1.0}
Tencent01: [2020-11-11 06:42:29,107] [INFO] [engine.py:542:_configure_optimizer] DeepSpeed Final Optimizer = {'dynamic_loss_scale': True, 'cur_scale': 4294967296, 'cur_iter': 0, 'last_overflow_iter': -1, 'scale_factor': 2, 'scale_window': 1000, 'optimizer_state_dict': {'state': {0: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 1: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}}, 'param_groups': [{'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'step': 1, 'params': [0]}, {'weight_decay': 0.0, 'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'step': 1, 'params': [1]}]}, 'fp32_groups_flat': [tensor([-0.0022, -0.0099,  0.0033,  ..., -0.0024,  0.0022, -0.0027],
Tencent01:        device='cuda:0', requires_grad=True), tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:0', requires_grad=True)], 'clip_grad': 1.0}
Tencent01: [2020-11-11 06:42:29,107] [INFO] [engine.py:381:_configure_lr_scheduler] DeepSpeed using client LR scheduler
Tencent01: [2020-11-11 06:42:29,107] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
Tencent01: [2020-11-11 06:42:29,107] [INFO] [logging.py:60:log_dist] [Rank 0] step=0, skipped=0, lr=[0.00015, 0.00015], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 06:42:29,107] [INFO] [config.py:620:print] DeepSpeedLight configuration:
Tencent01: [2020-11-11 06:42:29,107] [INFO] [config.py:624:print]   activation_checkpointing_config  <deepspeed.runtime.activation_checkpointing.config.DeepSpeedActivationCheckpointingConfig object at 0x7f04dd5ac850>
Tencent01: [2020-11-11 06:42:29,107] [INFO] [config.py:624:print]   allreduce_always_fp32 ........ False
Tencent01: [2020-11-11 06:42:29,107] [INFO] [config.py:624:print]   amp_enabled .................. False
Tencent01: [2020-11-11 06:42:29,107] [INFO] [config.py:624:print]   amp_params ................... False
Tencent01: [2020-11-11 06:42:29,107] [INFO] [config.py:624:print]   disable_allgather ............ False
Tencent01: [2020-11-11 06:42:29,107] [INFO] [config.py:624:print]   dump_state ................... False
Tencent01: [2020-11-11 06:42:29,107] [INFO] [config.py:624:print]   dynamic_loss_scale_args ...... {'init_scale': 4294967296, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}
Tencent01: [2020-11-11 06:42:29,107] [INFO] [config.py:624:print]   fp16_enabled ................. True
Tencent01: [2020-11-11 06:42:29,107] [INFO] [config.py:624:print]   global_rank .................. 0
Tencent01: [2020-11-11 06:42:29,107] [INFO] [config.py:624:print]   gradient_accumulation_steps .. 1
Tencent01: [2020-11-11 06:42:29,107] [INFO] [config.py:624:print]   gradient_clipping ............ 1.0
Tencent01: [2020-11-11 06:42:29,107] [INFO] [config.py:624:print]   gradient_predivide_factor .... 1.0
Tencent01: [2020-11-11 06:42:29,107] [INFO] [config.py:624:print]   initial_dynamic_scale ........ 4294967296
Tencent01: [2020-11-11 06:42:29,108] [INFO] [config.py:624:print]   loss_scale ................... 0
Tencent01: [2020-11-11 06:42:29,108] [INFO] [config.py:624:print]   memory_breakdown ............. False
Tencent01: [2020-11-11 06:42:29,108] [INFO] [config.py:624:print]   optimizer_legacy_fusion ...... False
Tencent01: [2020-11-11 06:42:29,108] [INFO] [config.py:624:print]   optimizer_name ............... adam
Tencent01: [2020-11-11 06:42:29,108] [INFO] [config.py:624:print]   optimizer_params ............. {'lr': 0.00015, 'weight_decay': 0.01}
Tencent01: [2020-11-11 06:42:29,108] [INFO] [config.py:624:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
Tencent01: [2020-11-11 06:42:29,108] [INFO] [config.py:624:print]   prescale_gradients ........... False
Tencent01: [2020-11-11 06:42:29,108] [INFO] [config.py:624:print]   scheduler_name ............... None
Tencent01: [2020-11-11 06:42:29,108] [INFO] [config.py:624:print]   scheduler_params ............. None
Tencent01: [2020-11-11 06:42:29,108] [INFO] [config.py:624:print]   sparse_attention ............. None
Tencent01: [2020-11-11 06:42:29,108] [INFO] [config.py:624:print]   sparse_gradients_enabled ..... False
Tencent01: [2020-11-11 06:42:29,108] [INFO] [config.py:624:print]   steps_per_print .............. 100
Tencent01: [2020-11-11 06:42:29,108] [INFO] [config.py:624:print]   tensorboard_enabled .......... False
Tencent01: [2020-11-11 06:42:29,108] [INFO] [config.py:624:print]   tensorboard_job_name ......... DeepSpeedJobName
Tencent01: [2020-11-11 06:42:29,108] [INFO] [config.py:624:print]   tensorboard_output_path ...... 
Tencent01: [2020-11-11 06:42:29,108] [INFO] [config.py:624:print]   train_batch_size ............. 128
Tencent01: [2020-11-11 06:42:29,108] [INFO] [config.py:624:print]   train_micro_batch_size_per_gpu  8
Tencent01: [2020-11-11 06:42:29,108] [INFO] [config.py:624:print]   wall_clock_breakdown ......... False
Tencent01: [2020-11-11 06:42:29,108] [INFO] [config.py:624:print]   world_size ................... 16
Tencent01: [2020-11-11 06:42:29,108] [INFO] [config.py:624:print]   zero_allow_untested_optimizer  False
Tencent01: [2020-11-11 06:42:29,108] [INFO] [config.py:624:print]   zero_config .................. <deepspeed.runtime.zero.config.DeepSpeedZeroConfig object at 0x7f04dd5ac8d0>
Tencent01: [2020-11-11 06:42:29,108] [INFO] [config.py:624:print]   zero_enabled ................. False
Tencent01: [2020-11-11 06:42:29,108] [INFO] [config.py:624:print]   zero_optimization_stage ...... 0
Tencent01: [2020-11-11 06:42:29,108] [INFO] [config.py:630:print]   json = {
Tencent01:     "activation_checkpointing":{
Tencent01:         "contiguous_memory_optimization":false,
Tencent01:         "partition_activations":false
Tencent01:     },
Tencent01:     "fp16":{
Tencent01:         "enabled":true,
Tencent01:         "hysteresis":2,
Tencent01:         "loss_scale":0,
Tencent01:         "loss_scale_window":1000,
Tencent01:         "min_loss_scale":1
Tencent01:     },
Tencent01:     "gradient_accumulation_steps":1,
Tencent01:     "gradient_clipping":1.0,
Tencent01:     "optimizer":{
Tencent01:         "params":{
Tencent01:             "lr":0.00015,
Tencent01:             "weight_decay":0.01
Tencent01:         },
Tencent01:         "type":"Adam"
Tencent01:     },
Tencent01:     "steps_per_print":100,
Tencent01:     "train_micro_batch_size_per_gpu":8,
Tencent01:     "wall_clock_breakdown":false
Tencent01: }
Tencent01: learning rate decaying cosine
Tencent01: [2020-11-11 06:42:29,110] [INFO] [engine.py:541:_configure_optimizer] DeepSpeed Final Optimizer = FusedAdam (
Tencent01: Parameter Group 0
Tencent01:     betas: (0.9, 0.999)
Tencent01:     bias_correction: True
Tencent01:     eps: 1e-08
Tencent01:     lr: 0.00015
Tencent01:     step: 1
Tencent01:     weight_decay: 0.01
Tencent01: 
Tencent01: Parameter Group 1
Tencent01:     betas: (0.9, 0.999)
Tencent01:     bias_correction: True
Tencent01:     eps: 1e-08
Tencent01:     lr: 0.00015
Tencent01:     step: 1
Tencent01:     weight_decay: 0.0
Tencent01: )
Tencent02: [2020-11-11 06:42:29,110] [INFO] [engine.py:541:_configure_optimizer] DeepSpeed Final Optimizer = FusedAdam (
Tencent02: Parameter Group 0
Tencent02:     betas: (0.9, 0.999)
Tencent02:     bias_correction: True
Tencent02:     eps: 1e-08
Tencent02:     lr: 0.00015
Tencent02:     step: 1
Tencent02:     weight_decay: 0.01
Tencent02: 
Tencent02: Parameter Group 1
Tencent02:     betas: (0.9, 0.999)
Tencent02:     bias_correction: True
Tencent02:     eps: 1e-08
Tencent02:     lr: 0.00015
Tencent02:     step: 1
Tencent02:     weight_decay: 0.0
Tencent02: )
Tencent01: [2020-11-11 06:42:29,110] [INFO] [engine.py:541:_configure_optimizer] DeepSpeed Final Optimizer = FusedAdam (
Tencent01: Parameter Group 0
Tencent01:     betas: (0.9, 0.999)
Tencent01:     bias_correction: True
Tencent01:     eps: 1e-08
Tencent01:     lr: 0.00015
Tencent01:     step: 1
Tencent01:     weight_decay: 0.01
Tencent01: 
Tencent01: Parameter Group 1
Tencent01:     betas: (0.9, 0.999)
Tencent01:     bias_correction: True
Tencent01:     eps: 1e-08
Tencent01:     lr: 0.00015
Tencent01:     step: 1
Tencent02: [2020-11-11 06:42:29,110] [INFO] [engine.py:541:_configure_optimizer] DeepSpeed Final Optimizer = FusedAdam (
Tencent01:     weight_decay: 0.0
Tencent01: )
Tencent02: Parameter Group 0
Tencent02:     betas: (0.9, 0.999)
Tencent02:     bias_correction: True
Tencent02:     eps: 1e-08
Tencent02:     lr: 0.00015
Tencent02:     step: 1
Tencent02:     weight_decay: 0.01
Tencent02: 
Tencent02: Parameter Group 1
Tencent02:     betas: (0.9, 0.999)
Tencent02:     bias_correction: True
Tencent02:     eps: 1e-08
Tencent02:     lr: 0.00015
Tencent02:     step: 1
Tencent02:     weight_decay: 0.0
Tencent02: )
Tencent01: [2020-11-11 06:42:29,110] [INFO] [engine.py:541:_configure_optimizer] DeepSpeed Final Optimizer = FusedAdam (
Tencent01: Parameter Group 0
Tencent01:     betas: (0.9, 0.999)
Tencent01:     bias_correction: True
Tencent01:     eps: 1e-08
Tencent01:     lr: 0.00015
Tencent01:     step: 1
Tencent01:     weight_decay: 0.01
Tencent01: 
Tencent01: Parameter Group 1
Tencent01:     betas: (0.9, 0.999)
Tencent01:     bias_correction: True
Tencent01:     eps: 1e-08
Tencent01:     lr: 0.00015
Tencent01:     step: 1
Tencent01:     weight_decay: 0.0
Tencent01: )
Tencent01: [2020-11-11 06:42:29,111] [INFO] [engine.py:541:_configure_optimizer] DeepSpeed Final Optimizer = FusedAdam (
Tencent01: Parameter Group 0
Tencent01:     betas: (0.9, 0.999)
Tencent01:     bias_correction: True
Tencent01:     eps: 1e-08
Tencent01:     lr: 0.00015
Tencent01:     step: 1
Tencent01:     weight_decay: 0.01
Tencent01: 
Tencent01: Parameter Group 1
Tencent01:     betas: (0.9, 0.999)
Tencent01:     bias_correction: True
Tencent01:     eps: 1e-08
Tencent01:     lr: 0.00015
Tencent01:     step: 1
Tencent01:     weight_decay: 0.0
Tencent01: )
Tencent02: [2020-11-11 06:42:29,110] [INFO] [engine.py:541:_configure_optimizer] DeepSpeed Final Optimizer = FusedAdam (
Tencent02: Parameter Group 0
Tencent02:     betas: (0.9, 0.999)
Tencent02:     bias_correction: True
Tencent02:     eps: 1e-08
Tencent02:     lr: 0.00015
Tencent02:     step: 1
Tencent02:     weight_decay: 0.01
Tencent02: 
Tencent02: Parameter Group 1
Tencent02:     betas: (0.9, 0.999)
Tencent02:     bias_correction: True
Tencent02:     eps: 1e-08
Tencent02:     lr: 0.00015
Tencent02:     step: 1
Tencent02:     weight_decay: 0.0
Tencent02: )
Tencent01: [2020-11-11 06:42:29,111] [INFO] [engine.py:541:_configure_optimizer] DeepSpeed Final Optimizer = FusedAdam (
Tencent01: Parameter Group 0
Tencent01:     betas: (0.9, 0.999)
Tencent01:     bias_correction: True
Tencent01:     eps: 1e-08
Tencent01:     lr: 0.00015
Tencent01:     step: 1
Tencent01:     weight_decay: 0.01
Tencent01: 
Tencent01: Parameter Group 1
Tencent01:     betas: (0.9, 0.999)
Tencent01:     bias_correction: True
Tencent01:     eps: 1e-08
Tencent01:     lr: 0.00015
Tencent01:     step: 1
Tencent01:     weight_decay: 0.0
Tencent01: )
Tencent02: [2020-11-11 06:42:29,110] [INFO] [engine.py:541:_configure_optimizer] DeepSpeed Final Optimizer = FusedAdam (
Tencent02: Parameter Group 0
Tencent02:     betas: (0.9, 0.999)
Tencent02:     bias_correction: True
Tencent02:     eps: 1e-08
Tencent02:     lr: 0.00015
Tencent02:     step: 1
Tencent02:     weight_decay: 0.01
Tencent02: 
Tencent02: Parameter Group 1
Tencent02:     betas: (0.9, 0.999)
Tencent02:     bias_correction: True
Tencent02:     eps: 1e-08
Tencent02:     lr: 0.00015
Tencent02:     step: 1
Tencent02:     weight_decay: 0.0
Tencent02: )
Tencent01: [2020-11-11 06:42:29,111] [INFO] [engine.py:541:_configure_optimizer] DeepSpeed Final Optimizer = FusedAdam (
Tencent01: Parameter Group 0
Tencent01:     betas: (0.9, 0.999)
Tencent01:     bias_correction: True
Tencent01:     eps: 1e-08
Tencent01:     lr: 0.00015
Tencent01:     step: 1
Tencent01:     weight_decay: 0.01
Tencent01: 
Tencent01: Parameter Group 1
Tencent01:     betas: (0.9, 0.999)
Tencent01:     bias_correction: True
Tencent01:     eps: 1e-08
Tencent01:     lr: 0.00015
Tencent01:     step: 1
Tencent01:     weight_decay: 0.0
Tencent01: )
Tencent02: [2020-11-11 06:42:29,110] [INFO] [engine.py:541:_configure_optimizer] DeepSpeed Final Optimizer = FusedAdam (
Tencent02: Parameter Group 0
Tencent02:     betas: (0.9, 0.999)
Tencent02:     bias_correction: True
Tencent02:     eps: 1e-08
Tencent02:     lr: 0.00015
Tencent02:     step: 1
Tencent02:     weight_decay: 0.01
Tencent02: 
Tencent02: Parameter Group 1
Tencent02:     betas: (0.9, 0.999)
Tencent02:     bias_correction: True
Tencent02:     eps: 1e-08
Tencent02:     lr: 0.00015
Tencent02:     step: 1
Tencent02:     weight_decay: 0.0
Tencent02: )
Tencent02: [2020-11-11 06:42:29,111] [INFO] [engine.py:541:_configure_optimizer] DeepSpeed Final Optimizer = FusedAdam (
Tencent02: Parameter Group 0
Tencent02:     betas: (0.9, 0.999)
Tencent02:     bias_correction: True
Tencent02:     eps: 1e-08
Tencent02:     lr: 0.00015
Tencent02:     step: 1
Tencent02:     weight_decay: 0.01
Tencent02: 
Tencent02: Parameter Group 1
Tencent02:     betas: (0.9, 0.999)
Tencent02:     bias_correction: True
Tencent02:     eps: 1e-08
Tencent02:     lr: 0.00015
Tencent02:     step: 1
Tencent02:     weight_decay: 0.0
Tencent02: )
Tencent02: [2020-11-11 06:42:29,111] [INFO] [engine.py:541:_configure_optimizer] DeepSpeed Final Optimizer = FusedAdam (
Tencent02: Parameter Group 0
Tencent02:     betas: (0.9, 0.999)
Tencent02:     bias_correction: True
Tencent02:     eps: 1e-08
Tencent02:     lr: 0.00015
Tencent02:     step: 1
Tencent02:     weight_decay: 0.01
Tencent02: 
Tencent02: Parameter Group 1
Tencent02:     betas: (0.9, 0.999)
Tencent02:     bias_correction: True
Tencent02:     eps: 1e-08
Tencent02:     lr: 0.00015
Tencent02:     step: 1
Tencent02:     weight_decay: 0.0
Tencent02: )
Tencent02: [2020-11-11 06:42:29,111] [INFO] [engine.py:541:_configure_optimizer] DeepSpeed Final Optimizer = FusedAdam (
Tencent02: Parameter Group 0
Tencent02:     betas: (0.9, 0.999)
Tencent02:     bias_correction: True
Tencent02:     eps: 1e-08
Tencent02:     lr: 0.00015
Tencent02:     step: 1
Tencent02:     weight_decay: 0.01
Tencent02: 
Tencent02: Parameter Group 1
Tencent02:     betas: (0.9, 0.999)
Tencent02:     bias_correction: True
Tencent02:     eps: 1e-08
Tencent02:     lr: 0.00015
Tencent02:     step: 1
Tencent02:     weight_decay: 0.0
Tencent02: )
Tencent02: [2020-11-11 06:42:29,157] [INFO] [engine.py:542:_configure_optimizer] DeepSpeed Final Optimizer = {'dynamic_loss_scale': True, 'cur_scale': 4294967296, 'cur_iter': 0, 'last_overflow_iter': -1, 'scale_factor': 2, 'scale_window': 1000, 'optimizer_state_dict': {'state': {0: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:4'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:4')}, 1: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:4'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:4')}}, 'param_groups': [{'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'step': 1, 'params': [0]}, {'weight_decay': 0.0, 'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'step': 1, 'params': [1]}]}, 'fp32_groups_flat': [tensor([-0.0022, -0.0099,  0.0033,  ..., -0.0024,  0.0022, -0.0027],
Tencent02:        device='cuda:4', requires_grad=True), tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:4', requires_grad=True)], 'clip_grad': 1.0}
Tencent02: [2020-11-11 06:42:29,157] [INFO] [engine.py:542:_configure_optimizer] DeepSpeed Final Optimizer = {'dynamic_loss_scale': True, 'cur_scale': 4294967296, 'cur_iter': 0, 'last_overflow_iter': -1, 'scale_factor': 2, 'scale_window': 1000, 'optimizer_state_dict': {'state': {0: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:2'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:2')}, 1: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:2'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:2')}}, 'param_groups': [{'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'step': 1, 'params': [0]}, {'weight_decay': 0.0, 'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'step': 1, 'params': [1]}]}, 'fp32_groups_flat': [tensor([-0.0022, -0.0099,  0.0033,  ..., -0.0024,  0.0022, -0.0027],
Tencent02:        device='cuda:2', requires_grad=True), tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:2', requires_grad=True)], 'clip_grad': 1.0}
Tencent02: [2020-11-11 06:42:29,158] [INFO] [engine.py:542:_configure_optimizer] DeepSpeed Final Optimizer = {'dynamic_loss_scale': True, 'cur_scale': 4294967296, 'cur_iter': 0, 'last_overflow_iter': -1, 'scale_factor': 2, 'scale_window': 1000, 'optimizer_state_dict': {'state': {0: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:7'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:7')}, 1: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:7'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:7')}}, 'param_groups': [{'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'step': 1, 'params': [0]}, {'weight_decay': 0.0, 'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'step': 1, 'params': [1]}]}, 'fp32_groups_flat': [tensor([-0.0022, -0.0099,  0.0033,  ..., -0.0024,  0.0022, -0.0027],
Tencent02:        device='cuda:7', requires_grad=True), tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:7', requires_grad=True)], 'clip_grad': 1.0}
Tencent01: [2020-11-11 06:42:29,158] [INFO] [engine.py:542:_configure_optimizer] DeepSpeed Final Optimizer = {'dynamic_loss_scale': True, 'cur_scale': 4294967296, 'cur_iter': 0, 'last_overflow_iter': -1, 'scale_factor': 2, 'scale_window': 1000, 'optimizer_state_dict': {'state': {0: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:4'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:4')}, 1: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:4'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:4')}}, 'param_groups': [{'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'step': 1, 'params': [0]}, {'weight_decay': 0.0, 'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'step': 1, 'params': [1]}]}, 'fp32_groups_flat': [tensor([-0.0022, -0.0099,  0.0033,  ..., -0.0024,  0.0022, -0.0027],
Tencent01:        device='cuda:4', requires_grad=True), tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:4', requires_grad=True)], 'clip_grad': 1.0}
Tencent01: [2020-11-11 06:42:29,158] [INFO] [engine.py:542:_configure_optimizer] DeepSpeed Final Optimizer = {'dynamic_loss_scale': True, 'cur_scale': 4294967296, 'cur_iter': 0, 'last_overflow_iter': -1, 'scale_factor': 2, 'scale_window': 1000, 'optimizer_state_dict': {'state': {0: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:5'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:5')}, 1: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:5'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:5')}}, 'param_groups': [{'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'step': 1, 'params': [0]}, {'weight_decay': 0.0, 'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'step': 1, 'params': [1]}]}, 'fp32_groups_flat': [tensor([-0.0022, -0.0099,  0.0033,  ..., -0.0024,  0.0022, -0.0027],
Tencent01:        device='cuda:5', requires_grad=True), tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:5', requires_grad=True)], 'clip_grad': 1.0}
Tencent01: [2020-11-11 06:42:29,158] [INFO] [engine.py:542:_configure_optimizer] DeepSpeed Final Optimizer = {'dynamic_loss_scale': True, 'cur_scale': 4294967296, 'cur_iter': 0, 'last_overflow_iter': -1, 'scale_factor': 2, 'scale_window': 1000, 'optimizer_state_dict': {'state': {0: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:7'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:7')}, 1: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:7'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:7')}}, 'param_groups': [{'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'step': 1, 'params': [0]}, {'weight_decay': 0.0, 'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'step': 1, 'params': [1]}]}, 'fp32_groups_flat': [tensor([-0.0022, -0.0099,  0.0033,  ..., -0.0024,  0.0022, -0.0027],
Tencent02: [2020-11-11 06:42:29,158] [INFO] [engine.py:542:_configure_optimizer] DeepSpeed Final Optimizer = {'dynamic_loss_scale': True, 'cur_scale': 4294967296, 'cur_iter': 0, 'last_overflow_iter': -1, 'scale_factor': 2, 'scale_window': 1000, 'optimizer_state_dict': {'state': {0: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:1'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:1')}, 1: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:1'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:1')}}, 'param_groups': [{'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'step': 1, 'params': [0]}, {'weight_decay': 0.0, 'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'step': 1, 'params': [1]}]}, 'fp32_groups_flat': [tensor([-0.0022, -0.0099,  0.0033,  ..., -0.0024,  0.0022, -0.0027],
Tencent01:        device='cuda:7', requires_grad=True), tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:7', requires_grad=True)], 'clip_grad': 1.0}
Tencent02:        device='cuda:1', requires_grad=True), tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:1', requires_grad=True)], 'clip_grad': 1.0}
Tencent02: [2020-11-11 06:42:29,158] [INFO] [engine.py:542:_configure_optimizer] DeepSpeed Final Optimizer = {'dynamic_loss_scale': True, 'cur_scale': 4294967296, 'cur_iter': 0, 'last_overflow_iter': -1, 'scale_factor': 2, 'scale_window': 1000, 'optimizer_state_dict': {'state': {0: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:3'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:3')}, 1: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:3'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:3')}}, 'param_groups': [{'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'step': 1, 'params': [0]}, {'weight_decay': 0.0, 'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'step': 1, 'params': [1]}]}, 'fp32_groups_flat': [tensor([-0.0022, -0.0099,  0.0033,  ..., -0.0024,  0.0022, -0.0027],
Tencent02:        device='cuda:3', requires_grad=True), tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:3', requires_grad=True)], 'clip_grad': 1.0}
Tencent01: [2020-11-11 06:42:29,158] [INFO] [engine.py:542:_configure_optimizer] DeepSpeed Final Optimizer = {'dynamic_loss_scale': True, 'cur_scale': 4294967296, 'cur_iter': 0, 'last_overflow_iter': -1, 'scale_factor': 2, 'scale_window': 1000, 'optimizer_state_dict': {'state': {0: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:6'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:6')}, 1: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:6'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:6')}}, 'param_groups': [{'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'step': 1, 'params': [0]}, {'weight_decay': 0.0, 'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'step': 1, 'params': [1]}]}, 'fp32_groups_flat': [tensor([-0.0022, -0.0099,  0.0033,  ..., -0.0024,  0.0022, -0.0027],
Tencent01:        device='cuda:6', requires_grad=True), tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:6', requires_grad=True)], 'clip_grad': 1.0}
Tencent02: [2020-11-11 06:42:29,158] [INFO] [engine.py:542:_configure_optimizer] DeepSpeed Final Optimizer = {'dynamic_loss_scale': True, 'cur_scale': 4294967296, 'cur_iter': 0, 'last_overflow_iter': -1, 'scale_factor': 2, 'scale_window': 1000, 'optimizer_state_dict': {'state': {0: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 1: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}}, 'param_groups': [{'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'step': 1, 'params': [0]}, {'weight_decay': 0.0, 'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'step': 1, 'params': [1]}]}, 'fp32_groups_flat': [tensor([-0.0022, -0.0099,  0.0033,  ..., -0.0024,  0.0022, -0.0027],
Tencent02:        device='cuda:0', requires_grad=True), tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:0', requires_grad=True)], 'clip_grad': 1.0}
Tencent01: [2020-11-11 06:42:29,159] [INFO] [engine.py:542:_configure_optimizer] DeepSpeed Final Optimizer = {'dynamic_loss_scale': True, 'cur_scale': 4294967296, 'cur_iter': 0, 'last_overflow_iter': -1, 'scale_factor': 2, 'scale_window': 1000, 'optimizer_state_dict': {'state': {0: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:2'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:2')}, 1: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:2'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:2')}}, 'param_groups': [{'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'step': 1, 'params': [0]}, {'weight_decay': 0.0, 'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'step': 1, 'params': [1]}]}, 'fp32_groups_flat': [tensor([-0.0022, -0.0099,  0.0033,  ..., -0.0024,  0.0022, -0.0027],
Tencent01:        device='cuda:2', requires_grad=True), tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:2', requires_grad=True)], 'clip_grad': 1.0}
Tencent02: [2020-11-11 06:42:29,159] [INFO] [engine.py:542:_configure_optimizer] DeepSpeed Final Optimizer = {'dynamic_loss_scale': True, 'cur_scale': 4294967296, 'cur_iter': 0, 'last_overflow_iter': -1, 'scale_factor': 2, 'scale_window': 1000, 'optimizer_state_dict': {'state': {0: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:5'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:5')}, 1: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:5'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:5')}}, 'param_groups': [{'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'step': 1, 'params': [0]}, {'weight_decay': 0.0, 'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'step': 1, 'params': [1]}]}, 'fp32_groups_flat': [tensor([-0.0022, -0.0099,  0.0033,  ..., -0.0024,  0.0022, -0.0027],
Tencent02:        device='cuda:5', requires_grad=True), tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:5', requires_grad=True)], 'clip_grad': 1.0}
Tencent01: [2020-11-11 06:42:29,160] [INFO] [engine.py:542:_configure_optimizer] DeepSpeed Final Optimizer = {'dynamic_loss_scale': True, 'cur_scale': 4294967296, 'cur_iter': 0, 'last_overflow_iter': -1, 'scale_factor': 2, 'scale_window': 1000, 'optimizer_state_dict': {'state': {0: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:3'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:3')}, 1: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:3'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:3')}}, 'param_groups': [{'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'step': 1, 'params': [0]}, {'weight_decay': 0.0, 'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'step': 1, 'params': [1]}]}, 'fp32_groups_flat': [tensor([-0.0022, -0.0099,  0.0033,  ..., -0.0024,  0.0022, -0.0027],
Tencent01:        device='cuda:3', requires_grad=True), tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:3', requires_grad=True)], 'clip_grad': 1.0}
Tencent02: [2020-11-11 06:42:29,160] [INFO] [engine.py:542:_configure_optimizer] DeepSpeed Final Optimizer = {'dynamic_loss_scale': True, 'cur_scale': 4294967296, 'cur_iter': 0, 'last_overflow_iter': -1, 'scale_factor': 2, 'scale_window': 1000, 'optimizer_state_dict': {'state': {0: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:6'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:6')}, 1: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:6'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:6')}}, 'param_groups': [{'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'step': 1, 'params': [0]}, {'weight_decay': 0.0, 'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'step': 1, 'params': [1]}]}, 'fp32_groups_flat': [tensor([-0.0022, -0.0099,  0.0033,  ..., -0.0024,  0.0022, -0.0027],
Tencent02:        device='cuda:6', requires_grad=True), tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:6', requires_grad=True)], 'clip_grad': 1.0}
Tencent01: [2020-11-11 06:42:31,164] [INFO] [checkpointing.py:63:see_memory_usage] First Forward Begining
Tencent01: [2020-11-11 06:42:31,164] [INFO] [checkpointing.py:66:see_memory_usage] Memory Allocated 15.348738670349121 GigaBytes
Tencent01: [2020-11-11 06:42:31,165] [INFO] [checkpointing.py:70:see_memory_usage] Max Memory Allocated 19.08716630935669 GigaBytes
Tencent01: [2020-11-11 06:42:31,165] [INFO] [checkpointing.py:74:see_memory_usage] Cache Allocated 25.01171875 GigaBytes
Tencent01: [2020-11-11 06:42:31,165] [INFO] [checkpointing.py:78:see_memory_usage] Max cache Allocated 25.01171875 GigaBytes
Tencent01: [2020-11-11 06:42:31,165] [INFO] [checkpointing.py:357:forward] Activation Checkpointing Information
Tencent01: [2020-11-11 06:42:31,166] [INFO] [checkpointing.py:359:forward] ----Partition Activations False, CPU CHECKPOINTING False
Tencent01: [2020-11-11 06:42:31,166] [INFO] [checkpointing.py:362:forward] ----contiguous Memory Checkpointing False with 40 total layers
Tencent01: [2020-11-11 06:42:31,166] [INFO] [checkpointing.py:364:forward] ----Synchronization False
Tencent01: [2020-11-11 06:42:31,166] [INFO] [checkpointing.py:365:forward] ----Profiling False
Tencent01: [2020-11-11 06:42:34,969] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 0
Tencent01: [2020-11-11 06:42:34,969] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 0
Tencent01: [2020-11-11 06:42:34,969] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 0
Tencent01: [2020-11-11 06:42:34,969] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 4294967296 to 2147483648.0
Tencent01: [2020-11-11 06:42:34,969] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 0
Tencent01: [2020-11-11 06:42:34,969] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 0
Tencent01: [2020-11-11 06:42:34,969] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 4294967296 to 2147483648.0
Tencent01: [2020-11-11 06:42:34,969] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 0
Tencent01: [2020-11-11 06:42:34,969] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 0
Tencent01: [2020-11-11 06:42:34,969] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 0
Tencent01: [2020-11-11 06:42:34,969] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 4294967296 to 2147483648.0
Tencent01: [2020-11-11 06:42:34,969] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
Tencent01: [2020-11-11 06:42:34,969] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 4294967296 to 2147483648.0
Tencent01: [2020-11-11 06:42:34,969] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 4294967296 to 2147483648.0
Tencent01: [2020-11-11 06:42:34,969] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 4294967296 to 2147483648.0
Tencent01: [2020-11-11 06:42:34,969] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 4294967296 to 2147483648.0
Tencent02: [2020-11-11 06:42:34,968] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-11 06:42:34,969] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 4294967296 to 2147483648.0
Tencent02: Grad overflow on iteration 0
Tencent02: [2020-11-11 06:42:34,968] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 0
Tencent02: [2020-11-11 06:42:34,968] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 0
Tencent02: [2020-11-11 06:42:34,968] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 0
Tencent02: [2020-11-11 06:42:34,968] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 4294967296 to 2147483648.0
Tencent02: [2020-11-11 06:42:34,968] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 4294967296 to 2147483648.0
Tencent02: [2020-11-11 06:42:34,968] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 4294967296 to 2147483648.0
Tencent02: [2020-11-11 06:42:34,968] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 4294967296 to 2147483648.0
Tencent02: [2020-11-11 06:42:34,968] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 0
Tencent02: [2020-11-11 06:42:34,968] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 0
Tencent02: [2020-11-11 06:42:34,968] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 0
Tencent02: [2020-11-11 06:42:34,968] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 4294967296 to 2147483648.0
Tencent02: [2020-11-11 06:42:34,968] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 4294967296 to 2147483648.0
Tencent02: [2020-11-11 06:42:34,968] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 0
Tencent02: [2020-11-11 06:42:34,968] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 4294967296 to 2147483648.0
Tencent02: [2020-11-11 06:42:34,969] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 4294967296 to 2147483648.0
Tencent01: [2020-11-11 06:42:37,437] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 1
Tencent01: [2020-11-11 06:42:37,437] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 2147483648.0 to 1073741824.0
Tencent01: [2020-11-11 06:42:37,437] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 1
Tencent01: [2020-11-11 06:42:37,437] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 1
Tencent01: [2020-11-11 06:42:37,437] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 2147483648.0 to 1073741824.0
Tencent01: [2020-11-11 06:42:37,437] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 1
Tencent01: [2020-11-11 06:42:37,437] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 2147483648.0 to 1073741824.0
Tencent01: [2020-11-11 06:42:37,437] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 1
Tencent01: [2020-11-11 06:42:37,437] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 2147483648.0 to 1073741824.0
Tencent01: [2020-11-11 06:42:37,437] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
Tencent01: [2020-11-11 06:42:37,437] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 1
Tencent01: [2020-11-11 06:42:37,437] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 1
Tencent01: [2020-11-11 06:42:37,437] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 2147483648.0 to 1073741824.0
Tencent01: [2020-11-11 06:42:37,437] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 1
Tencent01: [2020-11-11 06:42:37,437] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 2147483648.0 to 1073741824.0
Tencent01: [2020-11-11 06:42:37,437] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 2147483648.0 to 1073741824.0
Tencent01: [2020-11-11 06:42:37,437] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 2147483648.0 to 1073741824.0
Tencent02: [2020-11-11 06:42:37,436] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 1
Tencent02: [2020-11-11 06:42:37,436] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 1
Tencent02: [2020-11-11 06:42:37,436] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 1
Tencent02: [2020-11-11 06:42:37,436] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 2147483648.0 to 1073741824.0
Tencent02: [2020-11-11 06:42:37,436] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 2147483648.0 to 1073741824.0
Tencent02: [2020-11-11 06:42:37,436] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 2147483648.0 to 1073741824.0
Tencent02: [2020-11-11 06:42:37,436] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 1
Tencent02: [2020-11-11 06:42:37,436] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 1
Tencent02: [2020-11-11 06:42:37,436] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 2147483648.0 to 1073741824.0
Tencent02: [2020-11-11 06:42:37,436] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 1
Tencent02: [2020-11-11 06:42:37,437] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 2147483648.0 to 1073741824.0
Tencent02: [2020-11-11 06:42:37,437] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 2147483648.0 to 1073741824.0
Tencent02: [2020-11-11 06:42:37,436] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 1
Tencent02: [2020-11-11 06:42:37,437] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 1
Tencent02: [2020-11-11 06:42:37,437] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 2147483648.0 to 1073741824.0
Tencent02: [2020-11-11 06:42:37,437] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 2147483648.0 to 1073741824.0
Tencent01: [2020-11-11 06:42:39,911] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 2
Tencent01: [2020-11-11 06:42:39,911] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 1073741824.0 to 536870912.0
Tencent01: [2020-11-11 06:42:39,911] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 2
Tencent01: [2020-11-11 06:42:39,911] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 1073741824.0 to 536870912.0
Tencent01: [2020-11-11 06:42:39,911] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 2
Tencent01: [2020-11-11 06:42:39,911] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
Tencent01: [2020-11-11 06:42:39,911] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 1073741824.0 to 536870912.0
Tencent01: [2020-11-11 06:42:39,911] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 2
Tencent01: [2020-11-11 06:42:39,911] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 2
Tencent01: [2020-11-11 06:42:39,911] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 2
Tencent01: [2020-11-11 06:42:39,911] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 2
Tencent01: [2020-11-11 06:42:39,911] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 1073741824.0 to 536870912.0
Tencent01: [2020-11-11 06:42:39,911] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 1073741824.0 to 536870912.0
Tencent02: [2020-11-11 06:42:39,911] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-11 06:42:39,911] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 2
Tencent01: [2020-11-11 06:42:39,911] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 1073741824.0 to 536870912.0
Tencent02: Grad overflow on iteration 2
Tencent01: [2020-11-11 06:42:39,911] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 1073741824.0 to 536870912.0
Tencent01: [2020-11-11 06:42:39,911] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 1073741824.0 to 536870912.0
Tencent02: [2020-11-11 06:42:39,911] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 1073741824.0 to 536870912.0
Tencent02: [2020-11-11 06:42:39,911] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 2
Tencent02: [2020-11-11 06:42:39,911] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 2
Tencent02: [2020-11-11 06:42:39,911] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 1073741824.0 to 536870912.0
Tencent02: [2020-11-11 06:42:39,911] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 2
Tencent02: [2020-11-11 06:42:39,911] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 2
Tencent02: [2020-11-11 06:42:39,911] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 1073741824.0 to 536870912.0
Tencent02: [2020-11-11 06:42:39,911] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 2
Tencent02: [2020-11-11 06:42:39,911] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 1073741824.0 to 536870912.0
Tencent02: [2020-11-11 06:42:39,911] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 1073741824.0 to 536870912.0
Tencent02: [2020-11-11 06:42:39,911] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 1073741824.0 to 536870912.0
Tencent02: [2020-11-11 06:42:39,911] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 2
Tencent02: [2020-11-11 06:42:39,911] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 1073741824.0 to 536870912.0
Tencent02: [2020-11-11 06:42:39,911] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 2
Tencent02: [2020-11-11 06:42:39,911] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 1073741824.0 to 536870912.0
Tencent01: [2020-11-11 06:42:42,382] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 3
Tencent01: [2020-11-11 06:42:42,382] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 536870912.0 to 268435456.0
Tencent01: [2020-11-11 06:42:42,382] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 3
Tencent01: [2020-11-11 06:42:42,382] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 3
Tencent01: [2020-11-11 06:42:42,382] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 3
Tencent01: [2020-11-11 06:42:42,382] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 3
Tencent01: [2020-11-11 06:42:42,382] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
Tencent01: [2020-11-11 06:42:42,382] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 536870912.0 to 268435456.0
Tencent01: [2020-11-11 06:42:42,382] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 536870912.0 to 268435456.0
Tencent01: [2020-11-11 06:42:42,382] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 3
Tencent01: [2020-11-11 06:42:42,382] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 536870912.0 to 268435456.0
Tencent01: [2020-11-11 06:42:42,382] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 536870912.0 to 268435456.0
Tencent01: [2020-11-11 06:42:42,382] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 3
Tencent01: [2020-11-11 06:42:42,382] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 536870912.0 to 268435456.0
Tencent01: [2020-11-11 06:42:42,382] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 536870912.0 to 268435456.0
Tencent01: [2020-11-11 06:42:42,382] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 3
Tencent01: [2020-11-11 06:42:42,382] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 536870912.0 to 268435456.0
Tencent02: [2020-11-11 06:42:42,381] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 3
Tencent02: [2020-11-11 06:42:42,381] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 3
Tencent02: [2020-11-11 06:42:42,381] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 3
Tencent02: [2020-11-11 06:42:42,381] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 536870912.0 to 268435456.0
Tencent02: [2020-11-11 06:42:42,381] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 536870912.0 to 268435456.0
Tencent02: [2020-11-11 06:42:42,381] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 3
Tencent02: [2020-11-11 06:42:42,381] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 536870912.0 to 268435456.0
Tencent02: [2020-11-11 06:42:42,381] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 3
Tencent02: [2020-11-11 06:42:42,382] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 536870912.0 to 268435456.0
Tencent02: [2020-11-11 06:42:42,381] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 3
Tencent02: [2020-11-11 06:42:42,382] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 536870912.0 to 268435456.0
Tencent02: [2020-11-11 06:42:42,382] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 536870912.0 to 268435456.0
Tencent02: [2020-11-11 06:42:42,382] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 3
Tencent02: [2020-11-11 06:42:42,382] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 536870912.0 to 268435456.0
Tencent02: [2020-11-11 06:42:42,382] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 3
Tencent02: [2020-11-11 06:42:42,382] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 536870912.0 to 268435456.0
Tencent01: [2020-11-11 06:42:44,884] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 4
Tencent01: [2020-11-11 06:42:44,884] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 4
Tencent01: [2020-11-11 06:42:44,884] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 268435456.0 to 134217728.0
Tencent01: [2020-11-11 06:42:44,884] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 4
Tencent01: [2020-11-11 06:42:44,884] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 4
Tencent01: [2020-11-11 06:42:44,884] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 4
Tencent01: [2020-11-11 06:42:44,884] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 268435456.0 to 134217728.0
Tencent01: [2020-11-11 06:42:44,884] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 4
Tencent01: [2020-11-11 06:42:44,884] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
Tencent01: [2020-11-11 06:42:44,884] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 268435456.0 to 134217728.0
Tencent01: [2020-11-11 06:42:44,884] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 268435456.0 to 134217728.0
Tencent01: [2020-11-11 06:42:44,884] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 268435456.0 to 134217728.0
Tencent01: [2020-11-11 06:42:44,884] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 268435456.0 to 134217728.0
Tencent01: [2020-11-11 06:42:44,884] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 4
Tencent01: [2020-11-11 06:42:44,884] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 268435456.0 to 134217728.0
Tencent01: [2020-11-11 06:42:44,884] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 4
Tencent01: [2020-11-11 06:42:44,884] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 268435456.0 to 134217728.0
Tencent02: [2020-11-11 06:42:44,884] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 4
Tencent02: [2020-11-11 06:42:44,884] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 4
Tencent02: [2020-11-11 06:42:44,884] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 268435456.0 to 134217728.0
Tencent02: [2020-11-11 06:42:44,884] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 4
Tencent02: [2020-11-11 06:42:44,884] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 268435456.0 to 134217728.0
Tencent02: [2020-11-11 06:42:44,884] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 4
Tencent02: [2020-11-11 06:42:44,884] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 268435456.0 to 134217728.0
Tencent02: [2020-11-11 06:42:44,884] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 268435456.0 to 134217728.0
Tencent02: [2020-11-11 06:42:44,884] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 4
Tencent02: [2020-11-11 06:42:44,884] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 4
Tencent02: [2020-11-11 06:42:44,884] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 268435456.0 to 134217728.0
Tencent02: [2020-11-11 06:42:44,884] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 268435456.0 to 134217728.0
Tencent02: [2020-11-11 06:42:44,884] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 4
Tencent02: [2020-11-11 06:42:44,884] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 268435456.0 to 134217728.0
Tencent02: [2020-11-11 06:42:44,884] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 4
Tencent02: [2020-11-11 06:42:44,884] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 268435456.0 to 134217728.0
Tencent01: [2020-11-11 06:42:47,390] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 5
Tencent01: [2020-11-11 06:42:47,390] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 5
Tencent01: [2020-11-11 06:42:47,390] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 134217728.0 to 67108864.0
Tencent01: [2020-11-11 06:42:47,390] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 134217728.0 to 67108864.0
Tencent02: [2020-11-11 06:42:47,390] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 5
Tencent01: [2020-11-11 06:42:47,390] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: [2020-11-11 06:42:47,390] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 5
Tencent02: Grad overflow on iteration 5
Tencent01: [2020-11-11 06:42:47,390] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: [2020-11-11 06:42:47,390] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 5
Tencent02: Grad overflow on iteration 5
Tencent01: [2020-11-11 06:42:47,390] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 134217728.0 to 67108864.0
Tencent02: [2020-11-11 06:42:47,390] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 134217728.0 to 67108864.0
Tencent01: [2020-11-11 06:42:47,390] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 5
Tencent02: [2020-11-11 06:42:47,390] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 134217728.0 to 67108864.0
Tencent01: [2020-11-11 06:42:47,390] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 134217728.0 to 67108864.0
Tencent01: [2020-11-11 06:42:47,390] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: [2020-11-11 06:42:47,390] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 5
Tencent01: Grad overflow on iteration 5
Tencent02: [2020-11-11 06:42:47,390] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 134217728.0 to 67108864.0
Tencent01: [2020-11-11 06:42:47,390] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 5
Tencent01: [2020-11-11 06:42:47,390] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 134217728.0 to 67108864.0
Tencent01: [2020-11-11 06:42:47,390] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 5
Tencent01: [2020-11-11 06:42:47,390] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 134217728.0 to 67108864.0
Tencent02: [2020-11-11 06:42:47,390] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 134217728.0 to 67108864.0
Tencent01: [2020-11-11 06:42:47,390] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
Tencent02: [2020-11-11 06:42:47,390] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-11 06:42:47,390] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 134217728.0 to 67108864.0
Tencent01: [2020-11-11 06:42:47,390] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 134217728.0 to 67108864.0
Tencent02: Grad overflow on iteration 5
Tencent02: [2020-11-11 06:42:47,390] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 5
Tencent02: [2020-11-11 06:42:47,390] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 134217728.0 to 67108864.0
Tencent02: [2020-11-11 06:42:47,390] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 134217728.0 to 67108864.0
Tencent02: [2020-11-11 06:42:47,390] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 5
Tencent02: [2020-11-11 06:42:47,390] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 134217728.0 to 67108864.0
Tencent02: [2020-11-11 06:42:47,390] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 5
Tencent02: [2020-11-11 06:42:47,390] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 134217728.0 to 67108864.0
Tencent01: [2020-11-11 06:42:49,890] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6
Tencent01: [2020-11-11 06:42:49,890] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 67108864.0 to 33554432.0
Tencent01: [2020-11-11 06:42:49,890] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6
Tencent01: [2020-11-11 06:42:49,890] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6
Tencent01: [2020-11-11 06:42:49,890] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6
Tencent01: [2020-11-11 06:42:49,890] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6
Tencent01: [2020-11-11 06:42:49,890] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
Tencent01: [2020-11-11 06:42:49,890] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6
Tencent01: [2020-11-11 06:42:49,890] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 67108864.0 to 33554432.0
Tencent01: [2020-11-11 06:42:49,890] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 67108864.0 to 33554432.0
Tencent01: [2020-11-11 06:42:49,890] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 67108864.0 to 33554432.0
Tencent01: [2020-11-11 06:42:49,890] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 67108864.0 to 33554432.0
Tencent01: [2020-11-11 06:42:49,890] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6
Tencent01: [2020-11-11 06:42:49,891] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 67108864.0 to 33554432.0
Tencent01: [2020-11-11 06:42:49,891] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 67108864.0 to 33554432.0
Tencent01: [2020-11-11 06:42:49,891] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6
Tencent02: [2020-11-11 06:42:49,890] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6
Tencent02: [2020-11-11 06:42:49,890] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6
Tencent02: [2020-11-11 06:42:49,890] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 67108864.0 to 33554432.0
Tencent02: [2020-11-11 06:42:49,890] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6
Tencent01: [2020-11-11 06:42:49,891] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 67108864.0 to 33554432.0
Tencent02: [2020-11-11 06:42:49,890] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 67108864.0 to 33554432.0
Tencent02: [2020-11-11 06:42:49,890] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6
Tencent02: [2020-11-11 06:42:49,890] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 67108864.0 to 33554432.0
Tencent02: [2020-11-11 06:42:49,890] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 67108864.0 to 33554432.0
Tencent02: [2020-11-11 06:42:49,890] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6
Tencent02: [2020-11-11 06:42:49,890] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6
Tencent02: [2020-11-11 06:42:49,890] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 67108864.0 to 33554432.0
Tencent02: [2020-11-11 06:42:49,890] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 67108864.0 to 33554432.0
Tencent02: [2020-11-11 06:42:49,890] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6
Tencent02: [2020-11-11 06:42:49,890] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 67108864.0 to 33554432.0
Tencent02: [2020-11-11 06:42:49,890] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6
Tencent02: [2020-11-11 06:42:49,890] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 67108864.0 to 33554432.0
Tencent01: [2020-11-11 06:42:52,391] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 7
Tencent01: [2020-11-11 06:42:52,391] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 7
Tencent01: [2020-11-11 06:42:52,391] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 7
Tencent01: [2020-11-11 06:42:52,391] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 33554432.0 to 16777216.0
Tencent01: [2020-11-11 06:42:52,391] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 33554432.0 to 16777216.0
Tencent01: [2020-11-11 06:42:52,391] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 33554432.0 to 16777216.0
Tencent01: [2020-11-11 06:42:52,391] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 7
Tencent01: [2020-11-11 06:42:52,391] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 7
Tencent01: [2020-11-11 06:42:52,391] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 7
Tencent01: [2020-11-11 06:42:52,391] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
Tencent01: [2020-11-11 06:42:52,391] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 33554432.0 to 16777216.0
Tencent01: [2020-11-11 06:42:52,391] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 7
Tencent01: [2020-11-11 06:42:52,391] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 33554432.0 to 16777216.0
Tencent01: [2020-11-11 06:42:52,391] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 33554432.0 to 16777216.0
Tencent01: [2020-11-11 06:42:52,391] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 33554432.0 to 16777216.0
Tencent01: [2020-11-11 06:42:52,391] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 7
Tencent01: [2020-11-11 06:42:52,391] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 33554432.0 to 16777216.0
Tencent02: [2020-11-11 06:42:52,391] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 7
Tencent02: [2020-11-11 06:42:52,391] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 7
Tencent02: [2020-11-11 06:42:52,391] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 7
Tencent02: [2020-11-11 06:42:52,391] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 7
Tencent02: [2020-11-11 06:42:52,391] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 33554432.0 to 16777216.0
Tencent02: [2020-11-11 06:42:52,391] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 33554432.0 to 16777216.0
Tencent02: [2020-11-11 06:42:52,391] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 33554432.0 to 16777216.0
Tencent02: [2020-11-11 06:42:52,391] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 7
Tencent02: [2020-11-11 06:42:52,391] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 33554432.0 to 16777216.0
Tencent02: [2020-11-11 06:42:52,391] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 7
Tencent02: [2020-11-11 06:42:52,391] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 7
Tencent02: [2020-11-11 06:42:52,391] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 33554432.0 to 16777216.0
Tencent02: [2020-11-11 06:42:52,391] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 33554432.0 to 16777216.0
Tencent02: [2020-11-11 06:42:52,391] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 33554432.0 to 16777216.0
Tencent02: [2020-11-11 06:42:52,391] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 7
Tencent02: [2020-11-11 06:42:52,391] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 33554432.0 to 16777216.0
Tencent01: [2020-11-11 06:42:54,898] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 8
Tencent01: [2020-11-11 06:42:54,898] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 8
Tencent01: [2020-11-11 06:42:54,898] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 8
Tencent01: [2020-11-11 06:42:54,898] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16777216.0 to 8388608.0
Tencent01: [2020-11-11 06:42:54,898] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16777216.0 to 8388608.0
Tencent01: [2020-11-11 06:42:54,898] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16777216.0 to 8388608.0
Tencent01: [2020-11-11 06:42:54,898] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 8
Tencent01: [2020-11-11 06:42:54,898] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 8
Tencent01: [2020-11-11 06:42:54,899] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
Tencent01: [2020-11-11 06:42:54,898] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 8
Tencent01: [2020-11-11 06:42:54,899] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16777216.0 to 8388608.0
Tencent01: [2020-11-11 06:42:54,899] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16777216.0 to 8388608.0
Tencent01: [2020-11-11 06:42:54,899] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16777216.0 to 8388608.0
Tencent01: [2020-11-11 06:42:54,899] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 8
Tencent01: [2020-11-11 06:42:54,899] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16777216.0 to 8388608.0
Tencent02: [2020-11-11 06:42:54,898] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 8
Tencent01: [2020-11-11 06:42:54,899] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 8
Tencent02: [2020-11-11 06:42:54,898] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16777216.0 to 8388608.0
Tencent02: [2020-11-11 06:42:54,898] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 8
Tencent02: [2020-11-11 06:42:54,898] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 8
Tencent02: [2020-11-11 06:42:54,898] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-11 06:42:54,899] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16777216.0 to 8388608.0
Tencent02: Grad overflow on iteration 8
Tencent02: [2020-11-11 06:42:54,898] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 8
Tencent02: [2020-11-11 06:42:54,898] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 8
Tencent02: [2020-11-11 06:42:54,898] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16777216.0 to 8388608.0
Tencent02: [2020-11-11 06:42:54,898] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16777216.0 to 8388608.0
Tencent02: [2020-11-11 06:42:54,898] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16777216.0 to 8388608.0
Tencent02: [2020-11-11 06:42:54,898] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16777216.0 to 8388608.0
Tencent02: [2020-11-11 06:42:54,898] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16777216.0 to 8388608.0
Tencent02: [2020-11-11 06:42:54,898] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 8
Tencent02: [2020-11-11 06:42:54,898] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16777216.0 to 8388608.0
Tencent02: [2020-11-11 06:42:54,898] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 8
Tencent02: [2020-11-11 06:42:54,898] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16777216.0 to 8388608.0
Tencent01: [2020-11-11 06:42:57,409] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 9
Tencent01: [2020-11-11 06:42:57,409] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 9
Tencent01: [2020-11-11 06:42:57,409] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 9
Tencent01: [2020-11-11 06:42:57,409] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 8388608.0 to 4194304.0
Tencent01: [2020-11-11 06:42:57,409] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 8388608.0 to 4194304.0
Tencent01: [2020-11-11 06:42:57,409] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 8388608.0 to 4194304.0
Tencent01: [2020-11-11 06:42:57,409] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
Tencent01: [2020-11-11 06:42:57,409] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 9
Tencent01: [2020-11-11 06:42:57,409] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 9
Tencent01: [2020-11-11 06:42:57,409] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 9
Tencent01: [2020-11-11 06:42:57,409] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 9
Tencent01: [2020-11-11 06:42:57,409] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 8388608.0 to 4194304.0
Tencent01: [2020-11-11 06:42:57,409] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 8388608.0 to 4194304.0
Tencent01: [2020-11-11 06:42:57,409] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 8388608.0 to 4194304.0
Tencent01: [2020-11-11 06:42:57,409] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 8388608.0 to 4194304.0
Tencent02: [2020-11-11 06:42:57,408] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 9
Tencent01: [2020-11-11 06:42:57,409] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 9
Tencent02: [2020-11-11 06:42:57,408] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 9
Tencent02: [2020-11-11 06:42:57,408] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 8388608.0 to 4194304.0
Tencent02: [2020-11-11 06:42:57,408] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 8388608.0 to 4194304.0
Tencent02: [2020-11-11 06:42:57,408] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 9
Tencent02: [2020-11-11 06:42:57,408] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-11 06:42:57,409] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 8388608.0 to 4194304.0
Tencent02: Grad overflow on iteration 9
Tencent02: [2020-11-11 06:42:57,408] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 8388608.0 to 4194304.0
Tencent02: [2020-11-11 06:42:57,408] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 8388608.0 to 4194304.0
Tencent02: [2020-11-11 06:42:57,408] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 9
Tencent02: [2020-11-11 06:42:57,408] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 9
Tencent02: [2020-11-11 06:42:57,409] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 8388608.0 to 4194304.0
Tencent02: [2020-11-11 06:42:57,409] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 8388608.0 to 4194304.0
Tencent02: [2020-11-11 06:42:57,408] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 9
Tencent02: [2020-11-11 06:42:57,409] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 9
Tencent02: [2020-11-11 06:42:57,409] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 8388608.0 to 4194304.0
Tencent02: [2020-11-11 06:42:57,409] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 8388608.0 to 4194304.0
Tencent01: [2020-11-11 06:42:59,919] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 10
Tencent01: [2020-11-11 06:42:59,919] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 10
Tencent01: [2020-11-11 06:42:59,919] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 4194304.0 to 2097152.0
Tencent01: [2020-11-11 06:42:59,919] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 4194304.0 to 2097152.0
Tencent01: [2020-11-11 06:42:59,919] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 10
Tencent01: [2020-11-11 06:42:59,919] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
Tencent01: [2020-11-11 06:42:59,919] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 4194304.0 to 2097152.0
Tencent01: [2020-11-11 06:42:59,919] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 10
Tencent01: [2020-11-11 06:42:59,919] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 10
Tencent01: [2020-11-11 06:42:59,919] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 4194304.0 to 2097152.0
Tencent01: [2020-11-11 06:42:59,919] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 4194304.0 to 2097152.0
Tencent01: [2020-11-11 06:42:59,919] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 10
Tencent01: [2020-11-11 06:42:59,919] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 10
Tencent01: [2020-11-11 06:42:59,919] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 4194304.0 to 2097152.0
Tencent01: [2020-11-11 06:42:59,919] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 4194304.0 to 2097152.0
Tencent01: [2020-11-11 06:42:59,919] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 10
Tencent01: [2020-11-11 06:42:59,919] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 4194304.0 to 2097152.0
Tencent02: [2020-11-11 06:42:59,919] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 10
Tencent02: [2020-11-11 06:42:59,919] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 10
Tencent02: [2020-11-11 06:42:59,919] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 4194304.0 to 2097152.0
Tencent02: [2020-11-11 06:42:59,919] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 4194304.0 to 2097152.0
Tencent02: [2020-11-11 06:42:59,919] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 10
Tencent02: [2020-11-11 06:42:59,919] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 10
Tencent02: [2020-11-11 06:42:59,919] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 10
Tencent02: [2020-11-11 06:42:59,919] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 4194304.0 to 2097152.0
Tencent02: [2020-11-11 06:42:59,919] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 4194304.0 to 2097152.0
Tencent02: [2020-11-11 06:42:59,919] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 10
Tencent02: [2020-11-11 06:42:59,919] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 4194304.0 to 2097152.0
Tencent02: [2020-11-11 06:42:59,919] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 4194304.0 to 2097152.0
Tencent02: [2020-11-11 06:42:59,919] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 10
Tencent02: [2020-11-11 06:42:59,919] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 4194304.0 to 2097152.0
Tencent02: [2020-11-11 06:42:59,919] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 10
Tencent02: [2020-11-11 06:42:59,919] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 4194304.0 to 2097152.0
Tencent01: [2020-11-11 06:43:02,430] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 11
Tencent01: [2020-11-11 06:43:02,430] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 11
Tencent01: [2020-11-11 06:43:02,430] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 2097152.0 to 1048576.0
Tencent01: [2020-11-11 06:43:02,430] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 2097152.0 to 1048576.0
Tencent01: [2020-11-11 06:43:02,430] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 11
Tencent01: [2020-11-11 06:43:02,430] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
Tencent01: [2020-11-11 06:43:02,430] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 2097152.0 to 1048576.0
Tencent01: [2020-11-11 06:43:02,430] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 11
Tencent01: [2020-11-11 06:43:02,430] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 11
Tencent01: [2020-11-11 06:43:02,430] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 2097152.0 to 1048576.0
Tencent01: [2020-11-11 06:43:02,430] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 2097152.0 to 1048576.0
Tencent01: [2020-11-11 06:43:02,430] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 11
Tencent01: [2020-11-11 06:43:02,430] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 11
Tencent01: [2020-11-11 06:43:02,430] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 2097152.0 to 1048576.0
Tencent01: [2020-11-11 06:43:02,430] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 2097152.0 to 1048576.0
Tencent01: [2020-11-11 06:43:02,430] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 11
Tencent01: [2020-11-11 06:43:02,430] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 2097152.0 to 1048576.0
Tencent02: [2020-11-11 06:43:02,429] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 11
Tencent02: [2020-11-11 06:43:02,429] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 2097152.0 to 1048576.0
Tencent02: [2020-11-11 06:43:02,429] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 11
Tencent02: [2020-11-11 06:43:02,429] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 11
Tencent02: [2020-11-11 06:43:02,429] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 11
Tencent02: [2020-11-11 06:43:02,430] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 2097152.0 to 1048576.0
Tencent02: [2020-11-11 06:43:02,429] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 11
Tencent02: [2020-11-11 06:43:02,430] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 2097152.0 to 1048576.0
Tencent02: [2020-11-11 06:43:02,429] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 11
Tencent02: [2020-11-11 06:43:02,430] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 2097152.0 to 1048576.0
Tencent02: [2020-11-11 06:43:02,430] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 2097152.0 to 1048576.0
Tencent02: [2020-11-11 06:43:02,430] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 11
Tencent02: [2020-11-11 06:43:02,430] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 2097152.0 to 1048576.0
Tencent02: [2020-11-11 06:43:02,430] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 2097152.0 to 1048576.0
Tencent02: [2020-11-11 06:43:02,430] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 11
Tencent02: [2020-11-11 06:43:02,430] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 2097152.0 to 1048576.0
Tencent01: [2020-11-11 06:43:04,943] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 12
Tencent01: [2020-11-11 06:43:04,943] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 12
Tencent01: [2020-11-11 06:43:04,943] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
Tencent01: [2020-11-11 06:43:04,943] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
Tencent01: [2020-11-11 06:43:04,943] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 12
Tencent01: [2020-11-11 06:43:04,943] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 12
Tencent01: [2020-11-11 06:43:04,943] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
Tencent01: [2020-11-11 06:43:04,943] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
Tencent01: [2020-11-11 06:43:04,943] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
Tencent01: [2020-11-11 06:43:04,943] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 12
Tencent01: [2020-11-11 06:43:04,943] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 12
Tencent01: [2020-11-11 06:43:04,943] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
Tencent01: [2020-11-11 06:43:04,943] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 12
Tencent01: [2020-11-11 06:43:04,943] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
Tencent01: [2020-11-11 06:43:04,943] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
Tencent02: [2020-11-11 06:43:04,942] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 12
Tencent01: [2020-11-11 06:43:04,943] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 12
Tencent01: [2020-11-11 06:43:04,943] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
Tencent02: [2020-11-11 06:43:04,942] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 12
Tencent02: [2020-11-11 06:43:04,942] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
Tencent02: [2020-11-11 06:43:04,942] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 12
Tencent02: [2020-11-11 06:43:04,943] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
Tencent02: [2020-11-11 06:43:04,942] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 12
Tencent02: [2020-11-11 06:43:04,942] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 12
Tencent02: [2020-11-11 06:43:04,942] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 12
Tencent02: [2020-11-11 06:43:04,943] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
Tencent02: [2020-11-11 06:43:04,943] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
Tencent02: [2020-11-11 06:43:04,943] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
Tencent02: [2020-11-11 06:43:04,943] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
Tencent02: [2020-11-11 06:43:04,943] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 12
Tencent02: [2020-11-11 06:43:04,943] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 12
Tencent02: [2020-11-11 06:43:04,943] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
Tencent02: [2020-11-11 06:43:04,943] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
Tencent01: [2020-11-11 06:43:07,453] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 13
Tencent01: [2020-11-11 06:43:07,453] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 13
Tencent01: [2020-11-11 06:43:07,453] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 13
Tencent01: [2020-11-11 06:43:07,453] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 13
Tencent01: [2020-11-11 06:43:07,453] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-11 06:43:07,453] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-11 06:43:07,453] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-11 06:43:07,453] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-11 06:43:07,453] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
Tencent01: [2020-11-11 06:43:07,453] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 13
Tencent02: [2020-11-11 06:43:07,453] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-11 06:43:07,453] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 13
Tencent01: Grad overflow on iteration 13
Tencent01: [2020-11-11 06:43:07,453] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-11 06:43:07,453] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 13
Tencent01: [2020-11-11 06:43:07,453] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 13
Tencent01: [2020-11-11 06:43:07,453] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-11 06:43:07,453] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 13
Tencent01: [2020-11-11 06:43:07,453] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-11 06:43:07,453] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-11 06:43:07,453] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-11 06:43:07,453] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 13
Tencent02: [2020-11-11 06:43:07,453] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-11 06:43:07,453] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 13
Tencent02: [2020-11-11 06:43:07,453] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 13
Tencent02: [2020-11-11 06:43:07,453] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 13
Tencent02: [2020-11-11 06:43:07,453] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 13
Tencent02: [2020-11-11 06:43:07,453] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-11 06:43:07,453] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-11 06:43:07,453] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-11 06:43:07,453] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 13
Tencent02: [2020-11-11 06:43:07,453] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-11 06:43:07,453] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-11 06:43:07,453] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-11 06:43:09,965] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 14
Tencent01: [2020-11-11 06:43:09,965] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 14
Tencent01: [2020-11-11 06:43:09,965] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-11 06:43:09,965] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 14
Tencent01: [2020-11-11 06:43:09,965] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-11 06:43:09,965] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Tencent01: [2020-11-11 06:43:09,965] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 14
Tencent01: [2020-11-11 06:43:09,965] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-11 06:43:09,965] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 14
Tencent01: [2020-11-11 06:43:09,965] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-11 06:43:09,965] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-11 06:43:09,965] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 14
Tencent01: [2020-11-11 06:43:09,965] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 14
Tencent01: [2020-11-11 06:43:09,965] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-11 06:43:09,964] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 14
Tencent01: [2020-11-11 06:43:09,965] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 14
Tencent02: [2020-11-11 06:43:09,964] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-11 06:43:09,965] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-11 06:43:09,965] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: Grad overflow on iteration 14
Tencent02: [2020-11-11 06:43:09,964] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-11 06:43:09,964] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 14
Tencent02: [2020-11-11 06:43:09,964] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 14
Tencent02: [2020-11-11 06:43:09,964] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 14
Tencent02: [2020-11-11 06:43:09,964] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-11 06:43:09,964] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 14
Tencent02: [2020-11-11 06:43:09,965] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-11 06:43:09,965] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-11 06:43:09,965] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-11 06:43:09,964] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 14
Tencent02: [2020-11-11 06:43:09,964] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 14
Tencent02: [2020-11-11 06:43:09,965] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-11 06:43:09,965] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-11 06:43:09,965] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-11 06:43:12,476] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 15
Tencent01: [2020-11-11 06:43:12,476] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 15
Tencent01: [2020-11-11 06:43:12,476] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-11 06:43:12,476] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-11 06:43:12,476] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 15
Tencent01: [2020-11-11 06:43:12,476] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Tencent01: [2020-11-11 06:43:12,476] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-11 06:43:12,476] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 15
Tencent01: [2020-11-11 06:43:12,476] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 15
Tencent01: [2020-11-11 06:43:12,476] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 15
Tencent01: [2020-11-11 06:43:12,476] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-11 06:43:12,476] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-11 06:43:12,476] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 15
Tencent01: [2020-11-11 06:43:12,476] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-11 06:43:12,476] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-11 06:43:12,476] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 15
Tencent01: [2020-11-11 06:43:12,476] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 15
Tencent01: [2020-11-11 06:43:12,477] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-11 06:43:12,476] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-11 06:43:12,476] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 15
Tencent02: [2020-11-11 06:43:12,476] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 15
Tencent02: [2020-11-11 06:43:12,476] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 15
Tencent02: [2020-11-11 06:43:12,476] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 15
Tencent02: [2020-11-11 06:43:12,476] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-11 06:43:12,476] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-11 06:43:12,476] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-11 06:43:12,476] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 15
Tencent02: [2020-11-11 06:43:12,476] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 15
Tencent02: [2020-11-11 06:43:12,476] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 15
Tencent02: [2020-11-11 06:43:12,476] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-11 06:43:12,476] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-11 06:43:12,476] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-11 06:43:12,476] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-11 06:43:14,986] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 16
Tencent01: [2020-11-11 06:43:14,986] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent01: [2020-11-11 06:43:14,986] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Tencent01: [2020-11-11 06:43:14,986] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 16
Tencent01: [2020-11-11 06:43:14,986] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 16
Tencent01: [2020-11-11 06:43:14,986] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 16
Tencent01: [2020-11-11 06:43:14,986] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 16
Tencent01: [2020-11-11 06:43:14,986] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent01: [2020-11-11 06:43:14,986] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 16
Tencent01: [2020-11-11 06:43:14,986] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent01: [2020-11-11 06:43:14,986] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent01: [2020-11-11 06:43:14,986] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent01: [2020-11-11 06:43:14,986] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent01: [2020-11-11 06:43:14,986] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 16
Tencent01: [2020-11-11 06:43:14,986] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 16
Tencent01: [2020-11-11 06:43:14,986] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent02: [2020-11-11 06:43:14,986] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 16
Tencent02: [2020-11-11 06:43:14,986] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 16
Tencent02: [2020-11-11 06:43:14,986] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 16
Tencent01: [2020-11-11 06:43:14,986] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent02: [2020-11-11 06:43:14,986] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent02: [2020-11-11 06:43:14,986] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent02: [2020-11-11 06:43:14,986] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 16
Tencent02: [2020-11-11 06:43:14,986] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 16
Tencent02: [2020-11-11 06:43:14,986] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent02: [2020-11-11 06:43:14,986] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 16
Tencent02: [2020-11-11 06:43:14,986] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 16
Tencent02: [2020-11-11 06:43:14,986] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 16
Tencent02: [2020-11-11 06:43:14,986] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent02: [2020-11-11 06:43:14,986] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent02: [2020-11-11 06:43:14,986] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent02: [2020-11-11 06:43:14,986] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent02: [2020-11-11 06:43:14,986] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent01: [2020-11-11 06:43:17,516] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 17
Tencent01: [2020-11-11 06:43:17,516] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent01: [2020-11-11 06:43:17,516] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Tencent01: [2020-11-11 06:43:17,516] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 17
Tencent01: [2020-11-11 06:43:17,516] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent01: [2020-11-11 06:43:17,517] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 17
Tencent01: [2020-11-11 06:43:17,517] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 17
Tencent01: [2020-11-11 06:43:17,517] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 17
Tencent01: [2020-11-11 06:43:17,517] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 17
Tencent01: [2020-11-11 06:43:17,517] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent01: [2020-11-11 06:43:17,517] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent01: [2020-11-11 06:43:17,517] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent01: [2020-11-11 06:43:17,517] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent01: [2020-11-11 06:43:17,517] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 17
Tencent02: [2020-11-11 06:43:17,516] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 17
Tencent02: [2020-11-11 06:43:17,516] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 17
Tencent02: [2020-11-11 06:43:17,516] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent01: [2020-11-11 06:43:17,517] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent02: [2020-11-11 06:43:17,516] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent02: [2020-11-11 06:43:17,516] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 17
Tencent02: [2020-11-11 06:43:17,516] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 17
Tencent02: [2020-11-11 06:43:17,516] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 17
Tencent02: [2020-11-11 06:43:17,516] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent02: [2020-11-11 06:43:17,516] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent02: [2020-11-11 06:43:17,516] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent02: [2020-11-11 06:43:17,516] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 17
Tencent01: [2020-11-11 06:43:17,517] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 17
Tencent01: [2020-11-11 06:43:17,517] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent02: [2020-11-11 06:43:17,516] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 17
Tencent02: [2020-11-11 06:43:17,516] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent02: [2020-11-11 06:43:17,516] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent02: [2020-11-11 06:43:17,517] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 17
Tencent02: [2020-11-11 06:43:17,517] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent01: [2020-11-11 06:46:54,165] [INFO] [logging.py:60:log_dist] [Rank 0] step=100, skipped=18, lr=[2.4299999999999998e-05, 2.4299999999999998e-05], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 06:46:54,217] [INFO] [timer.py:157:stop] 0/100, SamplesPerSec=48.878825612152085
Tencent01:  iteration      100/   50000 | elapsed time per iteration (ms): 2630.6 | learning rate 2.460E-05 | lm loss 9.125740E+00 | loss scale 16384.0 |
Tencent01: after 100 iterations memory (MB) | allocated: 15636.42041015625 | max allocated: 25509.12255859375 | cached: 27186.0 | max cached: 27186.0
Tencent01: time (ms) | forward: 580.75 | backward: 1949.66 | optimizer: 99.95 | batch generator: 1.23 | data loader: 0.25
Tencent01: [2020-11-11 06:51:18,943] [INFO] [logging.py:60:log_dist] [Rank 0] step=200, skipped=18, lr=[5.429999999999999e-05, 5.429999999999999e-05], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 06:51:18,995] [INFO] [timer.py:157:stop] 0/200, SamplesPerSec=48.621144221225
Tencent01:  iteration      200/   50000 | elapsed time per iteration (ms): 2647.8 | learning rate 5.460E-05 | lm loss 7.117331E+00 | loss scale 16384.0 |
Tencent01: time (ms) | forward: 576.80 | backward: 1949.75 | optimizer: 120.96 | batch generator: 1.27 | data loader: 0.28
Tencent01: [2020-11-11 06:55:46,097] [INFO] [logging.py:60:log_dist] [Rank 0] step=300, skipped=18, lr=[8.429999999999999e-05, 8.429999999999999e-05], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 06:55:46,148] [INFO] [timer.py:157:stop] 0/300, SamplesPerSec=48.39061474413318
Tencent01:  iteration      300/   50000 | elapsed time per iteration (ms): 2671.5 | learning rate 8.460E-05 | lm loss 6.510419E+00 | loss scale 16384.0 |
Tencent01: time (ms) | forward: 576.97 | backward: 1973.39 | optimizer: 120.91 | batch generator: 1.26 | data loader: 0.28
Tencent01: [2020-11-11 07:00:10,910] [INFO] [logging.py:60:log_dist] [Rank 0] step=400, skipped=18, lr=[0.00011429999999999999, 0.00011429999999999999], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 07:00:10,961] [INFO] [timer.py:157:stop] 0/400, SamplesPerSec=48.38405728399171
Tencent01:  iteration      400/   50000 | elapsed time per iteration (ms): 2648.1 | learning rate 1.146E-04 | lm loss 6.197698E+00 | loss scale 16384.0 |
Tencent01: time (ms) | forward: 576.77 | backward: 1949.92 | optimizer: 121.16 | batch generator: 1.25 | data loader: 0.28
Tencent01: [2020-11-11 07:04:35,653] [INFO] [logging.py:60:log_dist] [Rank 0] step=500, skipped=18, lr=[0.00014429999999999998, 0.00014429999999999998], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 07:04:35,705] [INFO] [timer.py:157:stop] 0/500, SamplesPerSec=48.3826115298935
Tencent01:  iteration      500/   50000 | elapsed time per iteration (ms): 2647.4 | learning rate 1.446E-04 | lm loss 5.900875E+00 | loss scale 16384.0 |
Tencent01: time (ms) | forward: 576.57 | backward: 1949.55 | optimizer: 121.02 | batch generator: 1.22 | data loader: 0.28
Tencent01: [2020-11-11 07:09:00,462] [INFO] [logging.py:60:log_dist] [Rank 0] step=600, skipped=18, lr=[0.0001499990286849794, 0.0001499990286849794], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 07:09:00,513] [INFO] [timer.py:157:stop] 0/600, SamplesPerSec=48.37949781555047
Tencent01:  iteration      600/   50000 | elapsed time per iteration (ms): 2648.1 | learning rate 1.500E-04 | lm loss 5.550071E+00 | loss scale 16384.0 |
Tencent01: time (ms) | forward: 576.80 | backward: 1950.70 | optimizer: 120.34 | batch generator: 1.22 | data loader: 0.26
Tencent01: [2020-11-11 07:13:27,872] [INFO] [logging.py:60:log_dist] [Rank 0] step=700, skipped=18, lr=[0.00014999514998062678, 0.00014999514998062678], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 07:13:27,924] [INFO] [timer.py:157:stop] 0/700, SamplesPerSec=48.30946336889846
Tencent01:  iteration      700/   50000 | elapsed time per iteration (ms): 2674.1 | learning rate 1.500E-04 | lm loss 5.199807E+00 | loss scale 16384.0 |
Tencent01: time (ms) | forward: 576.76 | backward: 1975.91 | optimizer: 121.16 | batch generator: 1.27 | data loader: 0.29
Tencent01: [2020-11-11 07:17:52,665] [INFO] [logging.py:60:log_dist] [Rank 0] step=800, skipped=18, lr=[0.00014998831059616512, 0.00014998831059616512], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 07:17:52,717] [INFO] [timer.py:157:stop] 0/800, SamplesPerSec=48.31676554997112
Tencent01:  iteration      800/   50000 | elapsed time per iteration (ms): 2647.9 | learning rate 1.500E-04 | lm loss 4.920318E+00 | loss scale 16384.0 |
Tencent01: time (ms) | forward: 576.96 | backward: 1949.55 | optimizer: 121.13 | batch generator: 1.23 | data loader: 0.29
Tencent01: [2020-11-11 07:22:17,413] [INFO] [logging.py:60:log_dist] [Rank 0] step=900, skipped=18, lr=[0.00014997851080160172, 0.00014997851080160172], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 07:22:17,465] [INFO] [timer.py:157:stop] 0/900, SamplesPerSec=48.32341202951831
Tencent01:  iteration      900/   50000 | elapsed time per iteration (ms): 2647.5 | learning rate 1.500E-04 | lm loss 4.600708E+00 | loss scale 16384.0 |
Tencent01: time (ms) | forward: 576.71 | backward: 1949.14 | optimizer: 121.32 | batch generator: 1.23 | data loader: 0.29
Tencent01: [2020-11-11 07:26:42,241] [INFO] [logging.py:60:log_dist] [Rank 0] step=1000, skipped=18, lr=[0.00014996575098381567, 0.00014996575098381567], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 07:26:42,293] [INFO] [timer.py:157:stop] 0/1000, SamplesPerSec=48.32724027380401
Tencent01:  iteration     1000/   50000 | elapsed time per iteration (ms): 2648.3 | learning rate 1.500E-04 | lm loss 4.393756E+00 | loss scale 16384.0 |
Tencent01: time (ms) | forward: 576.94 | backward: 1950.32 | optimizer: 120.74 | batch generator: 1.23 | data loader: 0.28
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: ----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 1000 | LM loss: 4.257572E+00 | LM PPL: 7.063824E+01
Tencent01: ----------------------------------------------------------------------------------
Tencent01: [2020-11-11 07:28:27,564] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 07:28:27,564] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent01: [2020-11-11 07:28:27,565] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 07:28:27,565] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 07:28:27,565] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent01: [2020-11-11 07:28:27,565] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 07:28:27,565] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent01: [2020-11-11 07:28:27,565] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent01: [2020-11-11 07:28:27,565] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 07:28:27,565] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent01: [2020-11-11 07:28:27,565] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 07:28:27,565] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent01: [2020-11-11 07:28:27,565] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 07:28:27,564] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 07:28:27,565] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent02: [2020-11-11 07:28:27,564] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent01: [2020-11-11 07:28:27,565] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 07:28:27,564] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 07:28:27,564] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent01: [2020-11-11 07:28:27,565] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent02: [2020-11-11 07:28:27,565] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 07:28:27,565] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 07:28:27,565] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent02: [2020-11-11 07:28:27,565] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 07:28:27,565] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent02: [2020-11-11 07:28:27,565] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent02: [2020-11-11 07:28:27,565] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 07:28:27,565] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 07:28:27,565] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent02: [2020-11-11 07:28:27,565] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 07:28:27,565] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent02: [2020-11-11 07:28:27,565] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent01: [2020-11-11 07:32:04,484] [INFO] [logging.py:60:log_dist] [Rank 0] step=1100, skipped=18, lr=[0.00014995003164654266, 0.00014995003164654266], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 07:32:04,536] [INFO] [timer.py:157:stop] 0/1100, SamplesPerSec=48.272564839819246
Tencent01:  iteration     1100/   50000 | elapsed time per iteration (ms): 3222.4 | learning rate 1.499E-04 | lm loss 4.243620E+00 | loss scale 32768.0 |
Tencent01: time (ms) | forward: 576.63 | backward: 1985.89 | optimizer: 120.25 | batch generator: 2.11 | data loader: 0.37
Tencent01: [2020-11-11 07:36:29,252] [INFO] [logging.py:60:log_dist] [Rank 0] step=1200, skipped=18, lr=[0.00014993135341035525, 0.00014993135341035525], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 07:36:29,304] [INFO] [timer.py:157:stop] 0/1200, SamplesPerSec=48.28091433616112
Tencent01:  iteration     1200/   50000 | elapsed time per iteration (ms): 2647.7 | learning rate 1.499E-04 | lm loss 4.122332E+00 | loss scale 32768.0 |
Tencent01: time (ms) | forward: 576.89 | backward: 1949.78 | optimizer: 120.74 | batch generator: 1.25 | data loader: 0.28
Tencent01: [2020-11-11 07:40:53,994] [INFO] [logging.py:60:log_dist] [Rank 0] step=1300, skipped=18, lr=[0.00014990971701263825, 0.00014990971701263825], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 07:40:54,045] [INFO] [timer.py:157:stop] 0/1300, SamplesPerSec=48.2883425338016
Tencent01:  iteration     1300/   50000 | elapsed time per iteration (ms): 2647.4 | learning rate 1.499E-04 | lm loss 4.071216E+00 | loss scale 32768.0 |
Tencent01: time (ms) | forward: 576.88 | backward: 1949.30 | optimizer: 120.96 | batch generator: 1.24 | data loader: 0.29
Tencent01: [2020-11-11 07:45:18,699] [INFO] [logging.py:60:log_dist] [Rank 0] step=1400, skipped=18, lr=[0.00014988512330755955, 0.00014988512330755955], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 07:45:18,751] [INFO] [timer.py:157:stop] 0/1400, SamplesPerSec=48.295126671710044
Tencent01:  iteration     1400/   50000 | elapsed time per iteration (ms): 2647.1 | learning rate 1.499E-04 | lm loss 3.966936E+00 | loss scale 32768.0 |
Tencent01: time (ms) | forward: 576.81 | backward: 1948.83 | optimizer: 121.14 | batch generator: 1.20 | data loader: 0.28
Tencent01: [2020-11-11 07:49:44,372] [INFO] [logging.py:60:log_dist] [Rank 0] step=1500, skipped=18, lr=[0.00014985757326603657, 0.00014985757326603657], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 07:49:44,425] [INFO] [timer.py:157:stop] 0/1500, SamplesPerSec=48.28926632862206
Tencent01:  iteration     1500/   50000 | elapsed time per iteration (ms): 2656.7 | learning rate 1.499E-04 | lm loss 3.881122E+00 | loss scale 32768.0 |
Tencent01: time (ms) | forward: 576.67 | backward: 1958.86 | optimizer: 120.94 | batch generator: 1.24 | data loader: 0.28
Tencent01: [2020-11-11 07:54:10,651] [INFO] [logging.py:60:log_dist] [Rank 0] step=1600, skipped=18, lr=[0.00014982706797569772, 0.00014982706797569772], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 07:54:10,703] [INFO] [timer.py:157:stop] 0/1600, SamplesPerSec=48.27721028897786
Tencent01:  iteration     1600/   50000 | elapsed time per iteration (ms): 2662.8 | learning rate 1.498E-04 | lm loss 3.860639E+00 | loss scale 32768.0 |
Tencent01: time (ms) | forward: 577.10 | backward: 1964.79 | optimizer: 120.63 | batch generator: 1.21 | data loader: 0.27
Tencent01: [2020-11-11 07:58:35,428] [INFO] [logging.py:60:log_dist] [Rank 0] step=1700, skipped=18, lr=[0.00014979360864083967, 0.00014979360864083967], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 07:58:35,479] [INFO] [timer.py:157:stop] 0/1700, SamplesPerSec=48.28271253173329
Tencent01:  iteration     1700/   50000 | elapsed time per iteration (ms): 2647.8 | learning rate 1.498E-04 | lm loss 3.814576E+00 | loss scale 32768.0 |
Tencent01: time (ms) | forward: 576.73 | backward: 1949.65 | optimizer: 121.11 | batch generator: 1.23 | data loader: 0.28
Tencent01: [2020-11-11 08:03:00,192] [INFO] [logging.py:60:log_dist] [Rank 0] step=1800, skipped=18, lr=[0.00014975719658237964, 0.00014975719658237964], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 08:03:00,244] [INFO] [timer.py:157:stop] 0/1800, SamplesPerSec=48.28770690597155
Tencent01:  iteration     1800/   50000 | elapsed time per iteration (ms): 2647.6 | learning rate 1.498E-04 | lm loss 3.748436E+00 | loss scale 32768.0 |
Tencent01: time (ms) | forward: 576.62 | backward: 1949.65 | optimizer: 121.12 | batch generator: 1.23 | data loader: 0.28
Tencent01: [2020-11-11 08:07:25,033] [INFO] [logging.py:60:log_dist] [Rank 0] step=1900, skipped=18, lr=[0.00014971783323780336, 0.00014971783323780336], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 08:07:25,084] [INFO] [timer.py:157:stop] 0/1900, SamplesPerSec=48.291427668290126
Tencent01:  iteration     1900/   50000 | elapsed time per iteration (ms): 2648.4 | learning rate 1.497E-04 | lm loss 3.712597E+00 | loss scale 32768.0 |
Tencent01: time (ms) | forward: 576.85 | backward: 1950.18 | optimizer: 121.13 | batch generator: 1.21 | data loader: 0.27
Tencent01: [2020-11-11 08:11:52,378] [INFO] [logging.py:60:log_dist] [Rank 0] step=2000, skipped=18, lr=[0.00014967552016110827, 0.00014967552016110827], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 08:11:52,430] [INFO] [timer.py:157:stop] 0/2000, SamplesPerSec=48.27195637374868
Tencent01:  iteration     2000/   50000 | elapsed time per iteration (ms): 2673.5 | learning rate 1.497E-04 | lm loss 3.688984E+00 | loss scale 32768.0 |
Tencent01: time (ms) | forward: 576.51 | backward: 1975.60 | optimizer: 121.09 | batch generator: 1.22 | data loader: 0.28
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: ----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 2000 | LM loss: 3.611153E+00 | LM PPL: 3.700871E+01
Tencent01: ----------------------------------------------------------------------------------
Tencent01: [2020-11-11 08:13:37,691] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 08:13:37,691] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent01: [2020-11-11 08:13:37,692] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 08:13:37,691] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 08:13:37,692] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent02: [2020-11-11 08:13:37,691] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent02: [2020-11-11 08:13:37,691] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 08:13:37,691] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent02: [2020-11-11 08:13:37,691] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 08:13:37,691] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent01: [2020-11-11 08:13:37,692] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 08:13:37,692] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 08:13:37,692] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent01: [2020-11-11 08:13:37,692] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent01: [2020-11-11 08:13:37,692] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 08:13:37,692] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent01: [2020-11-11 08:13:37,692] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 08:13:37,692] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 08:13:37,692] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent01: [2020-11-11 08:13:37,692] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent01: [2020-11-11 08:13:37,692] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 08:13:37,692] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent02: [2020-11-11 08:13:37,692] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 08:13:37,692] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent02: [2020-11-11 08:13:37,692] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 08:13:37,692] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent02: [2020-11-11 08:13:37,692] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 08:13:37,692] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 08:13:37,692] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent02: [2020-11-11 08:13:37,692] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent01: [2020-11-11 08:13:37,693] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 08:13:37,693] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent01: [2020-11-11 08:17:12,165] [INFO] [logging.py:60:log_dist] [Rank 0] step=2100, skipped=18, lr=[0.00014963025902274218, 0.00014963025902274218], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 08:17:12,217] [INFO] [timer.py:157:stop] 0/2100, SamplesPerSec=48.26714397345234
Tencent01:  iteration     2100/   50000 | elapsed time per iteration (ms): 3197.9 | learning rate 1.496E-04 | lm loss 3.651910E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.45 | backward: 1960.99 | optimizer: 121.00 | batch generator: 2.22 | data loader: 0.41
Tencent01: [2020-11-11 08:21:37,010] [INFO] [logging.py:60:log_dist] [Rank 0] step=2200, skipped=18, lr=[0.00014958205160953735, 0.00014958205160953735], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 08:21:37,063] [INFO] [timer.py:157:stop] 0/2200, SamplesPerSec=48.27128377941223
Tencent01:  iteration     2200/   50000 | elapsed time per iteration (ms): 2648.5 | learning rate 1.496E-04 | lm loss 3.673530E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.77 | backward: 1950.60 | optimizer: 120.81 | batch generator: 1.23 | data loader: 0.28
Tencent01: [2020-11-11 08:26:01,742] [INFO] [logging.py:60:log_dist] [Rank 0] step=2300, skipped=18, lr=[0.00014953089982463992, 0.00014953089982463992], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 08:26:01,794] [INFO] [timer.py:157:stop] 0/2300, SamplesPerSec=48.27595721543642
Tencent01:  iteration     2300/   50000 | elapsed time per iteration (ms): 2647.3 | learning rate 1.495E-04 | lm loss 3.611836E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.62 | backward: 1949.53 | optimizer: 120.90 | batch generator: 1.22 | data loader: 0.29
Tencent01: [2020-11-11 08:30:29,252] [INFO] [logging.py:60:log_dist] [Rank 0] step=2400, skipped=18, lr=[0.00014947680568743474, 0.00014947680568743474], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 08:30:29,304] [INFO] [timer.py:157:stop] 0/2400, SamplesPerSec=48.25914283928041
Tencent01:  iteration     2400/   50000 | elapsed time per iteration (ms): 2675.1 | learning rate 1.495E-04 | lm loss 3.574255E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.91 | backward: 1976.92 | optimizer: 121.00 | batch generator: 1.21 | data loader: 0.28
Tencent01: [2020-11-11 08:34:54,085] [INFO] [logging.py:60:log_dist] [Rank 0] step=2500, skipped=18, lr=[0.00014941977133346574, 0.00014941977133346574], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 08:34:54,137] [INFO] [timer.py:157:stop] 0/2500, SamplesPerSec=48.26315126742543
Tencent01:  iteration     2500/   50000 | elapsed time per iteration (ms): 2648.3 | learning rate 1.494E-04 | lm loss 3.567292E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.95 | backward: 1950.52 | optimizer: 120.60 | batch generator: 1.19 | data loader: 0.28
Tencent01: [2020-11-11 08:39:18,900] [INFO] [logging.py:60:log_dist] [Rank 0] step=2600, skipped=18, lr=[0.00014935979901435157, 0.00014935979901435157], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 08:39:18,952] [INFO] [timer.py:157:stop] 0/2600, SamplesPerSec=48.26695792016791
Tencent01:  iteration     2600/   50000 | elapsed time per iteration (ms): 2648.1 | learning rate 1.494E-04 | lm loss 3.585796E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.84 | backward: 1950.79 | optimizer: 120.29 | batch generator: 1.19 | data loader: 0.26
Tencent01: [2020-11-11 08:43:43,632] [INFO] [logging.py:60:log_dist] [Rank 0] step=2700, skipped=18, lr=[0.00014929689109769665, 0.00014929689109769665], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 08:43:43,684] [INFO] [timer.py:157:stop] 0/2700, SamplesPerSec=48.27101974270232
Tencent01:  iteration     2700/   50000 | elapsed time per iteration (ms): 2647.3 | learning rate 1.493E-04 | lm loss 3.513468E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.44 | backward: 1950.49 | optimizer: 120.17 | batch generator: 1.16 | data loader: 0.26
Tencent01: [2020-11-11 08:48:10,839] [INFO] [logging.py:60:log_dist] [Rank 0] step=2800, skipped=18, lr=[0.00014923105006699785, 0.00014923105006699785], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 08:48:10,891] [INFO] [timer.py:157:stop] 0/2800, SamplesPerSec=48.25870521593848
Tencent01:  iteration     2800/   50000 | elapsed time per iteration (ms): 2672.1 | learning rate 1.492E-04 | lm loss 3.509328E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.72 | backward: 1974.90 | optimizer: 120.21 | batch generator: 1.17 | data loader: 0.26
Tencent01: [2020-11-11 08:52:35,670] [INFO] [logging.py:60:log_dist] [Rank 0] step=2900, skipped=18, lr=[0.00014916227852154634, 0.00014916227852154634], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 08:52:35,722] [INFO] [timer.py:157:stop] 0/2900, SamplesPerSec=48.26220137334593
Tencent01:  iteration     2900/   50000 | elapsed time per iteration (ms): 2648.3 | learning rate 1.492E-04 | lm loss 3.456968E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.86 | backward: 1950.72 | optimizer: 120.48 | batch generator: 1.22 | data loader: 0.27
Tencent01: [2020-11-11 08:57:00,463] [INFO] [logging.py:60:log_dist] [Rank 0] step=3000, skipped=18, lr=[0.00014909057917632498, 0.00014909057917632498], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 08:57:00,514] [INFO] [timer.py:157:stop] 0/3000, SamplesPerSec=48.26573943346696
Tencent01:  iteration     3000/   50000 | elapsed time per iteration (ms): 2647.9 | learning rate 1.491E-04 | lm loss 3.454069E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 577.00 | backward: 1949.49 | optimizer: 121.16 | batch generator: 1.27 | data loader: 0.29
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: ----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 3000 | LM loss: 3.392464E+00 | LM PPL: 2.973913E+01
Tencent01: ----------------------------------------------------------------------------------
Tencent01: [2020-11-11 08:58:45,736] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 08:58:45,737] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-11 08:58:45,737] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 08:58:45,737] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-11 08:58:45,737] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 08:58:45,737] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-11 08:58:45,737] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 08:58:45,737] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 08:58:45,737] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-11 08:58:45,737] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 08:58:45,737] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-11 08:58:45,737] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-11 08:58:45,737] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 08:58:45,737] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-11 08:58:45,737] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 08:58:45,737] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-11 08:58:45,736] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 08:58:45,736] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-11 08:58:45,736] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 08:58:45,737] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-11 08:58:45,736] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 08:58:45,736] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 08:58:45,737] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 08:58:45,737] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-11 08:58:45,737] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 08:58:45,737] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-11 08:58:45,737] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-11 08:58:45,737] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-11 08:58:45,737] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 08:58:45,737] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-11 08:58:45,737] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 08:58:45,737] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-11 09:02:20,293] [INFO] [logging.py:60:log_dist] [Rank 0] step=3100, skipped=18, lr=[0.00014901595486190112, 0.00014901595486190112], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 09:02:20,345] [INFO] [timer.py:157:stop] 0/3100, SamplesPerSec=48.26277215714235
Tencent01:  iteration     3100/   50000 | elapsed time per iteration (ms): 3198.3 | learning rate 1.490E-04 | lm loss 3.429753E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 576.45 | backward: 1960.52 | optimizer: 121.35 | batch generator: 2.24 | data loader: 0.41
Tencent01: [2020-11-11 09:06:47,540] [INFO] [logging.py:60:log_dist] [Rank 0] step=3200, skipped=18, lr=[0.0001489384085243149, 0.0001489384085243149], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 09:06:47,592] [INFO] [timer.py:157:stop] 0/3200, SamplesPerSec=48.25208262307215
Tencent01:  iteration     3200/   50000 | elapsed time per iteration (ms): 2672.5 | learning rate 1.489E-04 | lm loss 3.447921E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 576.88 | backward: 1974.14 | optimizer: 121.19 | batch generator: 1.23 | data loader: 0.28
Tencent01: [2020-11-11 09:11:12,512] [INFO] [logging.py:60:log_dist] [Rank 0] step=3300, skipped=18, lr=[0.00014885794322496297, 0.00014885794322496297], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 09:11:12,564] [INFO] [timer.py:157:stop] 0/3300, SamplesPerSec=48.25459965993004
Tencent01:  iteration     3300/   50000 | elapsed time per iteration (ms): 2649.7 | learning rate 1.489E-04 | lm loss 3.419980E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 577.00 | backward: 1951.67 | optimizer: 120.78 | batch generator: 1.25 | data loader: 0.28
Tencent01: [2020-11-11 09:15:37,523] [INFO] [logging.py:60:log_dist] [Rank 0] step=3400, skipped=18, lr=[0.0001487745621404776, 0.0001487745621404776], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 09:15:37,575] [INFO] [timer.py:157:stop] 0/3400, SamplesPerSec=48.256720544142375
Tencent01:  iteration     3400/   50000 | elapsed time per iteration (ms): 2650.1 | learning rate 1.488E-04 | lm loss 3.416055E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 576.69 | backward: 1952.77 | optimizer: 120.42 | batch generator: 1.21 | data loader: 0.26
Tencent01: [2020-11-11 09:20:02,456] [INFO] [logging.py:60:log_dist] [Rank 0] step=3500, skipped=18, lr=[0.0001486882685626012, 0.0001486882685626012], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 09:20:02,509] [INFO] [timer.py:157:stop] 0/3500, SamplesPerSec=48.25910168097907
Tencent01:  iteration     3500/   50000 | elapsed time per iteration (ms): 2649.3 | learning rate 1.487E-04 | lm loss 3.382873E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 576.74 | backward: 1951.96 | optimizer: 120.40 | batch generator: 1.17 | data loader: 0.26
Tencent01: [2020-11-11 09:24:28,634] [INFO] [logging.py:60:log_dist] [Rank 0] step=3600, skipped=18, lr=[0.00014859906589805648, 0.00014859906589805648], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 09:24:28,686] [INFO] [timer.py:157:stop] 0/3600, SamplesPerSec=48.25508438893378
Tencent01:  iteration     3600/   50000 | elapsed time per iteration (ms): 2661.8 | learning rate 1.486E-04 | lm loss 3.368706E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 576.84 | backward: 1964.44 | optimizer: 120.26 | batch generator: 1.20 | data loader: 0.27
Tencent01: [2020-11-11 09:28:54,985] [INFO] [logging.py:60:log_dist] [Rank 0] step=3700, skipped=18, lr=[0.00014850695766841187, 0.00014850695766841187], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 09:28:55,037] [INFO] [timer.py:157:stop] 0/3700, SamplesPerSec=48.250449001742076
Tencent01:  iteration     3700/   50000 | elapsed time per iteration (ms): 2663.5 | learning rate 1.485E-04 | lm loss 3.352938E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 577.04 | backward: 1965.64 | optimizer: 120.58 | batch generator: 1.23 | data loader: 0.27
Tencent01: [2020-11-11 09:33:19,845] [INFO] [logging.py:60:log_dist] [Rank 0] step=3800, skipped=18, lr=[0.0001484119475099426, 0.0001484119475099426], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 09:33:19,897] [INFO] [timer.py:157:stop] 0/3800, SamplesPerSec=48.25319929147474
Tencent01:  iteration     3800/   50000 | elapsed time per iteration (ms): 2648.6 | learning rate 1.484E-04 | lm loss 3.350416E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 576.85 | backward: 1950.99 | optimizer: 120.50 | batch generator: 1.22 | data loader: 0.27
Tencent01: [2020-11-11 09:37:44,689] [INFO] [logging.py:60:log_dist] [Rank 0] step=3900, skipped=18, lr=[0.00014831403917348702, 0.00014831403917348702], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 09:37:44,741] [INFO] [timer.py:157:stop] 0/3900, SamplesPerSec=48.25584493174969
Tencent01:  iteration     3900/   50000 | elapsed time per iteration (ms): 2648.4 | learning rate 1.483E-04 | lm loss 3.324369E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 576.54 | backward: 1951.55 | optimizer: 120.13 | batch generator: 1.17 | data loader: 0.26
Tencent01: [2020-11-11 09:42:09,627] [INFO] [logging.py:60:log_dist] [Rank 0] step=4000, skipped=18, lr=[0.0001482132365242986, 0.0001482132365242986], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 09:42:09,679] [INFO] [timer.py:157:stop] 0/4000, SamplesPerSec=48.257919921637274
Tencent01:  iteration     4000/   50000 | elapsed time per iteration (ms): 2649.4 | learning rate 1.482E-04 | lm loss 3.355091E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 576.74 | backward: 1952.20 | optimizer: 120.21 | batch generator: 1.14 | data loader: 0.26
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: ----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 4000 | LM loss: 3.292757E+00 | LM PPL: 2.691697E+01
Tencent01: ----------------------------------------------------------------------------------
Tencent01: [2020-11-11 09:43:56,196] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 09:43:56,196] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-11 09:43:56,196] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 09:43:56,196] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-11 09:43:56,195] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 09:43:56,196] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 09:43:56,195] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-11 09:43:56,196] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 09:43:56,196] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-11 09:43:56,196] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-11 09:43:56,196] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 09:43:56,196] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 09:43:56,196] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-11 09:43:56,196] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 09:43:56,196] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-11 09:43:56,196] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-11 09:43:56,196] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 09:43:56,196] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-11 09:43:56,196] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 09:43:56,196] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 09:43:56,197] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-11 09:43:56,196] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 09:43:56,196] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-11 09:43:56,196] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-11 09:43:56,196] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 09:43:56,197] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 09:43:56,196] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-11 09:43:56,197] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-11 09:43:56,196] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 09:43:56,196] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 09:43:56,196] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-11 09:43:56,196] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-11 09:47:32,124] [INFO] [logging.py:60:log_dist] [Rank 0] step=4100, skipped=18, lr=[0.00014810954354189335, 0.00014810954354189335], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 09:47:32,176] [INFO] [timer.py:157:stop] 0/4100, SamplesPerSec=48.244042440504195
Tencent01:  iteration     4100/   50000 | elapsed time per iteration (ms): 3225.0 | learning rate 1.481E-04 | lm loss 3.339048E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 576.43 | backward: 1987.53 | optimizer: 121.02 | batch generator: 2.22 | data loader: 0.41
Tencent01: [2020-11-11 09:51:57,089] [INFO] [logging.py:60:log_dist] [Rank 0] step=4200, skipped=18, lr=[0.00014800296431989268, 0.00014800296431989268], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 09:51:57,141] [INFO] [timer.py:157:stop] 0/4200, SamplesPerSec=48.24623344784168
Tencent01:  iteration     4200/   50000 | elapsed time per iteration (ms): 2649.6 | learning rate 1.480E-04 | lm loss 3.311277E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 576.86 | backward: 1951.26 | optimizer: 121.26 | batch generator: 1.22 | data loader: 0.29
Tencent01: [2020-11-11 09:56:22,083] [INFO] [logging.py:60:log_dist] [Rank 0] step=4300, skipped=18, lr=[0.00014789350306586174, 0.00014789350306586174], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 09:56:22,134] [INFO] [timer.py:157:stop] 0/4300, SamplesPerSec=48.248217715801104
Tencent01:  iteration     4300/   50000 | elapsed time per iteration (ms): 2649.9 | learning rate 1.479E-04 | lm loss 3.303903E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 576.86 | backward: 1951.72 | optimizer: 121.09 | batch generator: 1.26 | data loader: 0.28
Tencent01: [2020-11-11 10:00:46,996] [INFO] [logging.py:60:log_dist] [Rank 0] step=4400, skipped=18, lr=[0.0001477811641011434, 0.0001477811641011434], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 10:00:47,048] [INFO] [timer.py:157:stop] 0/4400, SamplesPerSec=48.250430117855394
Tencent01:  iteration     4400/   50000 | elapsed time per iteration (ms): 2649.1 | learning rate 1.478E-04 | lm loss 3.318620E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 576.90 | backward: 1951.00 | optimizer: 120.96 | batch generator: 1.23 | data loader: 0.29
Tencent01: [2020-11-11 10:05:14,465] [INFO] [logging.py:60:log_dist] [Rank 0] step=4500, skipped=18, lr=[0.0001476659518606877, 0.0001476659518606877], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 10:05:14,517] [INFO] [timer.py:157:stop] 0/4500, SamplesPerSec=48.24221957974015
Tencent01:  iteration     4500/   50000 | elapsed time per iteration (ms): 2674.7 | learning rate 1.477E-04 | lm loss 3.294683E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 576.67 | backward: 1976.67 | optimizer: 121.08 | batch generator: 1.25 | data loader: 0.29
Tencent01: [2020-11-11 10:09:39,312] [INFO] [logging.py:60:log_dist] [Rank 0] step=4600, skipped=18, lr=[0.00014754787089287657, 0.00014754787089287657], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 10:09:39,364] [INFO] [timer.py:157:stop] 0/4600, SamplesPerSec=48.244726265784614
Tencent01:  iteration     4600/   50000 | elapsed time per iteration (ms): 2648.5 | learning rate 1.475E-04 | lm loss 3.274910E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 576.73 | backward: 1950.39 | optimizer: 121.09 | batch generator: 1.23 | data loader: 0.29
Tencent01: [2020-11-11 10:14:04,141] [INFO] [logging.py:60:log_dist] [Rank 0] step=4700, skipped=18, lr=[0.00014742692585934444, 0.00014742692585934444], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 10:14:04,193] [INFO] [timer.py:157:stop] 0/4700, SamplesPerSec=48.24718972470349
Tencent01:  iteration     4700/   50000 | elapsed time per iteration (ms): 2648.3 | learning rate 1.474E-04 | lm loss 3.255845E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 576.51 | backward: 1950.84 | optimizer: 120.66 | batch generator: 1.20 | data loader: 0.28
Tencent01: [2020-11-11 10:18:29,082] [INFO] [logging.py:60:log_dist] [Rank 0] step=4800, skipped=18, lr=[0.00014730312153479413, 0.00014730312153479413], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 10:18:29,134] [INFO] [timer.py:157:stop] 0/4800, SamplesPerSec=48.24914157748913
Tencent01:  iteration     4800/   50000 | elapsed time per iteration (ms): 2649.4 | learning rate 1.473E-04 | lm loss 3.236843E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 576.99 | backward: 1951.19 | optimizer: 120.97 | batch generator: 1.25 | data loader: 0.28
Tencent01: [2020-11-11 10:22:56,587] [INFO] [logging.py:60:log_dist] [Rank 0] step=4900, skipped=18, lr=[0.00014717646280680842, 0.00014717646280680842], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 10:22:56,639] [INFO] [timer.py:157:stop] 0/4900, SamplesPerSec=48.2414869177883
Tencent01:  iteration     4900/   50000 | elapsed time per iteration (ms): 2675.1 | learning rate 1.472E-04 | lm loss 3.250610E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 576.69 | backward: 1977.44 | optimizer: 120.65 | batch generator: 1.22 | data loader: 0.28
Tencent01: [2020-11-11 10:27:21,489] [INFO] [logging.py:60:log_dist] [Rank 0] step=5000, skipped=18, lr=[0.00014704695467565698, 0.00014704695467565698], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 10:27:21,541] [INFO] [timer.py:157:stop] 0/5000, SamplesPerSec=48.243620243938224
Tencent01:  iteration     5000/   50000 | elapsed time per iteration (ms): 2649.0 | learning rate 1.470E-04 | lm loss 3.261484E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 576.91 | backward: 1950.69 | optimizer: 121.14 | batch generator: 1.24 | data loader: 0.29
Tencent01: [2020-11-11 10:27:21,544] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: checkpoints/gpt-345M11-11-06-41/5000/mp_rank_00_model_states.pt
Tencent01: VM-0-14-centos:107:809 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent01: VM-0-14-centos:107:809 [1] NCCL INFO Trees [0] 6/-1/-1->1->3|3->1->6/-1/-1 [1] 6/-1/-1->1->3|3->1->6/-1/-1
Tencent02: VM-0-9-centos:80:806 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent02: VM-0-9-centos:81:809 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent01: VM-0-14-centos:108:807 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent01: VM-0-14-centos:106:806 [0] NCCL INFO Channel 00/02 :    0   1   3   2   5  12  14  15   8   9  11  10  13   4   6   7
Tencent02: VM-0-9-centos:80:806 [5] NCCL INFO Trees [0] 15/-1/-1->13->12|12->13->15/-1/-1 [1] 15/4/-1->13->12|12->13->15/4/-1
Tencent01: VM-0-14-centos:109:810 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent01: VM-0-14-centos:107:809 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff
Tencent01: VM-0-14-centos:111:808 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent02: VM-0-9-centos:81:809 [6] NCCL INFO Trees [0] -1/-1/-1->14->9|9->14->-1/-1/-1 [1] -1/-1/-1->14->9|9->14->-1/-1/-1
Tencent01: VM-0-14-centos:110:812 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent02: VM-0-9-centos:82:810 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent02: VM-0-9-centos:80:806 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000
Tencent01: VM-0-14-centos:113:813 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent02: VM-0-9-centos:81:809 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000
Tencent01: VM-0-14-centos:108:807 [2] NCCL INFO Trees [0] 3/-1/-1->2->0|0->2->3/-1/-1 [1] 3/-1/-1->2->0|0->2->3/-1/-1
Tencent02: VM-0-9-centos:82:810 [7] NCCL INFO Trees [0] 8/-1/-1->15->13|13->15->8/-1/-1 [1] 8/-1/-1->15->13|13->15->8/-1/-1
Tencent01: VM-0-14-centos:109:810 [3] NCCL INFO Trees [0] 1/-1/-1->3->2|2->3->1/-1/-1 [1] 1/-1/-1->3->2|2->3->1/-1/-1
Tencent02: VM-0-9-centos:82:810 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000
Tencent01: VM-0-14-centos:106:806 [0] NCCL INFO Channel 01/02 :    0   1   3   2   5  12  14  15   8   9  11  10  13   4   6   7
Tencent02: VM-0-9-centos:78:808 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent02: VM-0-9-centos:78:808 [3] NCCL INFO Trees [0] 9/-1/-1->11->10|10->11->9/-1/-1 [1] 9/-1/-1->11->10|10->11->9/-1/-1
Tencent01: VM-0-14-centos:111:808 [5] NCCL INFO Trees [0] 7/12/-1->5->4|4->5->7/12/-1 [1] 7/-1/-1->5->4|4->5->7/-1/-1
Tencent02: VM-0-9-centos:79:807 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent01: VM-0-14-centos:110:812 [4] NCCL INFO Trees [0] 5/-1/-1->4->-1|-1->4->5/-1/-1 [1] 5/-1/-1->4->13|13->4->5/-1/-1
Tencent01: VM-0-14-centos:113:813 [7] NCCL INFO Trees [0] 0/-1/-1->7->5|5->7->0/-1/-1 [1] 0/-1/-1->7->5|5->7->0/-1/-1
Tencent02: VM-0-9-centos:78:808 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff
Tencent01: VM-0-14-centos:108:807 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff
Tencent02: VM-0-9-centos:79:807 [4] NCCL INFO Trees [0] 13/-1/-1->12->5|5->12->13/-1/-1 [1] 13/-1/-1->12->-1|-1->12->13/-1/-1
Tencent01: VM-0-14-centos:109:810 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff
Tencent02: VM-0-9-centos:77:811 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent02: VM-0-9-centos:79:807 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000
Tencent01: VM-0-14-centos:110:812 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000
Tencent02: VM-0-9-centos:76:813 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent02: VM-0-9-centos:77:811 [2] NCCL INFO Trees [0] 11/-1/-1->10->8|8->10->11/-1/-1 [1] 11/-1/-1->10->8|8->10->11/-1/-1
Tencent01: VM-0-14-centos:111:808 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000
Tencent02: VM-0-9-centos:76:813 [1] NCCL INFO Trees [0] 14/-1/-1->9->11|11->9->14/-1/-1 [1] 14/-1/-1->9->11|11->9->14/-1/-1
Tencent01: VM-0-14-centos:112:811 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent02: VM-0-9-centos:77:811 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff
Tencent02: VM-0-9-centos:76:813 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff
Tencent02: VM-0-9-centos:75:812 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent01: VM-0-14-centos:113:813 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000
Tencent02: VM-0-9-centos:75:812 [0] NCCL INFO Trees [0] 10/-1/-1->8->15|15->8->10/-1/-1 [1] 10/-1/-1->8->15|15->8->10/-1/-1
Tencent01: VM-0-14-centos:112:811 [6] NCCL INFO Trees [0] -1/-1/-1->6->1|1->6->-1/-1/-1 [1] -1/-1/-1->6->1|1->6->-1/-1/-1
Tencent02: VM-0-9-centos:75:812 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff
Tencent01: VM-0-14-centos:112:811 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000
Tencent01: VM-0-14-centos:106:806 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent01: VM-0-14-centos:106:806 [0] NCCL INFO Trees [0] 2/-1/-1->0->7|7->0->2/-1/-1 [1] 2/-1/-1->0->7|7->0->2/-1/-1
Tencent01: VM-0-14-centos:106:806 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff
Tencent01: VM-0-14-centos:108:807 [2] NCCL INFO Ring 00 : 2[3d000] -> 5[89000] via P2P/IPC
Tencent01: VM-0-14-centos:113:813 [7] NCCL INFO Ring 00 : 7[b2000] -> 0[1a000] via P2P/IPC
Tencent01: VM-0-14-centos:106:806 [0] NCCL INFO Ring 00 : 0[1a000] -> 1[1b000] via P2P/IPC
Tencent02: VM-0-9-centos:82:810 [7] NCCL INFO Ring 00 : 15[b2000] -> 8[1a000] via P2P/IPC
Tencent02: VM-0-9-centos:75:812 [0] NCCL INFO Ring 00 : 8[1a000] -> 9[1b000] via P2P/IPC
Tencent02: VM-0-9-centos:78:808 [3] NCCL INFO Ring 00 : 11[3e000] -> 10[3d000] via P2P/IPC
Tencent02: VM-0-9-centos:77:811 [2] NCCL INFO Ring 00 : 10[3d000] -> 13[89000] via P2P/IPC
Tencent02: VM-0-9-centos:81:809 [6] NCCL INFO Ring 00 : 14[b1000] -> 15[b2000] via P2P/IPC
Tencent01: VM-0-14-centos:110:812 [4] NCCL INFO Ring 00 : 13[89000] -> 4[88000] [receive] via NET/IB/0
Tencent02: VM-0-9-centos:76:813 [1] NCCL INFO Ring 00 : 9[1b000] -> 11[3e000] via P2P/IPC
Tencent01: VM-0-14-centos:109:810 [3] NCCL INFO Ring 00 : 3[3e000] -> 2[3d000] via P2P/IPC
Tencent01: VM-0-14-centos:112:811 [6] NCCL INFO Ring 00 : 6[b1000] -> 7[b2000] via P2P/IPC
Tencent01: VM-0-14-centos:107:809 [1] NCCL INFO Ring 00 : 1[1b000] -> 3[3e000] via P2P/IPC
Tencent02: VM-0-9-centos:79:807 [4] NCCL INFO Ring 00 : 5[89000] -> 12[88000] [receive] via NET/IB/0
Tencent01: VM-0-14-centos:110:812 [4] NCCL INFO Ring 00 : 4[88000] -> 6[b1000] via P2P/IPC
Tencent01: VM-0-14-centos:111:808 [5] NCCL INFO Ring 00 : 5[89000] -> 12[88000] [send] via NET/IB/0
Tencent02: VM-0-9-centos:80:806 [5] NCCL INFO Ring 00 : 13[89000] -> 4[88000] [send] via NET/IB/0
Tencent02: VM-0-9-centos:79:807 [4] NCCL INFO Ring 00 : 12[88000] -> 14[b1000] via P2P/IPC
Tencent01: VM-0-14-centos:108:807 [2] NCCL INFO Ring 00 : 2[3d000] -> 0[1a000] via P2P/IPC
Tencent02: VM-0-9-centos:77:811 [2] NCCL INFO Ring 00 : 10[3d000] -> 8[1a000] via P2P/IPC
Tencent01: VM-0-14-centos:112:811 [6] NCCL INFO Ring 00 : 6[b1000] -> 1[1b000] via P2P/IPC
Tencent01: VM-0-14-centos:113:813 [7] NCCL INFO Ring 00 : 7[b2000] -> 5[89000] via P2P/IPC
Tencent01: VM-0-14-centos:106:806 [0] NCCL INFO Ring 00 : 0[1a000] -> 7[b2000] via P2P/IPC
Tencent01: VM-0-14-centos:109:810 [3] NCCL INFO Ring 00 : 3[3e000] -> 1[1b000] via P2P/IPC
Tencent02: VM-0-9-centos:82:810 [7] NCCL INFO Ring 00 : 15[b2000] -> 13[89000] via P2P/IPC
Tencent02: VM-0-9-centos:75:812 [0] NCCL INFO Ring 00 : 8[1a000] -> 15[b2000] via P2P/IPC
Tencent02: VM-0-9-centos:81:809 [6] NCCL INFO Ring 00 : 14[b1000] -> 9[1b000] via P2P/IPC
Tencent02: VM-0-9-centos:78:808 [3] NCCL INFO Ring 00 : 11[3e000] -> 9[1b000] via P2P/IPC
Tencent01: VM-0-14-centos:108:807 [2] NCCL INFO Ring 00 : 2[3d000] -> 3[3e000] via P2P/IPC
Tencent01: VM-0-14-centos:106:806 [0] NCCL INFO Ring 00 : 0[1a000] -> 2[3d000] via P2P/IPC
Tencent01: VM-0-14-centos:107:809 [1] NCCL INFO Ring 00 : 1[1b000] -> 6[b1000] via P2P/IPC
Tencent01: VM-0-14-centos:111:808 [5] NCCL INFO Ring 00 : 12[88000] -> 5[89000] [receive] via NET/IB/0
Tencent02: VM-0-9-centos:80:806 [5] NCCL INFO Ring 00 : 13[89000] -> 12[88000] via P2P/IPC
Tencent02: VM-0-9-centos:77:811 [2] NCCL INFO Ring 00 : 10[3d000] -> 11[3e000] via P2P/IPC
Tencent02: VM-0-9-centos:75:812 [0] NCCL INFO Ring 00 : 8[1a000] -> 10[3d000] via P2P/IPC
Tencent01: VM-0-14-centos:111:808 [5] NCCL INFO Ring 00 : 5[89000] -> 4[88000] via P2P/IPC
Tencent02: VM-0-9-centos:76:813 [1] NCCL INFO Ring 00 : 9[1b000] -> 14[b1000] via P2P/IPC
Tencent02: VM-0-9-centos:79:807 [4] NCCL INFO Ring 00 : 12[88000] -> 5[89000] [send] via NET/IB/0
Tencent01: VM-0-14-centos:109:810 [3] NCCL INFO Ring 01 : 3[3e000] -> 2[3d000] via P2P/IPC
Tencent01: VM-0-14-centos:106:806 [0] NCCL INFO Ring 01 : 0[1a000] -> 1[1b000] via P2P/IPC
Tencent01: VM-0-14-centos:112:811 [6] NCCL INFO Ring 01 : 6[b1000] -> 7[b2000] via P2P/IPC
Tencent01: VM-0-14-centos:108:807 [2] NCCL INFO Ring 01 : 2[3d000] -> 5[89000] via P2P/IPC
Tencent01: VM-0-14-centos:110:812 [4] NCCL INFO Ring 00 : 4[88000] -> 5[89000] via P2P/IPC
Tencent01: VM-0-14-centos:107:809 [1] NCCL INFO Ring 01 : 1[1b000] -> 3[3e000] via P2P/IPC
Tencent01: VM-0-14-centos:111:808 [5] NCCL INFO Ring 00 : 5[89000] -> 7[b2000] via P2P/IPC
Tencent01: VM-0-14-centos:109:810 [3] NCCL INFO Ring 01 : 3[3e000] -> 1[1b000] via P2P/IPC
Tencent02: VM-0-9-centos:75:812 [0] NCCL INFO Ring 01 : 8[1a000] -> 9[1b000] via P2P/IPC
Tencent02: VM-0-9-centos:80:806 [5] NCCL INFO Ring 00 : 13[89000] -> 15[b2000] via P2P/IPC
Tencent02: VM-0-9-centos:78:808 [3] NCCL INFO Ring 01 : 11[3e000] -> 10[3d000] via P2P/IPC
Tencent02: VM-0-9-centos:77:811 [2] NCCL INFO Ring 01 : 10[3d000] -> 13[89000] via P2P/IPC
Tencent02: VM-0-9-centos:79:807 [4] NCCL INFO Ring 00 : 12[88000] -> 13[89000] via P2P/IPC
Tencent02: VM-0-9-centos:81:809 [6] NCCL INFO Ring 01 : 14[b1000] -> 15[b2000] via P2P/IPC
Tencent01: VM-0-14-centos:113:813 [7] NCCL INFO Ring 01 : 7[b2000] -> 0[1a000] via P2P/IPC
Tencent01: VM-0-14-centos:110:812 [4] NCCL INFO Ring 01 : 13[89000] -> 4[88000] [receive] via NET/IB/0
Tencent02: VM-0-9-centos:76:813 [1] NCCL INFO Ring 01 : 9[1b000] -> 11[3e000] via P2P/IPC
Tencent01: VM-0-14-centos:110:812 [4] NCCL INFO Ring 01 : 4[88000] -> 6[b1000] via P2P/IPC
Tencent01: VM-0-14-centos:108:807 [2] NCCL INFO Ring 01 : 2[3d000] -> 0[1a000] via P2P/IPC
Tencent01: VM-0-14-centos:111:808 [5] NCCL INFO Ring 01 : 5[89000] -> 12[88000] [send] via NET/IB/0
Tencent02: VM-0-9-centos:82:810 [7] NCCL INFO Ring 01 : 15[b2000] -> 8[1a000] via P2P/IPC
Tencent01: VM-0-14-centos:106:806 [0] NCCL INFO Ring 01 : 0[1a000] -> 7[b2000] via P2P/IPC
Tencent01: VM-0-14-centos:112:811 [6] NCCL INFO Ring 01 : 6[b1000] -> 1[1b000] via P2P/IPC
Tencent01: VM-0-14-centos:113:813 [7] NCCL INFO Ring 01 : 7[b2000] -> 5[89000] via P2P/IPC
Tencent02: VM-0-9-centos:79:807 [4] NCCL INFO Ring 01 : 5[89000] -> 12[88000] [receive] via NET/IB/0
Tencent02: VM-0-9-centos:78:808 [3] NCCL INFO Ring 01 : 11[3e000] -> 9[1b000] via P2P/IPC
Tencent01: VM-0-14-centos:108:807 [2] NCCL INFO Ring 01 : 2[3d000] -> 3[3e000] via P2P/IPC
Tencent01: VM-0-14-centos:106:806 [0] NCCL INFO Ring 01 : 0[1a000] -> 2[3d000] via P2P/IPC
Tencent02: VM-0-9-centos:77:811 [2] NCCL INFO Ring 01 : 10[3d000] -> 8[1a000] via P2P/IPC
Tencent01: VM-0-14-centos:106:806 [0] NCCL INFO comm 0x7ef918001020 rank 0 nranks 16 cudaDev 0 busId 1a000 - Init COMPLETE
Tencent01: VM-0-14-centos:106:106 [0] NCCL INFO Launch mode Parallel
Tencent01: VM-0-14-centos:107:809 [1] NCCL INFO Ring 01 : 1[1b000] -> 6[b1000] via P2P/IPC
Tencent02: VM-0-9-centos:79:807 [4] NCCL INFO Ring 01 : 12[88000] -> 14[b1000] via P2P/IPC
Tencent01: VM-0-14-centos:108:807 [2] NCCL INFO comm 0x7f6084001020 rank 2 nranks 16 cudaDev 2 busId 3d000 - Init COMPLETE
Tencent02: VM-0-9-centos:80:806 [5] NCCL INFO Ring 01 : 13[89000] -> 4[88000] [send] via NET/IB/0
Tencent01: VM-0-14-centos:109:810 [3] NCCL INFO comm 0x7f80b4001020 rank 3 nranks 16 cudaDev 3 busId 3e000 - Init COMPLETE
Tencent01: VM-0-14-centos:112:811 [6] NCCL INFO comm 0x7f5184001020 rank 6 nranks 16 cudaDev 6 busId b1000 - Init COMPLETE
Tencent02: VM-0-9-centos:75:812 [0] NCCL INFO Ring 01 : 8[1a000] -> 15[b2000] via P2P/IPC
Tencent01: VM-0-14-centos:107:809 [1] NCCL INFO comm 0x7f2564001020 rank 1 nranks 16 cudaDev 1 busId 1b000 - Init COMPLETE
Tencent02: VM-0-9-centos:82:810 [7] NCCL INFO Ring 01 : 15[b2000] -> 13[89000] via P2P/IPC
Tencent01: VM-0-14-centos:111:808 [5] NCCL INFO Ring 01 : 5[89000] -> 4[88000] via P2P/IPC
Tencent02: VM-0-9-centos:81:809 [6] NCCL INFO Ring 01 : 14[b1000] -> 9[1b000] via P2P/IPC
Tencent01: VM-0-14-centos:111:808 [5] NCCL INFO Ring 01 : 5[89000] -> 7[b2000] via P2P/IPC
Tencent02: VM-0-9-centos:77:811 [2] NCCL INFO Ring 01 : 10[3d000] -> 11[3e000] via P2P/IPC
Tencent01: VM-0-14-centos:113:813 [7] NCCL INFO comm 0x7f0ab8001020 rank 7 nranks 16 cudaDev 7 busId b2000 - Init COMPLETE
Tencent02: VM-0-9-centos:75:812 [0] NCCL INFO Ring 01 : 8[1a000] -> 10[3d000] via P2P/IPC
Tencent01: VM-0-14-centos:110:812 [4] NCCL INFO Ring 01 : 4[88000] -> 13[89000] [send] via NET/IB/0
Tencent02: VM-0-9-centos:75:812 [0] NCCL INFO comm 0x7f93b8001020 rank 8 nranks 16 cudaDev 0 busId 1a000 - Init COMPLETE
Tencent02: VM-0-9-centos:77:811 [2] NCCL INFO comm 0x7fee74001020 rank 10 nranks 16 cudaDev 2 busId 3d000 - Init COMPLETE
Tencent02: VM-0-9-centos:76:813 [1] NCCL INFO Ring 01 : 9[1b000] -> 14[b1000] via P2P/IPC
Tencent02: VM-0-9-centos:78:808 [3] NCCL INFO comm 0x7f9bcc001020 rank 11 nranks 16 cudaDev 3 busId 3e000 - Init COMPLETE
Tencent02: VM-0-9-centos:81:809 [6] NCCL INFO comm 0x7fa03c001020 rank 14 nranks 16 cudaDev 6 busId b1000 - Init COMPLETE
Tencent02: VM-0-9-centos:80:806 [5] NCCL INFO Ring 01 : 4[88000] -> 13[89000] [receive] via NET/IB/0
Tencent02: VM-0-9-centos:76:813 [1] NCCL INFO comm 0x7fec60001020 rank 9 nranks 16 cudaDev 1 busId 1b000 - Init COMPLETE
Tencent02: VM-0-9-centos:80:806 [5] NCCL INFO Ring 01 : 13[89000] -> 12[88000] via P2P/IPC
Tencent01: VM-0-14-centos:110:812 [4] NCCL INFO Ring 01 : 4[88000] -> 5[89000] via P2P/IPC
Tencent01: VM-0-14-centos:110:812 [4] NCCL INFO comm 0x7f0f8c001020 rank 4 nranks 16 cudaDev 4 busId 88000 - Init COMPLETE
Tencent02: VM-0-9-centos:79:807 [4] NCCL INFO Ring 01 : 12[88000] -> 13[89000] via P2P/IPC
Tencent01: VM-0-14-centos:111:808 [5] NCCL INFO comm 0x7f00cc001020 rank 5 nranks 16 cudaDev 5 busId 89000 - Init COMPLETE
Tencent02: VM-0-9-centos:79:807 [4] NCCL INFO comm 0x7ef26c001020 rank 12 nranks 16 cudaDev 4 busId 88000 - Init COMPLETE
Tencent02: VM-0-9-centos:80:806 [5] NCCL INFO Ring 01 : 13[89000] -> 15[b2000] via P2P/IPC
Tencent02: VM-0-9-centos:82:810 [7] NCCL INFO comm 0x7f0d60001020 rank 15 nranks 16 cudaDev 7 busId b2000 - Init COMPLETE
Tencent02: VM-0-9-centos:80:806 [5] NCCL INFO comm 0x7f5de0001020 rank 13 nranks 16 cudaDev 5 busId 89000 - Init COMPLETE
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: ----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 5000 | LM loss: 3.202769E+00 | LM PPL: 2.460057E+01
Tencent01: ----------------------------------------------------------------------------------
Tencent01: [2020-11-11 10:29:43,178] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 10:29:43,178] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-11 10:29:43,178] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 10:29:43,178] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-11 10:29:43,178] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 10:29:43,177] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 10:29:43,178] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-11 10:29:43,178] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-11 10:29:43,178] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 10:29:43,178] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-11 10:29:43,178] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 10:29:43,178] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 10:29:43,178] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 10:29:43,179] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 10:29:43,179] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-11 10:29:43,179] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 10:29:43,179] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-11 10:29:43,178] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-11 10:29:43,178] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 10:29:43,178] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-11 10:29:43,179] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-11 10:29:43,178] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-11 10:29:43,179] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 10:29:43,179] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-11 10:29:43,179] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 10:29:43,178] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 10:29:43,178] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-11 10:29:43,179] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-11 10:29:43,178] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 10:29:43,178] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 10:29:43,178] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-11 10:29:43,178] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-11 10:32:06,126] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 5072
Tencent01: [2020-11-11 10:32:06,126] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 5072
Tencent01: [2020-11-11 10:32:06,126] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 5072
Tencent01: [2020-11-11 10:32:06,126] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 5072
Tencent01: [2020-11-11 10:32:06,126] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 5072
Tencent01: [2020-11-11 10:32:06,126] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-11 10:32:06,126] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-11 10:32:06,126] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-11 10:32:06,126] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-11 10:32:06,126] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-11 10:32:06,126] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 5072
Tencent01: [2020-11-11 10:32:06,126] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
Tencent01: [2020-11-11 10:32:06,126] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-11 10:32:06,126] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 5072
Tencent02: [2020-11-11 10:32:06,125] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-11 10:32:06,126] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: Grad overflow on iteration 5072
Tencent01: [2020-11-11 10:32:06,126] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 5072
Tencent02: [2020-11-11 10:32:06,126] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-11 10:32:06,125] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 5072
Tencent02: [2020-11-11 10:32:06,125] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 5072
Tencent01: [2020-11-11 10:32:06,126] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-11 10:32:06,126] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-11 10:32:06,126] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-11 10:32:06,126] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 5072
Tencent02: [2020-11-11 10:32:06,126] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-11 10:32:06,126] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 5072
Tencent02: [2020-11-11 10:32:06,126] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 5072
Tencent02: [2020-11-11 10:32:06,126] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 5072
Tencent02: [2020-11-11 10:32:06,126] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-11 10:32:06,126] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-11 10:32:06,126] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-11 10:32:06,126] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 5072
Tencent02: [2020-11-11 10:32:06,126] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-11 10:33:17,593] [INFO] [logging.py:60:log_dist] [Rank 0] step=5100, skipped=19, lr=[0.00014691593984048876, 0.00014691593984048876], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 10:33:17,645] [INFO] [timer.py:157:stop] 0/5100, SamplesPerSec=48.24176571033635
Tencent01:  iteration     5100/   50000 | elapsed time per iteration (ms): 3561.0 | learning rate 1.469E-04 | lm loss 3.247597E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 576.21 | backward: 1963.38 | optimizer: 120.09 | batch generator: 2.30 | data loader: 0.43
Tencent01: [2020-11-11 10:37:42,657] [INFO] [logging.py:60:log_dist] [Rank 0] step=5200, skipped=19, lr=[0.00014678077671799282, 0.00014678077671799282], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 10:37:42,709] [INFO] [timer.py:157:stop] 0/5200, SamplesPerSec=48.243256939938256
Tencent01:  iteration     5200/   50000 | elapsed time per iteration (ms): 2650.6 | learning rate 1.468E-04 | lm loss 3.201566E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 577.03 | backward: 1952.11 | optimizer: 121.22 | batch generator: 1.28 | data loader: 0.30
Tencent01: [2020-11-11 10:42:10,153] [INFO] [logging.py:60:log_dist] [Rank 0] step=5300, skipped=19, lr=[0.00014664277981334044, 0.00014664277981334044], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 10:42:10,205] [INFO] [timer.py:157:stop] 0/5300, SamplesPerSec=48.23633854126128
Tencent01:  iteration     5300/   50000 | elapsed time per iteration (ms): 2675.0 | learning rate 1.466E-04 | lm loss 3.213746E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 576.88 | backward: 1976.25 | optimizer: 121.55 | batch generator: 1.26 | data loader: 0.29
Tencent01: [2020-11-11 10:46:35,181] [INFO] [logging.py:60:log_dist] [Rank 0] step=5400, skipped=19, lr=[0.00014650195457441314, 0.00014650195457441314], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 10:46:35,233] [INFO] [timer.py:157:stop] 0/5400, SamplesPerSec=48.2379951022191
Tencent01:  iteration     5400/   50000 | elapsed time per iteration (ms): 2650.3 | learning rate 1.465E-04 | lm loss 3.242824E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 577.15 | backward: 1951.91 | optimizer: 120.95 | batch generator: 1.28 | data loader: 0.29
Tencent01: [2020-11-11 10:51:00,056] [INFO] [logging.py:60:log_dist] [Rank 0] step=5500, skipped=19, lr=[0.00014635830656075017, 0.00014635830656075017], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 10:51:00,108] [INFO] [timer.py:157:stop] 0/5500, SamplesPerSec=48.240072931369724
Tencent01:  iteration     5500/   50000 | elapsed time per iteration (ms): 2648.8 | learning rate 1.464E-04 | lm loss 3.198471E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 576.72 | backward: 1950.44 | optimizer: 121.32 | batch generator: 1.22 | data loader: 0.29
Tencent01: [2020-11-11 10:55:25,083] [INFO] [logging.py:60:log_dist] [Rank 0] step=5600, skipped=19, lr=[0.00014621184144332917, 0.00014621184144332917], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 10:55:25,135] [INFO] [timer.py:157:stop] 0/5600, SamplesPerSec=48.241579633894055
Tencent01:  iteration     5600/   50000 | elapsed time per iteration (ms): 2650.3 | learning rate 1.462E-04 | lm loss 3.219825E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 576.70 | backward: 1952.49 | optimizer: 120.83 | batch generator: 1.22 | data loader: 0.27
Tencent01: [2020-11-11 10:59:51,137] [INFO] [logging.py:60:log_dist] [Rank 0] step=5700, skipped=19, lr=[0.0001460625650043422, 0.0001460625650043422], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 10:59:51,190] [INFO] [timer.py:157:stop] 0/5700, SamplesPerSec=48.23973720944303
Tencent01:  iteration     5700/   50000 | elapsed time per iteration (ms): 2660.5 | learning rate 1.461E-04 | lm loss 3.211806E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 576.87 | backward: 1963.22 | optimizer: 120.22 | batch generator: 1.19 | data loader: 0.26
Tencent01: [2020-11-11 11:04:17,489] [INFO] [logging.py:60:log_dist] [Rank 0] step=5800, skipped=19, lr=[0.00014591048313696744, 0.00014591048313696744], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 11:04:17,542] [INFO] [timer.py:157:stop] 0/5800, SamplesPerSec=48.23700930857117
Tencent01:  iteration     5800/   50000 | elapsed time per iteration (ms): 2663.5 | learning rate 1.459E-04 | lm loss 3.179580E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 576.97 | backward: 1966.11 | optimizer: 120.22 | batch generator: 1.15 | data loader: 0.25
Tencent01: [2020-11-11 11:08:42,453] [INFO] [logging.py:60:log_dist] [Rank 0] step=5900, skipped=19, lr=[0.00014575560184513665, 0.00014575560184513665], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 11:08:42,505] [INFO] [timer.py:157:stop] 0/5900, SamplesPerSec=48.23865759225642
Tencent01:  iteration     5900/   50000 | elapsed time per iteration (ms): 2649.6 | learning rate 1.458E-04 | lm loss 3.176602E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 576.79 | backward: 1952.38 | optimizer: 120.24 | batch generator: 1.16 | data loader: 0.26
Tencent01: [2020-11-11 11:13:07,440] [INFO] [logging.py:60:log_dist] [Rank 0] step=6000, skipped=19, lr=[0.000145597927243298, 0.000145597927243298], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 11:13:07,493] [INFO] [timer.py:157:stop] 0/6000, SamplesPerSec=48.24018163007263
Tencent01:  iteration     6000/   50000 | elapsed time per iteration (ms): 2649.9 | learning rate 1.456E-04 | lm loss 3.206361E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 576.84 | backward: 1952.56 | optimizer: 120.25 | batch generator: 1.17 | data loader: 0.26
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: ----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 6000 | LM loss: 3.142948E+00 | LM PPL: 2.317207E+01
Tencent01: ----------------------------------------------------------------------------------
Tencent01: [2020-11-11 11:17:18,442] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 11:17:18,442] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-11 11:17:18,442] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 11:17:18,442] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-11 11:17:18,443] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 11:17:18,443] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-11 11:17:18,443] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 11:17:18,443] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-11 11:17:18,442] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 11:17:18,443] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-11 11:17:18,443] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 11:17:18,443] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 11:17:18,443] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 11:17:18,443] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-11 11:17:18,443] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-11 11:17:18,443] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-11 11:17:18,443] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 11:17:18,443] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 11:17:18,443] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 11:17:18,443] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-11 11:17:18,443] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-11 11:17:18,443] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 11:17:18,443] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-11 11:17:18,443] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 11:17:18,443] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 11:17:18,443] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-11 11:17:18,443] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-11 11:17:18,443] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-11 11:17:18,443] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 11:17:18,443] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-11 11:17:18,443] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 11:17:18,443] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-11 11:18:27,371] [INFO] [logging.py:60:log_dist] [Rank 0] step=6100, skipped=19, lr=[0.00014543746555617483, 0.00014543746555617483], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 11:18:27,423] [INFO] [timer.py:157:stop] 0/6100, SamplesPerSec=48.23877199629153
Tencent01:  iteration     6100/   50000 | elapsed time per iteration (ms): 3199.3 | learning rate 1.454E-04 | lm loss 3.158840E+00 | loss scale 524288.0 |
Tencent01: time (ms) | forward: 576.45 | backward: 1962.68 | optimizer: 120.20 | batch generator: 2.15 | data loader: 0.38
Tencent01: [2020-11-11 11:20:23,551] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6142
Tencent01: [2020-11-11 11:20:23,551] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6142
Tencent01: [2020-11-11 11:20:23,552] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-11 11:20:23,551] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6142
Tencent01: [2020-11-11 11:20:23,552] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-11 11:20:23,552] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6142
Tencent01: [2020-11-11 11:20:23,552] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-11 11:20:23,552] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
Tencent01: [2020-11-11 11:20:23,552] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6142
Tencent02: [2020-11-11 11:20:23,551] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-11 11:20:23,552] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6142
Tencent01: Grad overflow on iteration 6142
Tencent02: [2020-11-11 11:20:23,551] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6142
Tencent02: [2020-11-11 11:20:23,551] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6142
Tencent01: [2020-11-11 11:20:23,552] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-11 11:20:23,551] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6142
Tencent01: [2020-11-11 11:20:23,552] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-11 11:20:23,551] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-11 11:20:23,551] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-11 11:20:23,552] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-11 11:20:23,551] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-11 11:20:23,552] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6142
Tencent01: Grad overflow on iteration 6142
Tencent02: [2020-11-11 11:20:23,551] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-11 11:20:23,551] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-11 11:20:23,551] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-11 11:20:23,552] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: Grad overflow on iteration 6142
Tencent02: [2020-11-11 11:20:23,552] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-11 11:20:23,552] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: [2020-11-11 11:20:23,551] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6142
Tencent02: Grad overflow on iteration 6142
Tencent02: [2020-11-11 11:20:23,552] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-11 11:20:23,552] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-11 11:20:23,552] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-11 11:20:23,552] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6142
Tencent02: [2020-11-11 11:20:23,552] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-11 11:22:54,502] [INFO] [logging.py:60:log_dist] [Rank 0] step=6200, skipped=20, lr=[0.00014527586928654167, 0.00014527586928654167], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 11:22:54,554] [INFO] [timer.py:157:stop] 0/6200, SamplesPerSec=48.23397717555554
Tencent01:  iteration     6200/   50000 | elapsed time per iteration (ms): 2671.3 | learning rate 1.453E-04 | lm loss 3.178134E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 576.90 | backward: 1974.98 | optimizer: 119.18 | batch generator: 1.21 | data loader: 0.27
Tencent01: [2020-11-11 11:27:19,397] [INFO] [logging.py:60:log_dist] [Rank 0] step=6300, skipped=20, lr=[0.00014510988025368633, 0.00014510988025368633], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 11:27:19,449] [INFO] [timer.py:157:stop] 0/6300, SamplesPerSec=48.23577826369341
Tencent01:  iteration     6300/   50000 | elapsed time per iteration (ms): 2648.9 | learning rate 1.451E-04 | lm loss 3.148746E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 576.69 | backward: 1951.71 | optimizer: 120.31 | batch generator: 1.18 | data loader: 0.27
Tencent01: [2020-11-11 11:31:44,291] [INFO] [logging.py:60:log_dist] [Rank 0] step=6400, skipped=20, lr=[0.0001449411234028059, 0.0001449411234028059], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 11:31:44,344] [INFO] [timer.py:157:stop] 0/6400, SamplesPerSec=48.2375376420317
Tencent01:  iteration     6400/   50000 | elapsed time per iteration (ms): 2648.9 | learning rate 1.449E-04 | lm loss 3.131573E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 576.82 | backward: 1951.28 | optimizer: 120.59 | batch generator: 1.21 | data loader: 0.28
Tencent01: [2020-11-11 11:36:09,188] [INFO] [logging.py:60:log_dist] [Rank 0] step=6500, skipped=20, lr=[0.00014476960539613196, 0.00014476960539613196], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 11:36:09,241] [INFO] [timer.py:157:stop] 0/6500, SamplesPerSec=48.239225309604834
Tencent01:  iteration     6500/   50000 | elapsed time per iteration (ms): 2649.0 | learning rate 1.448E-04 | lm loss 3.194286E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 576.79 | backward: 1951.70 | optimizer: 120.25 | batch generator: 1.19 | data loader: 0.27
Tencent01: [2020-11-11 11:40:36,375] [INFO] [logging.py:60:log_dist] [Rank 0] step=6600, skipped=20, lr=[0.00014459533300490164, 0.00014459533300490164], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 11:40:36,427] [INFO] [timer.py:157:stop] 0/6600, SamplesPerSec=48.23454185957848
Tencent01:  iteration     6600/   50000 | elapsed time per iteration (ms): 2671.9 | learning rate 1.446E-04 | lm loss 3.157899E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 576.75 | backward: 1974.56 | optimizer: 120.32 | batch generator: 1.15 | data loader: 0.26
Tencent01: [2020-11-11 11:44:32,048] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6688
Tencent01: [2020-11-11 11:44:32,048] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6688
Tencent01: [2020-11-11 11:44:32,048] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6688
Tencent01: [2020-11-11 11:44:32,048] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-11 11:44:32,048] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-11 11:44:32,048] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6688
Tencent01: [2020-11-11 11:44:32,048] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6688
Tencent01: [2020-11-11 11:44:32,048] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6688
Tencent01: [2020-11-11 11:44:32,048] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-11 11:44:32,049] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-11 11:44:32,049] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-11 11:44:32,049] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-11 11:44:32,049] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Tencent01: [2020-11-11 11:44:32,048] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6688
Tencent01: [2020-11-11 11:44:32,049] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-11 11:44:32,049] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6688
Tencent02: [2020-11-11 11:44:32,048] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6688
Tencent02: [2020-11-11 11:44:32,048] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6688
Tencent02: [2020-11-11 11:44:32,048] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6688
Tencent01: [2020-11-11 11:44:32,049] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-11 11:44:32,048] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-11 11:44:32,048] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6688
Tencent02: [2020-11-11 11:44:32,048] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-11 11:44:32,048] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-11 11:44:32,048] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6688
Tencent02: [2020-11-11 11:44:32,048] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6688
Tencent02: [2020-11-11 11:44:32,048] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-11 11:44:32,048] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-11 11:44:32,048] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-11 11:44:32,048] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6688
Tencent02: [2020-11-11 11:44:32,048] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6688
Tencent02: [2020-11-11 11:44:32,049] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-11 11:44:32,049] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-11 11:45:01,120] [INFO] [logging.py:60:log_dist] [Rank 0] step=6700, skipped=21, lr=[0.00014442009688533962, 0.00014442009688533962], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 11:45:01,172] [INFO] [timer.py:157:stop] 0/6700, SamplesPerSec=48.23665246632998
Tencent01:  iteration     6700/   50000 | elapsed time per iteration (ms): 2647.4 | learning rate 1.444E-04 | lm loss 3.146834E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 576.75 | backward: 1951.00 | optimizer: 119.43 | batch generator: 1.22 | data loader: 0.27
Tencent01: [2020-11-11 11:45:03,702] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6700
Tencent01: [2020-11-11 11:45:03,702] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6700
Tencent01: [2020-11-11 11:45:03,703] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-11 11:45:03,703] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-11 11:45:03,702] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6700
Tencent01: [2020-11-11 11:45:03,703] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Tencent01: [2020-11-11 11:45:03,703] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-11 11:45:03,703] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6700
Tencent01: [2020-11-11 11:45:03,703] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6700
Tencent01: [2020-11-11 11:45:03,703] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6700
Tencent01: [2020-11-11 11:45:03,703] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-11 11:45:03,703] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-11 11:45:03,703] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6700
Tencent01: [2020-11-11 11:45:03,703] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-11 11:45:03,703] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-11 11:45:03,703] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6700
Tencent01: [2020-11-11 11:45:03,703] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-11 11:45:03,703] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6700
Tencent02: [2020-11-11 11:45:03,703] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6700
Tencent02: [2020-11-11 11:45:03,703] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-11 11:45:03,703] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-11 11:45:03,703] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6700
Tencent02: [2020-11-11 11:45:03,703] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6700
Tencent02: [2020-11-11 11:45:03,703] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6700
Tencent02: [2020-11-11 11:45:03,703] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-11 11:45:03,703] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-11 11:45:03,703] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6700
Tencent02: [2020-11-11 11:45:03,703] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-11 11:45:03,703] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6700
Tencent02: [2020-11-11 11:45:03,703] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-11 11:45:03,703] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6700
Tencent02: [2020-11-11 11:45:03,703] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-11 11:45:03,703] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-11 11:45:35,355] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6712
Tencent02: [2020-11-11 11:45:35,355] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-11 11:45:35,355] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6712
Tencent01: Grad overflow on iteration 6712
Tencent01: [2020-11-11 11:45:35,355] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: [2020-11-11 11:45:35,355] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6712
Tencent01: Grad overflow on iteration 6712
Tencent02: [2020-11-11 11:45:35,355] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent02: [2020-11-11 11:45:35,355] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent02: [2020-11-11 11:45:35,355] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6712
Tencent01: [2020-11-11 11:45:35,355] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent02: [2020-11-11 11:45:35,355] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent02: [2020-11-11 11:45:35,355] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-11 11:45:35,355] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6712
Tencent02: Grad overflow on iteration 6712
Tencent01: [2020-11-11 11:45:35,355] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent01: [2020-11-11 11:45:35,355] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent02: [2020-11-11 11:45:35,355] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6712
Tencent01: [2020-11-11 11:45:35,355] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Tencent02: [2020-11-11 11:45:35,355] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6712
Tencent01: [2020-11-11 11:45:35,355] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent02: [2020-11-11 11:45:35,355] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent01: [2020-11-11 11:45:35,355] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6712
Tencent01: [2020-11-11 11:45:35,355] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6712
Tencent02: [2020-11-11 11:45:35,355] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-11 11:45:35,355] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6712
Tencent02: Grad overflow on iteration 6712
Tencent01: [2020-11-11 11:45:35,355] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: [2020-11-11 11:45:35,355] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent01: Grad overflow on iteration 6712
Tencent02: [2020-11-11 11:45:35,355] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent01: [2020-11-11 11:45:35,355] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent02: [2020-11-11 11:45:35,355] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6712
Tencent02: [2020-11-11 11:45:35,356] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent01: [2020-11-11 11:45:35,355] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent02: [2020-11-11 11:45:35,356] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent01: [2020-11-11 11:45:35,355] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent01: [2020-11-11 11:45:35,355] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent01: [2020-11-11 11:45:37,882] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6713
Tencent01: [2020-11-11 11:45:37,882] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent01: [2020-11-11 11:45:37,882] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6713
Tencent01: [2020-11-11 11:45:37,882] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6713
Tencent01: [2020-11-11 11:45:37,882] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6713
Tencent01: [2020-11-11 11:45:37,882] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6713
Tencent01: [2020-11-11 11:45:37,882] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Tencent01: [2020-11-11 11:45:37,882] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent01: [2020-11-11 11:45:37,882] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6713
Tencent01: [2020-11-11 11:45:37,882] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent01: [2020-11-11 11:45:37,882] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent01: [2020-11-11 11:45:37,882] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent02: [2020-11-11 11:45:37,882] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-11 11:45:37,882] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6713
Tencent01: Grad overflow on iteration 6713
Tencent02: [2020-11-11 11:45:37,882] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-11 11:45:37,882] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent02: Grad overflow on iteration 6713
Tencent01: [2020-11-11 11:45:37,882] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent02: [2020-11-11 11:45:37,882] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent01: [2020-11-11 11:45:37,882] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: [2020-11-11 11:45:37,882] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent01: Grad overflow on iteration 6713
Tencent02: [2020-11-11 11:45:37,882] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-11 11:45:37,882] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent02: Grad overflow on iteration 6713
Tencent02: [2020-11-11 11:45:37,882] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent02: [2020-11-11 11:45:37,882] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6713
Tencent02: [2020-11-11 11:45:37,882] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6713
Tencent02: [2020-11-11 11:45:37,882] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6713
Tencent02: [2020-11-11 11:45:37,882] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6713
Tencent02: [2020-11-11 11:45:37,882] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent02: [2020-11-11 11:45:37,882] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent02: [2020-11-11 11:45:37,882] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent02: [2020-11-11 11:45:37,882] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent02: [2020-11-11 11:45:37,882] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6713
Tencent02: [2020-11-11 11:45:37,882] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent01: [2020-11-11 11:45:40,408] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6714
Tencent01: [2020-11-11 11:45:40,408] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6714
Tencent01: [2020-11-11 11:45:40,408] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Tencent01: [2020-11-11 11:45:40,408] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6714
Tencent01: [2020-11-11 11:45:40,408] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Tencent01: [2020-11-11 11:45:40,408] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6714
Tencent01: [2020-11-11 11:45:40,408] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Tencent01: [2020-11-11 11:45:40,408] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6714
Tencent01: [2020-11-11 11:45:40,408] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Tencent01: [2020-11-11 11:45:40,408] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Tencent01: [2020-11-11 11:45:40,408] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Tencent01: [2020-11-11 11:45:40,408] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6714
Tencent02: [2020-11-11 11:45:40,408] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6714
Tencent01: [2020-11-11 11:45:40,408] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: [2020-11-11 11:45:40,408] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6714
Tencent02: [2020-11-11 11:45:40,408] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Tencent01: Grad overflow on iteration 6714
Tencent02: [2020-11-11 11:45:40,408] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Tencent02: [2020-11-11 11:45:40,408] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6714
Tencent02: [2020-11-11 11:45:40,408] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6714
Tencent01: [2020-11-11 11:45:40,408] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6714
Tencent02: [2020-11-11 11:45:40,408] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-11 11:45:40,408] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Tencent02: Grad overflow on iteration 6714
Tencent02: [2020-11-11 11:45:40,408] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Tencent02: [2020-11-11 11:45:40,408] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Tencent01: [2020-11-11 11:45:40,408] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Tencent02: [2020-11-11 11:45:40,408] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Tencent02: [2020-11-11 11:45:40,408] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6714
Tencent02: [2020-11-11 11:45:40,408] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6714
Tencent02: [2020-11-11 11:45:40,408] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-11 11:45:40,408] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Tencent02: Grad overflow on iteration 6714
Tencent02: [2020-11-11 11:45:40,408] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Tencent02: [2020-11-11 11:45:40,408] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Tencent02: [2020-11-11 11:45:40,408] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Tencent01: [2020-11-11 11:45:42,937] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6715
Tencent01: [2020-11-11 11:45:42,937] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6715
Tencent01: [2020-11-11 11:45:42,937] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6715
Tencent01: [2020-11-11 11:45:42,937] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 6715
Tencent01: [2020-11-11 11:45:42,937] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
Tencent02: [2020-11-11 11:45:42,937] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-11 11:45:42,937] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
Tencent02: Grad overflow on iteration 6715
Tencent01: [2020-11-11 11:45:42,937] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: [2020-11-11 11:45:42,937] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6715
Tencent01: Grad overflow on iteration 6715
Tencent02: [2020-11-11 11:45:42,937] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
Tencent01: [2020-11-11 11:45:42,937] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
Tencent01: [2020-11-11 11:45:42,937] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
Tencent02: [2020-11-11 11:45:42,937] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
Tencent01: [2020-11-11 11:45:42,937] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: [2020-11-11 11:45:42,937] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6715
Tencent02: [2020-11-11 11:45:42,937] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6715
Tencent01: Grad overflow on iteration 6715
Tencent02: [2020-11-11 11:45:42,937] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
Tencent01: [2020-11-11 11:45:42,937] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
Tencent02: [2020-11-11 11:45:42,937] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-11 11:45:42,937] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
Tencent02: Grad overflow on iteration 6715
Tencent01: [2020-11-11 11:45:42,937] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: [2020-11-11 11:45:42,937] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6715
Tencent01: Grad overflow on iteration 6715
Tencent02: [2020-11-11 11:45:42,937] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
Tencent01: [2020-11-11 11:45:42,937] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
Tencent02: [2020-11-11 11:45:42,937] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6715
Tencent02: [2020-11-11 11:45:42,937] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-11 11:45:42,937] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 6715
Tencent01: Grad overflow on iteration 6715
Tencent02: [2020-11-11 11:45:42,937] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
Tencent01: [2020-11-11 11:45:42,937] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
Tencent02: [2020-11-11 11:45:42,937] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
Tencent01: [2020-11-11 11:45:42,937] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
Tencent02: [2020-11-11 11:45:42,937] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
Tencent02: [2020-11-11 11:45:42,937] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
Tencent01: [2020-11-11 11:49:25,183] [INFO] [logging.py:60:log_dist] [Rank 0] step=6800, skipped=26, lr=[0.0001442494154755211, 0.0001442494154755211], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 11:49:25,234] [INFO] [timer.py:157:stop] 0/6800, SamplesPerSec=48.24053050384592
Tencent01:  iteration     6800/   50000 | elapsed time per iteration (ms): 2640.6 | learning rate 1.442E-04 | lm loss 3.321935E+00 | loss scale 4096.0 |
Tencent01: time (ms) | forward: 576.87 | backward: 1948.15 | optimizer: 115.34 | batch generator: 1.23 | data loader: 0.28
Tencent01: [2020-11-11 11:53:49,813] [INFO] [logging.py:60:log_dist] [Rank 0] step=6900, skipped=26, lr=[0.0001440670854471426, 0.0001440670854471426], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 11:53:49,865] [INFO] [timer.py:157:stop] 0/6900, SamplesPerSec=48.242807897740946
Tencent01:  iteration     6900/   50000 | elapsed time per iteration (ms): 2646.3 | learning rate 1.441E-04 | lm loss 3.126343E+00 | loss scale 4096.0 |
Tencent01: time (ms) | forward: 576.94 | backward: 1948.04 | optimizer: 121.06 | batch generator: 1.26 | data loader: 0.29
Tencent01: [2020-11-11 11:58:17,098] [INFO] [logging.py:60:log_dist] [Rank 0] step=7000, skipped=26, lr=[0.00014388202876849246, 0.00014388202876849246], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 11:58:17,149] [INFO] [timer.py:157:stop] 0/7000, SamplesPerSec=48.238105338129564
Tencent01:  iteration     7000/   50000 | elapsed time per iteration (ms): 2672.8 | learning rate 1.439E-04 | lm loss 3.142067E+00 | loss scale 4096.0 |
Tencent01: time (ms) | forward: 576.91 | backward: 1974.43 | optimizer: 121.25 | batch generator: 1.20 | data loader: 0.28
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: ----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 7000 | LM loss: 3.096961E+00 | LM PPL: 2.213059E+01
Tencent01: ----------------------------------------------------------------------------------
Tencent01: [2020-11-11 12:03:36,655] [INFO] [logging.py:60:log_dist] [Rank 0] step=7100, skipped=26, lr=[0.00014369425274529148, 0.00014369425274529148], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 12:03:36,707] [INFO] [timer.py:157:stop] 0/7100, SamplesPerSec=48.23797362152821
Tencent01:  iteration     7100/   50000 | elapsed time per iteration (ms): 3195.6 | learning rate 1.437E-04 | lm loss 3.100072E+00 | loss scale 4096.0 |
Tencent01: time (ms) | forward: 576.65 | backward: 1957.32 | optimizer: 121.32 | batch generator: 2.19 | data loader: 0.40
Tencent01: [2020-11-11 12:08:01,331] [INFO] [logging.py:60:log_dist] [Rank 0] step=7200, skipped=26, lr=[0.0001435037647906155, 0.0001435037647906155], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 12:08:01,383] [INFO] [timer.py:157:stop] 0/7200, SamplesPerSec=48.24006705925989
Tencent01:  iteration     7200/   50000 | elapsed time per iteration (ms): 2646.8 | learning rate 1.435E-04 | lm loss 3.089426E+00 | loss scale 4096.0 |
Tencent01: time (ms) | forward: 576.96 | backward: 1948.73 | optimizer: 120.79 | batch generator: 1.23 | data loader: 0.27
Tencent01: [2020-11-11 12:12:25,965] [INFO] [logging.py:60:log_dist] [Rank 0] step=7300, skipped=26, lr=[0.00014331057242460284, 0.00014331057242460284], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 12:12:26,017] [INFO] [timer.py:157:stop] 0/7300, SamplesPerSec=48.24218715000942
Tencent01:  iteration     7300/   50000 | elapsed time per iteration (ms): 2646.3 | learning rate 1.433E-04 | lm loss 3.100754E+00 | loss scale 4096.0 |
Tencent01: time (ms) | forward: 576.86 | backward: 1948.96 | optimizer: 120.28 | batch generator: 1.17 | data loader: 0.27
Tencent01: [2020-11-11 12:16:53,156] [INFO] [logging.py:60:log_dist] [Rank 0] step=7400, skipped=26, lr=[0.00014311468327415726, 0.00014311468327415726], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 12:16:53,208] [INFO] [timer.py:157:stop] 0/7400, SamplesPerSec=48.23798064292318
Tencent01:  iteration     7400/   50000 | elapsed time per iteration (ms): 2671.9 | learning rate 1.431E-04 | lm loss 3.126609E+00 | loss scale 4096.0 |
Tencent01: time (ms) | forward: 576.77 | backward: 1974.36 | optimizer: 120.52 | batch generator: 1.21 | data loader: 0.27
Tencent01: [2020-11-11 12:21:17,812] [INFO] [logging.py:60:log_dist] [Rank 0] step=7500, skipped=26, lr=[0.00014291610507264705, 0.00014291610507264705], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 12:21:17,864] [INFO] [timer.py:157:stop] 0/7500, SamplesPerSec=48.240034956697805
Tencent01:  iteration     7500/   50000 | elapsed time per iteration (ms): 2646.6 | learning rate 1.429E-04 | lm loss 3.107127E+00 | loss scale 4096.0 |
Tencent01: time (ms) | forward: 577.05 | backward: 1948.90 | optimizer: 120.35 | batch generator: 1.23 | data loader: 0.27
Tencent01: [2020-11-11 12:25:42,545] [INFO] [logging.py:60:log_dist] [Rank 0] step=7600, skipped=26, lr=[0.0001427148456595996, 0.0001427148456595996], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 12:25:42,597] [INFO] [timer.py:157:stop] 0/7600, SamplesPerSec=48.241839734198805
Tencent01:  iteration     7600/   50000 | elapsed time per iteration (ms): 2647.3 | learning rate 1.427E-04 | lm loss 3.088489E+00 | loss scale 4096.0 |
Tencent01: time (ms) | forward: 576.82 | backward: 1949.89 | optimizer: 120.37 | batch generator: 1.18 | data loader: 0.27
Tencent01: [2020-11-11 12:30:07,153] [INFO] [logging.py:60:log_dist] [Rank 0] step=7700, skipped=26, lr=[0.00014251091298039188, 0.00014251091298039188], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 12:30:07,205] [INFO] [timer.py:157:stop] 0/7700, SamplesPerSec=48.243892698026215
Tencent01:  iteration     7700/   50000 | elapsed time per iteration (ms): 2646.1 | learning rate 1.425E-04 | lm loss 3.083945E+00 | loss scale 4096.0 |
Tencent01: time (ms) | forward: 576.81 | backward: 1948.70 | optimizer: 120.32 | batch generator: 1.18 | data loader: 0.27
Tencent01: [2020-11-11 12:30:52,093] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 12:30:52,093] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
Tencent01: [2020-11-11 12:30:52,094] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 12:30:52,094] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
Tencent01: [2020-11-11 12:30:52,094] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 12:30:52,094] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
Tencent01: [2020-11-11 12:30:52,094] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 12:30:52,094] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
Tencent01: [2020-11-11 12:30:52,094] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 12:30:52,094] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
Tencent01: [2020-11-11 12:30:52,094] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 12:30:52,094] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
Tencent01: [2020-11-11 12:30:52,094] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 12:30:52,094] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
Tencent02: [2020-11-11 12:30:52,094] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 12:30:52,094] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
Tencent02: [2020-11-11 12:30:52,094] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 12:30:52,094] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
Tencent02: [2020-11-11 12:30:52,094] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 12:30:52,094] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
Tencent02: [2020-11-11 12:30:52,094] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 12:30:52,094] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
Tencent02: [2020-11-11 12:30:52,094] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 12:30:52,094] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 12:30:52,094] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
Tencent02: [2020-11-11 12:30:52,094] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 12:30:52,094] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
Tencent02: [2020-11-11 12:30:52,094] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
Tencent01: [2020-11-11 12:30:52,096] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 12:30:52,096] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
Tencent02: [2020-11-11 12:30:52,118] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 12:30:52,118] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
Tencent01: [2020-11-11 12:34:31,794] [INFO] [logging.py:60:log_dist] [Rank 0] step=7800, skipped=26, lr=[0.00014230431508593686, 0.00014230431508593686], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 12:34:31,846] [INFO] [timer.py:157:stop] 0/7800, SamplesPerSec=48.24581011645043
Tencent01:  iteration     7800/   50000 | elapsed time per iteration (ms): 2646.4 | learning rate 1.423E-04 | lm loss 3.099814E+00 | loss scale 8192.0 |
Tencent01: time (ms) | forward: 576.71 | backward: 1949.17 | optimizer: 120.29 | batch generator: 1.17 | data loader: 0.27
Tencent01: [2020-11-11 12:38:58,982] [INFO] [logging.py:60:log_dist] [Rank 0] step=7900, skipped=26, lr=[0.0001420950601323657, 0.0001420950601323657], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 12:38:59,033] [INFO] [timer.py:157:stop] 0/7900, SamplesPerSec=48.24184347485707
Tencent01:  iteration     7900/   50000 | elapsed time per iteration (ms): 2671.9 | learning rate 1.421E-04 | lm loss 3.069213E+00 | loss scale 8192.0 |
Tencent01: time (ms) | forward: 577.02 | backward: 1973.72 | optimizer: 120.86 | batch generator: 1.25 | data loader: 0.28
Tencent01: [2020-11-11 12:43:23,628] [INFO] [logging.py:60:log_dist] [Rank 0] step=8000, skipped=26, lr=[0.00014188315638070568, 0.00014188315638070568], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 12:43:23,680] [INFO] [timer.py:157:stop] 0/8000, SamplesPerSec=48.24375073211236
Tencent01:  iteration     8000/   50000 | elapsed time per iteration (ms): 2646.5 | learning rate 1.419E-04 | lm loss 3.051086E+00 | loss scale 8192.0 |
Tencent01: time (ms) | forward: 577.05 | backward: 1948.04 | optimizer: 121.13 | batch generator: 1.26 | data loader: 0.28
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: ----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 8000 | LM loss: 3.052050E+00 | LM PPL: 2.115868E+01
Tencent01: ----------------------------------------------------------------------------------
Tencent01: [2020-11-11 12:48:43,285] [INFO] [logging.py:60:log_dist] [Rank 0] step=8100, skipped=26, lr=[0.00014166861219655407, 0.00014166861219655407], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 12:48:43,337] [INFO] [timer.py:157:stop] 0/8100, SamplesPerSec=48.24327546476042
Tencent01:  iteration     8100/   50000 | elapsed time per iteration (ms): 3196.6 | learning rate 1.417E-04 | lm loss 3.084009E+00 | loss scale 8192.0 |
Tencent01: time (ms) | forward: 576.53 | backward: 1959.23 | optimizer: 120.83 | batch generator: 2.27 | data loader: 0.41
Tencent01: [2020-11-11 12:53:07,961] [INFO] [logging.py:60:log_dist] [Rank 0] step=8200, skipped=26, lr=[0.00014145143604974786, 0.00014145143604974786], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 12:53:08,013] [INFO] [timer.py:157:stop] 0/8200, SamplesPerSec=48.24504368366234
Tencent01:  iteration     8200/   50000 | elapsed time per iteration (ms): 2646.8 | learning rate 1.414E-04 | lm loss 3.088486E+00 | loss scale 8192.0 |
Tencent01: time (ms) | forward: 576.94 | backward: 1949.14 | optimizer: 120.43 | batch generator: 1.22 | data loader: 0.28
Tencent01: [2020-11-11 12:57:34,889] [INFO] [logging.py:60:log_dist] [Rank 0] step=8300, skipped=26, lr=[0.00014123163651402952, 0.00014123163651402952], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 12:57:34,940] [INFO] [timer.py:157:stop] 0/8300, SamplesPerSec=48.241862485378896
Tencent01:  iteration     8300/   50000 | elapsed time per iteration (ms): 2669.3 | learning rate 1.412E-04 | lm loss 3.082373E+00 | loss scale 8192.0 |
Tencent01: time (ms) | forward: 577.10 | backward: 1970.71 | optimizer: 121.17 | batch generator: 1.30 | data loader: 0.29
Tencent01: [2020-11-11 13:01:59,508] [INFO] [logging.py:60:log_dist] [Rank 0] step=8400, skipped=26, lr=[0.00014100922226670828, 0.00014100922226670828], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 13:01:59,561] [INFO] [timer.py:157:stop] 0/8400, SamplesPerSec=48.24373063106645
Tencent01:  iteration     8400/   50000 | elapsed time per iteration (ms): 2646.2 | learning rate 1.410E-04 | lm loss 3.066034E+00 | loss scale 8192.0 |
Tencent01: time (ms) | forward: 576.87 | backward: 1948.71 | optimizer: 120.36 | batch generator: 1.23 | data loader: 0.27
Tencent01: [2020-11-11 13:06:24,236] [INFO] [logging.py:60:log_dist] [Rank 0] step=8500, skipped=26, lr=[0.00014078420208831787, 0.00014078420208831787], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 13:06:24,288] [INFO] [timer.py:157:stop] 0/8500, SamplesPerSec=48.24532332355079
Tencent01:  iteration     8500/   50000 | elapsed time per iteration (ms): 2647.3 | learning rate 1.408E-04 | lm loss 3.086638E+00 | loss scale 8192.0 |
Tencent01: time (ms) | forward: 577.04 | backward: 1949.13 | optimizer: 120.84 | batch generator: 1.23 | data loader: 0.27
Tencent01: [2020-11-11 13:10:48,888] [INFO] [logging.py:60:log_dist] [Rank 0] step=8600, skipped=26, lr=[0.00014055658486226963, 0.00014055658486226963], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 13:10:48,940] [INFO] [timer.py:157:stop] 0/8600, SamplesPerSec=48.24704725185156
Tencent01:  iteration     8600/   50000 | elapsed time per iteration (ms): 2646.5 | learning rate 1.406E-04 | lm loss 3.077088E+00 | loss scale 8192.0 |
Tencent01: time (ms) | forward: 576.78 | backward: 1948.82 | optimizer: 120.64 | batch generator: 1.25 | data loader: 0.28
Tencent01: [2020-11-11 13:15:16,228] [INFO] [logging.py:60:log_dist] [Rank 0] step=8700, skipped=26, lr=[0.00014032637957450185, 0.00014032637957450185], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 13:15:16,280] [INFO] [timer.py:157:stop] 0/8700, SamplesPerSec=48.24312109280149
Tencent01:  iteration     8700/   50000 | elapsed time per iteration (ms): 2673.4 | learning rate 1.403E-04 | lm loss 3.026287E+00 | loss scale 8192.0 |
Tencent01: time (ms) | forward: 577.18 | backward: 1974.79 | optimizer: 121.15 | batch generator: 1.29 | data loader: 0.30
Tencent01: [2020-11-11 13:16:01,224] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 13:16:01,224] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Tencent01: [2020-11-11 13:16:01,224] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 13:16:01,224] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 13:16:01,224] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Tencent01: [2020-11-11 13:16:01,225] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Tencent02: [2020-11-11 13:16:01,224] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 13:16:01,224] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Tencent01: [2020-11-11 13:16:01,225] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 13:16:01,225] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Tencent01: [2020-11-11 13:16:01,225] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 13:16:01,225] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Tencent02: [2020-11-11 13:16:01,224] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 13:16:01,224] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 13:16:01,225] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 13:16:01,224] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Tencent02: [2020-11-11 13:16:01,224] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Tencent01: [2020-11-11 13:16:01,225] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Tencent02: [2020-11-11 13:16:01,224] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 13:16:01,224] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Tencent02: [2020-11-11 13:16:01,224] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 13:16:01,224] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 13:16:01,225] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 13:16:01,225] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Tencent01: [2020-11-11 13:16:01,225] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Tencent02: [2020-11-11 13:16:01,225] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Tencent01: [2020-11-11 13:16:01,225] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 13:16:01,225] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Tencent02: [2020-11-11 13:16:01,225] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 13:16:01,225] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Tencent01: [2020-11-11 13:16:01,226] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 13:16:01,226] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Tencent01: [2020-11-11 13:19:41,006] [INFO] [logging.py:60:log_dist] [Rank 0] step=8800, skipped=26, lr=[0.00014009359531312517, 0.00014009359531312517], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 13:19:41,058] [INFO] [timer.py:157:stop] 0/8800, SamplesPerSec=48.244573002889375
Tencent01:  iteration     8800/   50000 | elapsed time per iteration (ms): 2647.8 | learning rate 1.401E-04 | lm loss 3.058242E+00 | loss scale 16384.0 |
Tencent01: time (ms) | forward: 576.98 | backward: 1949.33 | optimizer: 121.19 | batch generator: 1.26 | data loader: 0.29
Tencent01: [2020-11-11 13:24:05,796] [INFO] [logging.py:60:log_dist] [Rank 0] step=8900, skipped=26, lr=[0.0001398582412680636, 0.0001398582412680636], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 13:24:05,848] [INFO] [timer.py:157:stop] 0/8900, SamplesPerSec=48.24595524158089
Tencent01:  iteration     8900/   50000 | elapsed time per iteration (ms): 2647.9 | learning rate 1.399E-04 | lm loss 3.029807E+00 | loss scale 16384.0 |
Tencent01: time (ms) | forward: 577.03 | backward: 1949.34 | optimizer: 121.29 | batch generator: 1.24 | data loader: 0.27
Tencent01: [2020-11-11 13:28:30,506] [INFO] [logging.py:60:log_dist] [Rank 0] step=9000, skipped=26, lr=[0.00013962032673069187, 0.00013962032673069187], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 13:28:30,558] [INFO] [timer.py:157:stop] 0/9000, SamplesPerSec=48.24745802527704
Tencent01:  iteration     9000/   50000 | elapsed time per iteration (ms): 2647.1 | learning rate 1.396E-04 | lm loss 3.071171E+00 | loss scale 16384.0 |
Tencent01: time (ms) | forward: 576.66 | backward: 1949.12 | optimizer: 121.09 | batch generator: 1.20 | data loader: 0.26
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: ----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 9000 | LM loss: 2.990954E+00 | LM PPL: 1.990466E+01
Tencent01: ----------------------------------------------------------------------------------
Tencent01: [2020-11-11 13:33:52,840] [INFO] [logging.py:60:log_dist] [Rank 0] step=9100, skipped=26, lr=[0.00013937986109346855, 0.00013937986109346855], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 13:33:52,892] [INFO] [timer.py:157:stop] 0/9100, SamplesPerSec=48.241621628432576
Tencent01:  iteration     9100/   50000 | elapsed time per iteration (ms): 3223.3 | learning rate 1.394E-04 | lm loss 3.035577E+00 | loss scale 16384.0 |
Tencent01: time (ms) | forward: 576.91 | backward: 1985.80 | optimizer: 120.88 | batch generator: 2.37 | data loader: 0.44
Tencent01: [2020-11-11 13:38:17,700] [INFO] [logging.py:60:log_dist] [Rank 0] step=9200, skipped=26, lr=[0.0001391368538495652, 0.0001391368538495652], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 13:38:17,751] [INFO] [timer.py:157:stop] 0/9200, SamplesPerSec=48.24293053127814
Tencent01:  iteration     9200/   50000 | elapsed time per iteration (ms): 2648.6 | learning rate 1.391E-04 | lm loss 3.024941E+00 | loss scale 16384.0 |
Tencent01: time (ms) | forward: 577.32 | backward: 1948.92 | optimizer: 121.99 | batch generator: 1.49 | data loader: 0.35
Tencent01: [2020-11-11 13:42:42,520] [INFO] [logging.py:60:log_dist] [Rank 0] step=9300, skipped=26, lr=[0.0001388913145924918, 0.0001388913145924918], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 13:42:42,572] [INFO] [timer.py:157:stop] 0/9300, SamplesPerSec=48.2442658960846
Tencent01:  iteration     9300/   50000 | elapsed time per iteration (ms): 2648.2 | learning rate 1.389E-04 | lm loss 3.016660E+00 | loss scale 16384.0 |
Tencent01: time (ms) | forward: 577.21 | backward: 1949.42 | optimizer: 121.23 | batch generator: 1.40 | data loader: 0.32
Tencent01: [2020-11-11 13:47:07,231] [INFO] [logging.py:60:log_dist] [Rank 0] step=9400, skipped=26, lr=[0.0001386432530157177, 0.0001386432530157177], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 13:47:07,282] [INFO] [timer.py:157:stop] 0/9400, SamplesPerSec=48.2457705572755
Tencent01:  iteration     9400/   50000 | elapsed time per iteration (ms): 2647.1 | learning rate 1.386E-04 | lm loss 3.030680E+00 | loss scale 16384.0 |
Tencent01: time (ms) | forward: 576.92 | backward: 1948.60 | optimizer: 121.28 | batch generator: 1.37 | data loader: 0.32
Tencent01: [2020-11-11 13:51:33,263] [INFO] [logging.py:60:log_dist] [Rank 0] step=9500, skipped=26, lr=[0.00013839267891228927, 0.00013839267891228927], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 13:51:33,315] [INFO] [timer.py:157:stop] 0/9500, SamplesPerSec=48.24469092083746
Tencent01:  iteration     9500/   50000 | elapsed time per iteration (ms): 2660.3 | learning rate 1.384E-04 | lm loss 3.016199E+00 | loss scale 16384.0 |
Tencent01: time (ms) | forward: 576.88 | backward: 1962.00 | optimizer: 121.17 | batch generator: 1.28 | data loader: 0.30
Tencent01: [2020-11-11 13:55:59,425] [INFO] [logging.py:60:log_dist] [Rank 0] step=9600, skipped=26, lr=[0.00013813960217444303, 0.00013813960217444303], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 13:55:59,477] [INFO] [timer.py:157:stop] 0/9600, SamplesPerSec=48.243380169737165
Tencent01:  iteration     9600/   50000 | elapsed time per iteration (ms): 2661.6 | learning rate 1.381E-04 | lm loss 3.059819E+00 | loss scale 16384.0 |
Tencent01: time (ms) | forward: 577.14 | backward: 1962.94 | optimizer: 121.28 | batch generator: 1.26 | data loader: 0.28
Tencent01: [2020-11-11 14:00:24,159] [INFO] [logging.py:60:log_dist] [Rank 0] step=9700, skipped=26, lr=[0.00013788403279321526, 0.00013788403279321526], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 14:00:24,211] [INFO] [timer.py:157:stop] 0/9700, SamplesPerSec=48.244777109809405
Tencent01:  iteration     9700/   50000 | elapsed time per iteration (ms): 2647.3 | learning rate 1.379E-04 | lm loss 3.006881E+00 | loss scale 16384.0 |
Tencent01: time (ms) | forward: 576.80 | backward: 1949.10 | optimizer: 121.18 | batch generator: 1.27 | data loader: 0.29
Tencent01: [2020-11-11 14:01:09,122] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 14:01:09,123] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 14:01:09,123] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent01: [2020-11-11 14:01:09,123] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent01: [2020-11-11 14:01:09,123] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 14:01:09,123] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent01: [2020-11-11 14:01:09,123] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 14:01:09,123] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent01: [2020-11-11 14:01:09,123] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 14:01:09,123] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 14:01:09,123] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent01: [2020-11-11 14:01:09,123] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent02: [2020-11-11 14:01:09,123] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 14:01:09,123] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent02: [2020-11-11 14:01:09,123] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 14:01:09,123] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent02: [2020-11-11 14:01:09,123] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 14:01:09,123] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent01: [2020-11-11 14:01:09,123] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 14:01:09,123] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 14:01:09,123] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent02: [2020-11-11 14:01:09,124] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent02: [2020-11-11 14:01:09,124] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 14:01:09,124] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent01: [2020-11-11 14:01:09,123] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 14:01:09,123] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 14:01:09,124] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent01: [2020-11-11 14:01:09,124] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent02: [2020-11-11 14:01:09,124] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 14:01:09,124] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent02: [2020-11-11 14:01:09,124] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 14:01:09,124] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent01: [2020-11-11 14:04:48,903] [INFO] [logging.py:60:log_dist] [Rank 0] step=9800, skipped=26, lr=[0.00013762598085804753, 0.00013762598085804753], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 14:04:48,955] [INFO] [timer.py:157:stop] 0/9800, SamplesPerSec=48.24612892313475
Tencent01:  iteration     9800/   50000 | elapsed time per iteration (ms): 2647.4 | learning rate 1.376E-04 | lm loss 3.028528E+00 | loss scale 32768.0 |
Tencent01: time (ms) | forward: 576.84 | backward: 1948.42 | optimizer: 121.91 | batch generator: 1.27 | data loader: 0.29
Tencent01: [2020-11-11 14:09:13,642] [INFO] [logging.py:60:log_dist] [Rank 0] step=9900, skipped=26, lr=[0.00013736545655638837, 0.00013736545655638837], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 14:09:13,694] [INFO] [timer.py:157:stop] 0/9900, SamplesPerSec=48.247458236597204
Tencent01:  iteration     9900/   50000 | elapsed time per iteration (ms): 2647.4 | learning rate 1.374E-04 | lm loss 3.041840E+00 | loss scale 32768.0 |
Tencent01: time (ms) | forward: 576.84 | backward: 1948.94 | optimizer: 121.35 | batch generator: 1.26 | data loader: 0.29
Tencent01: [2020-11-11 14:13:41,018] [INFO] [logging.py:60:log_dist] [Rank 0] step=10000, skipped=26, lr=[0.00013710247017329112, 0.00013710247017329112], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 14:13:41,069] [INFO] [timer.py:157:stop] 0/10000, SamplesPerSec=48.24395601387604
Tencent01:  iteration    10000/   50000 | elapsed time per iteration (ms): 2673.8 | learning rate 1.371E-04 | lm loss 3.001809E+00 | loss scale 32768.0 |
Tencent01: time (ms) | forward: 576.73 | backward: 1975.66 | optimizer: 121.11 | batch generator: 1.21 | data loader: 0.28
Tencent01: [2020-11-11 14:13:41,073] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: checkpoints/gpt-345M11-11-06-41/10000/mp_rank_00_model_states.pt
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: -----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 10000 | LM loss: 2.984060E+00 | LM PPL: 1.976790E+01
Tencent01: -----------------------------------------------------------------------------------
Tencent01: [2020-11-11 14:19:37,286] [INFO] [logging.py:60:log_dist] [Rank 0] step=10100, skipped=26, lr=[0.00013683703209100792, 0.00013683703209100792], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 14:19:37,338] [INFO] [timer.py:157:stop] 0/10100, SamplesPerSec=48.242864540129275
Tencent01:  iteration    10100/   50000 | elapsed time per iteration (ms): 3562.7 | learning rate 1.368E-04 | lm loss 2.997320E+00 | loss scale 32768.0 |
Tencent01: time (ms) | forward: 576.54 | backward: 1962.95 | optimizer: 121.00 | batch generator: 2.15 | data loader: 0.38
Tencent01: [2020-11-11 14:24:02,067] [INFO] [logging.py:60:log_dist] [Rank 0] step=10200, skipped=26, lr=[0.00013656915278857968, 0.00013656915278857968], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 14:24:02,119] [INFO] [timer.py:157:stop] 0/10200, SamplesPerSec=48.24410099741985
Tencent01:  iteration    10200/   50000 | elapsed time per iteration (ms): 2647.8 | learning rate 1.366E-04 | lm loss 3.003149E+00 | loss scale 32768.0 |
Tencent01: time (ms) | forward: 576.89 | backward: 1950.28 | optimizer: 120.37 | batch generator: 1.20 | data loader: 0.27
Tencent01: [2020-11-11 14:28:26,873] [INFO] [logging.py:60:log_dist] [Rank 0] step=10300, skipped=26, lr=[0.00013629884284142263, 0.00013629884284142263], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 14:28:26,925] [INFO] [timer.py:157:stop] 0/10300, SamplesPerSec=48.245293269633244
Tencent01:  iteration    10300/   50000 | elapsed time per iteration (ms): 2648.1 | learning rate 1.363E-04 | lm loss 2.994766E+00 | loss scale 32768.0 |
Tencent01: time (ms) | forward: 577.20 | backward: 1949.81 | optimizer: 120.74 | batch generator: 1.30 | data loader: 0.28
Tencent01: [2020-11-11 14:32:54,229] [INFO] [logging.py:60:log_dist] [Rank 0] step=10400, skipped=26, lr=[0.00013602611292091066, 0.00013602611292091066], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 14:32:54,281] [INFO] [timer.py:157:stop] 0/10400, SamplesPerSec=48.241979644780564
Tencent01:  iteration    10400/   50000 | elapsed time per iteration (ms): 2673.6 | learning rate 1.360E-04 | lm loss 3.034728E+00 | loss scale 32768.0 |
Tencent01: time (ms) | forward: 576.89 | backward: 1975.29 | optimizer: 121.14 | batch generator: 1.22 | data loader: 0.27
Tencent01: [2020-11-11 14:37:18,967] [INFO] [logging.py:60:log_dist] [Rank 0] step=10500, skipped=26, lr=[0.000135750973793954, 0.000135750973793954], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 14:37:19,019] [INFO] [timer.py:157:stop] 0/10500, SamplesPerSec=48.24325666507605
Tencent01:  iteration    10500/   50000 | elapsed time per iteration (ms): 2647.4 | learning rate 1.357E-04 | lm loss 3.011297E+00 | loss scale 32768.0 |
Tencent01: time (ms) | forward: 576.70 | backward: 1949.42 | optimizer: 121.03 | batch generator: 1.19 | data loader: 0.25
Tencent01: [2020-11-11 14:41:43,787] [INFO] [logging.py:60:log_dist] [Rank 0] step=10600, skipped=26, lr=[0.00013547343632257428, 0.00013547343632257428], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 14:41:43,838] [INFO] [timer.py:157:stop] 0/10600, SamplesPerSec=48.24437915956458
Tencent01:  iteration    10600/   50000 | elapsed time per iteration (ms): 2648.2 | learning rate 1.355E-04 | lm loss 3.017927E+00 | loss scale 32768.0 |
Tencent01: time (ms) | forward: 576.84 | backward: 1949.99 | optimizer: 121.12 | batch generator: 1.23 | data loader: 0.27
Tencent01: [2020-11-11 14:46:08,517] [INFO] [logging.py:60:log_dist] [Rank 0] step=10700, skipped=26, lr=[0.00013519351146347566, 0.00013519351146347566], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 14:46:08,569] [INFO] [timer.py:157:stop] 0/10700, SamplesPerSec=48.24564980198002
Tencent01:  iteration    10700/   50000 | elapsed time per iteration (ms): 2647.3 | learning rate 1.352E-04 | lm loss 3.014211E+00 | loss scale 32768.0 |
Tencent01: time (ms) | forward: 576.94 | backward: 1948.92 | optimizer: 121.16 | batch generator: 1.29 | data loader: 0.30
Tencent01: [2020-11-11 14:46:53,556] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 14:46:53,556] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent01: [2020-11-11 14:46:53,556] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 14:46:53,556] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 14:46:53,556] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent01: [2020-11-11 14:46:53,556] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent01: [2020-11-11 14:46:53,556] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 14:46:53,557] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 14:46:53,557] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent01: [2020-11-11 14:46:53,557] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent02: [2020-11-11 14:46:53,556] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 14:46:53,556] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 14:46:53,557] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent02: [2020-11-11 14:46:53,557] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent02: [2020-11-11 14:46:53,557] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 14:46:53,557] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent02: [2020-11-11 14:46:53,557] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 14:46:53,557] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent02: [2020-11-11 14:46:53,557] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 14:46:53,557] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent01: [2020-11-11 14:46:53,557] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 14:46:53,557] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 14:46:53,557] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent01: [2020-11-11 14:46:53,557] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent02: [2020-11-11 14:46:53,557] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 14:46:53,557] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 14:46:53,557] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent02: [2020-11-11 14:46:53,557] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 14:46:53,557] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent02: [2020-11-11 14:46:53,557] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent01: [2020-11-11 14:46:53,557] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 14:46:53,557] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent01: [2020-11-11 14:50:36,066] [INFO] [logging.py:60:log_dist] [Rank 0] step=10800, skipped=26, lr=[0.00013491121026761228, 0.00013491121026761228], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 14:50:36,118] [INFO] [timer.py:157:stop] 0/10800, SamplesPerSec=48.24214857032757
Tencent01:  iteration    10800/   50000 | elapsed time per iteration (ms): 2675.5 | learning rate 1.349E-04 | lm loss 3.014107E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 577.05 | backward: 1976.96 | optimizer: 121.20 | batch generator: 1.28 | data loader: 0.30
Tencent01: [2020-11-11 14:55:00,921] [INFO] [logging.py:60:log_dist] [Rank 0] step=10900, skipped=26, lr=[0.0001346265438797519, 0.0001346265438797519], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 14:55:00,973] [INFO] [timer.py:157:stop] 0/10900, SamplesPerSec=48.24319654156424
Tencent01:  iteration    10900/   50000 | elapsed time per iteration (ms): 2648.6 | learning rate 1.346E-04 | lm loss 2.986789E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 577.10 | backward: 1950.10 | optimizer: 121.09 | batch generator: 1.25 | data loader: 0.28
Tencent01: [2020-11-11 14:59:25,718] [INFO] [logging.py:60:log_dist] [Rank 0] step=11000, skipped=26, lr=[0.00013433952353803622, 0.00013433952353803622], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 14:59:25,770] [INFO] [timer.py:157:stop] 0/11000, SamplesPerSec=48.24433804780988
Tencent01:  iteration    11000/   50000 | elapsed time per iteration (ms): 2648.0 | learning rate 1.343E-04 | lm loss 2.986313E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 577.08 | backward: 1949.32 | optimizer: 121.27 | batch generator: 1.31 | data loader: 0.30
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: -----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 11000 | LM loss: 2.968170E+00 | LM PPL: 1.945629E+01
Tencent01: -----------------------------------------------------------------------------------
Tencent01: [2020-11-11 15:04:45,379] [INFO] [logging.py:60:log_dist] [Rank 0] step=11100, skipped=26, lr=[0.00013405016057353676, 0.00013405016057353676], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 15:04:45,431] [INFO] [timer.py:157:stop] 0/11100, SamplesPerSec=48.244010891769115
Tencent01:  iteration    11100/   50000 | elapsed time per iteration (ms): 3196.6 | learning rate 1.340E-04 | lm loss 2.995922E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.62 | backward: 1959.17 | optimizer: 120.70 | batch generator: 2.40 | data loader: 0.45
Tencent01: [2020-11-11 15:05:38,232] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 11119
Tencent01: [2020-11-11 15:05:38,232] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 11119
Tencent01: [2020-11-11 15:05:38,232] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 11119
Tencent01: [2020-11-11 15:05:38,232] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 11119
Tencent01: [2020-11-11 15:05:38,233] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent01: [2020-11-11 15:05:38,233] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent01: [2020-11-11 15:05:38,233] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent01: [2020-11-11 15:05:38,233] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent01: [2020-11-11 15:05:38,233] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 11119
Tencent01: [2020-11-11 15:05:38,233] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 11119
Tencent02: [2020-11-11 15:05:38,232] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-11 15:05:38,233] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 11119
Tencent01: [2020-11-11 15:05:38,233] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent02: Grad overflow on iteration 11119
Tencent01: [2020-11-11 15:05:38,233] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent02: [2020-11-11 15:05:38,232] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-11 15:05:38,233] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent02: Grad overflow on iteration 11119
Tencent01: [2020-11-11 15:05:38,233] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Tencent01: [2020-11-11 15:05:38,233] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 11119
Tencent02: [2020-11-11 15:05:38,232] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent01: [2020-11-11 15:05:38,233] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent02: [2020-11-11 15:05:38,232] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 11119
Tencent02: [2020-11-11 15:05:38,232] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent02: [2020-11-11 15:05:38,232] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 11119
Tencent02: [2020-11-11 15:05:38,232] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent02: [2020-11-11 15:05:38,232] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 11119
Tencent02: [2020-11-11 15:05:38,232] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 11119
Tencent02: [2020-11-11 15:05:38,232] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent02: [2020-11-11 15:05:38,232] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 11119
Tencent02: [2020-11-11 15:05:38,232] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent02: [2020-11-11 15:05:38,232] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent02: [2020-11-11 15:05:38,232] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent02: [2020-11-11 15:05:38,232] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 11119
Tencent02: [2020-11-11 15:05:38,232] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Tencent02: [2020-11-11 15:08:43,385] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-11 15:08:43,386] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 11188
Tencent01: Grad overflow on iteration 11188
Tencent02: [2020-11-11 15:08:43,385] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-11 15:08:43,386] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 11188
Tencent01: Grad overflow on iteration 11188
Tencent02: [2020-11-11 15:08:43,385] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent02: [2020-11-11 15:08:43,385] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent01: [2020-11-11 15:08:43,386] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent02: [2020-11-11 15:08:43,385] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 11188
Tencent01: [2020-11-11 15:08:43,386] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: [2020-11-11 15:08:43,386] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent01: Grad overflow on iteration 11188
Tencent02: [2020-11-11 15:08:43,385] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 11188
Tencent01: [2020-11-11 15:08:43,386] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent01: [2020-11-11 15:08:43,386] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: [2020-11-11 15:08:43,386] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 11188
Tencent02: Grad overflow on iteration 11188
Tencent02: [2020-11-11 15:08:43,385] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 11188
Tencent01: [2020-11-11 15:08:43,386] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent01: [2020-11-11 15:08:43,386] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 11188
Tencent01: [2020-11-11 15:08:43,386] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent01: [2020-11-11 15:08:43,386] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent02: [2020-11-11 15:08:43,386] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent02: [2020-11-11 15:08:43,386] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent01: [2020-11-11 15:08:43,386] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: [2020-11-11 15:08:43,386] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent02: [2020-11-11 15:08:43,386] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 11188
Tencent02: [2020-11-11 15:08:43,386] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 11188
Tencent02: Grad overflow on iteration 11188
Tencent02: [2020-11-11 15:08:43,386] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent01: [2020-11-11 15:08:43,386] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent02: [2020-11-11 15:08:43,386] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent01: [2020-11-11 15:08:43,386] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Tencent01: [2020-11-11 15:08:43,386] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 11188
Tencent01: [2020-11-11 15:08:43,386] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 11188
Tencent01: [2020-11-11 15:08:43,386] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent01: [2020-11-11 15:08:43,386] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Tencent01: [2020-11-11 15:09:12,433] [INFO] [logging.py:60:log_dist] [Rank 0] step=11200, skipped=28, lr=[0.00013376432306450348, 0.00013376432306450348], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 15:09:12,485] [INFO] [timer.py:157:stop] 0/11200, SamplesPerSec=48.24144487361691
Tencent01:  iteration    11200/   50000 | elapsed time per iteration (ms): 2670.5 | learning rate 1.338E-04 | lm loss 2.962831E+00 | loss scale 16384.0 |
Tencent01: time (ms) | forward: 576.68 | backward: 1975.16 | optimizer: 118.44 | batch generator: 1.25 | data loader: 0.27
Tencent01: [2020-11-11 15:13:37,146] [INFO] [logging.py:60:log_dist] [Rank 0] step=11300, skipped=28, lr=[0.00013347035549735845, 0.00013347035549735845], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 15:13:37,198] [INFO] [timer.py:157:stop] 0/11300, SamplesPerSec=48.2426980366941
Tencent01:  iteration    11300/   50000 | elapsed time per iteration (ms): 2647.1 | learning rate 1.335E-04 | lm loss 2.944537E+00 | loss scale 16384.0 |
Tencent01: time (ms) | forward: 576.88 | backward: 1949.19 | optimizer: 120.78 | batch generator: 1.27 | data loader: 0.28
Tencent01: [2020-11-11 15:18:01,854] [INFO] [logging.py:60:log_dist] [Rank 0] step=11400, skipped=28, lr=[0.00013317407962069565, 0.00013317407962069565], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 15:18:01,906] [INFO] [timer.py:157:stop] 0/11400, SamplesPerSec=48.24394955481047
Tencent01:  iteration    11400/   50000 | elapsed time per iteration (ms): 2647.1 | learning rate 1.332E-04 | lm loss 2.986013E+00 | loss scale 16384.0 |
Tencent01: time (ms) | forward: 577.22 | backward: 1948.09 | optimizer: 121.45 | batch generator: 1.31 | data loader: 0.30
Tencent01: [2020-11-11 15:21:12,456] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 11471
Tencent01: [2020-11-11 15:21:12,456] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 11471
Tencent01: [2020-11-11 15:21:12,456] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 11471
Tencent01: [2020-11-11 15:21:12,456] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Tencent01: [2020-11-11 15:21:12,456] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Tencent01: [2020-11-11 15:21:12,456] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Tencent01: [2020-11-11 15:21:12,456] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 11471
Tencent01: [2020-11-11 15:21:12,456] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 11471
Tencent01: [2020-11-11 15:21:12,456] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 11471
Tencent01: [2020-11-11 15:21:12,456] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 11471
Tencent02: [2020-11-11 15:21:12,455] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-11 15:21:12,456] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Tencent02: Grad overflow on iteration 11471
Tencent01: [2020-11-11 15:21:12,456] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Tencent01: [2020-11-11 15:21:12,456] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Tencent02: [2020-11-11 15:21:12,455] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-11 15:21:12,456] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Tencent02: Grad overflow on iteration 11471
Tencent02: [2020-11-11 15:21:12,455] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 11471
Tencent02: [2020-11-11 15:21:12,455] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Tencent01: [2020-11-11 15:21:12,456] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Tencent02: [2020-11-11 15:21:12,455] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-11 15:21:12,456] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 11471
Tencent02: Grad overflow on iteration 11471
Tencent02: [2020-11-11 15:21:12,455] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 11471
Tencent01: [2020-11-11 15:21:12,456] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Tencent02: [2020-11-11 15:21:12,455] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Tencent02: [2020-11-11 15:21:12,455] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Tencent02: [2020-11-11 15:21:12,455] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 11471
Tencent02: [2020-11-11 15:21:12,455] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Tencent02: [2020-11-11 15:21:12,455] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Tencent02: [2020-11-11 15:21:12,455] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 11471
Tencent02: [2020-11-11 15:21:12,455] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Tencent02: [2020-11-11 15:21:12,455] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Tencent02: [2020-11-11 15:21:12,455] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 11471
Tencent02: [2020-11-11 15:21:12,455] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Tencent01: [2020-11-11 15:22:26,486] [INFO] [logging.py:60:log_dist] [Rank 0] step=11500, skipped=29, lr=[0.0001328785041854831, 0.0001328785041854831], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 15:22:26,538] [INFO] [timer.py:157:stop] 0/11500, SamplesPerSec=48.24528599528638
Tencent01:  iteration    11500/   50000 | elapsed time per iteration (ms): 2646.3 | learning rate 1.329E-04 | lm loss 2.977281E+00 | loss scale 8192.0 |
Tencent01: time (ms) | forward: 577.02 | backward: 1949.63 | optimizer: 119.39 | batch generator: 1.27 | data loader: 0.28
Tencent01: [2020-11-11 15:26:52,212] [INFO] [logging.py:60:log_dist] [Rank 0] step=11600, skipped=29, lr=[0.00013257766965946044, 0.00013257766965946044], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 15:26:52,264] [INFO] [timer.py:157:stop] 0/11600, SamplesPerSec=48.244881684621134
Tencent01:  iteration    11600/   50000 | elapsed time per iteration (ms): 2657.3 | learning rate 1.326E-04 | lm loss 2.983141E+00 | loss scale 8192.0 |
Tencent01: time (ms) | forward: 576.69 | backward: 1959.91 | optimizer: 120.39 | batch generator: 1.27 | data loader: 0.28
Tencent01: [2020-11-11 15:31:18,266] [INFO] [logging.py:60:log_dist] [Rank 0] step=11700, skipped=29, lr=[0.0001322745620656284, 0.0001322745620656284], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 15:31:18,318] [INFO] [timer.py:157:stop] 0/11700, SamplesPerSec=48.243978799012815
Tencent01:  iteration    11700/   50000 | elapsed time per iteration (ms): 2660.5 | learning rate 1.323E-04 | lm loss 2.978822E+00 | loss scale 8192.0 |
Tencent01: time (ms) | forward: 576.99 | backward: 1962.62 | optimizer: 120.66 | batch generator: 1.28 | data loader: 0.29
Tencent01: [2020-11-11 15:35:42,877] [INFO] [logging.py:60:log_dist] [Rank 0] step=11800, skipped=29, lr=[0.00013196919337015575, 0.00013196919337015575], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 15:35:42,929] [INFO] [timer.py:157:stop] 0/11800, SamplesPerSec=48.24532050761405
Tencent01:  iteration    11800/   50000 | elapsed time per iteration (ms): 2646.1 | learning rate 1.320E-04 | lm loss 2.964552E+00 | loss scale 8192.0 |
Tencent01: time (ms) | forward: 576.83 | backward: 1948.29 | optimizer: 120.69 | batch generator: 1.29 | data loader: 0.29
Tencent01: [2020-11-11 15:40:07,652] [INFO] [logging.py:60:log_dist] [Rank 0] step=11900, skipped=29, lr=[0.00013166157562847572, 0.00013166157562847572], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 15:40:07,704] [INFO] [timer.py:157:stop] 0/11900, SamplesPerSec=48.2464080264646
Tencent01:  iteration    11900/   50000 | elapsed time per iteration (ms): 2647.7 | learning rate 1.317E-04 | lm loss 2.985626E+00 | loss scale 8192.0 |
Tencent01: time (ms) | forward: 577.21 | backward: 1949.02 | optimizer: 121.21 | batch generator: 1.40 | data loader: 0.31
Tencent01: [2020-11-11 15:44:32,369] [INFO] [logging.py:60:log_dist] [Rank 0] step=12000, skipped=29, lr=[0.00013135172098481006, 0.00013135172098481006], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 15:44:32,420] [INFO] [timer.py:157:stop] 0/12000, SamplesPerSec=48.24755682404993
Tencent01:  iteration    12000/   50000 | elapsed time per iteration (ms): 2647.2 | learning rate 1.313E-04 | lm loss 2.948652E+00 | loss scale 8192.0 |
Tencent01: time (ms) | forward: 577.04 | backward: 1948.63 | optimizer: 121.19 | batch generator: 1.34 | data loader: 0.30
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: -----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 12000 | LM loss: 2.942645E+00 | LM PPL: 1.896595E+01
Tencent01: -----------------------------------------------------------------------------------
Tencent01: [2020-11-11 15:49:54,388] [INFO] [logging.py:60:log_dist] [Rank 0] step=12100, skipped=29, lr=[0.00013103964167168953, 0.00013103964167168953], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 15:49:54,440] [INFO] [timer.py:157:stop] 0/12100, SamplesPerSec=48.24365563426901
Tencent01:  iteration    12100/   50000 | elapsed time per iteration (ms): 3220.2 | learning rate 1.310E-04 | lm loss 2.922839E+00 | loss scale 8192.0 |
Tencent01: time (ms) | forward: 577.12 | backward: 1982.13 | optimizer: 121.11 | batch generator: 2.53 | data loader: 0.47
Tencent01: [2020-11-11 15:54:19,052] [INFO] [logging.py:60:log_dist] [Rank 0] step=12200, skipped=29, lr=[0.00013072535000947102, 0.00013072535000947102], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 15:54:19,103] [INFO] [timer.py:157:stop] 0/12200, SamplesPerSec=48.2449020137261
Tencent01:  iteration    12200/   50000 | elapsed time per iteration (ms): 2646.6 | learning rate 1.307E-04 | lm loss 2.959447E+00 | loss scale 8192.0 |
Tencent01: time (ms) | forward: 577.16 | backward: 1947.39 | optimizer: 121.76 | batch generator: 1.43 | data loader: 0.32
Tencent01: [2020-11-11 15:58:43,725] [INFO] [logging.py:60:log_dist] [Rank 0] step=12300, skipped=29, lr=[0.00013040885840585122, 0.00013040885840585122], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 15:58:43,777] [INFO] [timer.py:157:stop] 0/12300, SamplesPerSec=48.24611414010626
Tencent01:  iteration    12300/   50000 | elapsed time per iteration (ms): 2646.7 | learning rate 1.304E-04 | lm loss 2.945103E+00 | loss scale 8192.0 |
Tencent01: time (ms) | forward: 577.26 | backward: 1947.74 | optimizer: 121.41 | batch generator: 1.43 | data loader: 0.32
Tencent01: [2020-11-11 16:03:08,338] [INFO] [logging.py:60:log_dist] [Rank 0] step=12400, skipped=29, lr=[0.00013009017935537674, 0.00013009017935537674], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 16:03:08,390] [INFO] [timer.py:157:stop] 0/12400, SamplesPerSec=48.24737800760748
Tencent01:  iteration    12400/   50000 | elapsed time per iteration (ms): 2646.1 | learning rate 1.301E-04 | lm loss 2.925283E+00 | loss scale 8192.0 |
Tencent01: time (ms) | forward: 577.04 | backward: 1947.97 | optimizer: 120.82 | batch generator: 1.34 | data loader: 0.30
Tencent01: [2020-11-11 16:06:23,777] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 16:06:23,777] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Tencent01: [2020-11-11 16:06:23,777] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 16:06:23,777] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 16:06:23,777] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Tencent01: [2020-11-11 16:06:23,777] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Tencent02: [2020-11-11 16:06:23,777] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 16:06:23,777] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Tencent01: [2020-11-11 16:06:23,777] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 16:06:23,777] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Tencent02: [2020-11-11 16:06:23,777] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 16:06:23,777] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 16:06:23,777] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Tencent01: [2020-11-11 16:06:23,778] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 16:06:23,777] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Tencent01: [2020-11-11 16:06:23,778] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Tencent01: [2020-11-11 16:06:23,778] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 16:06:23,778] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Tencent01: [2020-11-11 16:06:23,778] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 16:06:23,778] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Tencent02: [2020-11-11 16:06:23,778] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 16:06:23,778] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 16:06:23,778] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Tencent02: [2020-11-11 16:06:23,778] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 16:06:23,778] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Tencent02: [2020-11-11 16:06:23,778] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 16:06:23,778] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Tencent02: [2020-11-11 16:06:23,778] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Tencent01: [2020-11-11 16:06:23,778] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 16:06:23,778] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Tencent01: [2020-11-11 16:06:23,778] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 16:06:23,778] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Tencent01: [2020-11-11 16:07:35,275] [INFO] [logging.py:60:log_dist] [Rank 0] step=12500, skipped=29, lr=[0.00012976932543895076, 0.00012976932543895076], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 16:07:35,327] [INFO] [timer.py:157:stop] 0/12500, SamplesPerSec=48.24523222832467
Tencent01:  iteration    12500/   50000 | elapsed time per iteration (ms): 2669.4 | learning rate 1.298E-04 | lm loss 2.988047E+00 | loss scale 16384.0 |
Tencent01: time (ms) | forward: 576.88 | backward: 1971.49 | optimizer: 120.71 | batch generator: 1.30 | data loader: 0.29
Tencent01: [2020-11-11 16:11:59,938] [INFO] [logging.py:60:log_dist] [Rank 0] step=12600, skipped=29, lr=[0.00012944630932333658, 0.00012944630932333658], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 16:11:59,990] [INFO] [timer.py:157:stop] 0/12600, SamplesPerSec=48.24638161736288
Tencent01:  iteration    12600/   50000 | elapsed time per iteration (ms): 2646.6 | learning rate 1.294E-04 | lm loss 2.942797E+00 | loss scale 16384.0 |
Tencent01: time (ms) | forward: 576.86 | backward: 1949.28 | optimizer: 120.26 | batch generator: 1.20 | data loader: 0.27
Tencent01: [2020-11-11 16:16:24,696] [INFO] [logging.py:60:log_dist] [Rank 0] step=12700, skipped=29, lr=[0.00012912114376065735, 0.00012912114376065735], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 16:16:24,748] [INFO] [timer.py:157:stop] 0/12700, SamplesPerSec=48.24737803317801
Tencent01:  iteration    12700/   50000 | elapsed time per iteration (ms): 2647.6 | learning rate 1.291E-04 | lm loss 2.945795E+00 | loss scale 16384.0 |
Tencent01: time (ms) | forward: 576.86 | backward: 1950.09 | optimizer: 120.38 | batch generator: 1.20 | data loader: 0.27
Tencent01: [2020-11-11 16:20:49,319] [INFO] [logging.py:60:log_dist] [Rank 0] step=12800, skipped=29, lr=[0.00012879384158789265, 0.00012879384158789265], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 16:20:49,371] [INFO] [timer.py:157:stop] 0/12800, SamplesPerSec=48.24855422045188
Tencent01:  iteration    12800/   50000 | elapsed time per iteration (ms): 2646.2 | learning rate 1.288E-04 | lm loss 2.919682E+00 | loss scale 16384.0 |
Tencent01: time (ms) | forward: 576.65 | backward: 1948.84 | optimizer: 120.47 | batch generator: 1.19 | data loader: 0.27
Tencent01: [2020-11-11 16:25:16,252] [INFO] [logging.py:60:log_dist] [Rank 0] step=12900, skipped=29, lr=[0.0001284644157263719, 0.0001284644157263719], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 16:25:16,304] [INFO] [timer.py:157:stop] 0/12900, SamplesPerSec=48.24645432357999
Tencent01:  iteration    12900/   50000 | elapsed time per iteration (ms): 2669.3 | learning rate 1.285E-04 | lm loss 2.926817E+00 | loss scale 16384.0 |
Tencent01: time (ms) | forward: 576.60 | backward: 1972.05 | optimizer: 120.40 | batch generator: 1.18 | data loader: 0.27
Tencent01: [2020-11-11 16:29:40,922] [INFO] [logging.py:60:log_dist] [Rank 0] step=13000, skipped=29, lr=[0.000128132879181264, 0.000128132879181264], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 16:29:40,957] [INFO] [timer.py:157:stop] 0/13000, SamplesPerSec=48.24757961525976
Tencent01:  iteration    13000/   50000 | elapsed time per iteration (ms): 2646.5 | learning rate 1.281E-04 | lm loss 2.940041E+00 | loss scale 16384.0 |
Tencent01: time (ms) | forward: 576.76 | backward: 1948.67 | optimizer: 120.81 | batch generator: 1.19 | data loader: 0.27
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: -----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 13000 | LM loss: 2.942313E+00 | LM PPL: 1.895966E+01
Tencent01: -----------------------------------------------------------------------------------
Tencent01: [2020-11-11 16:35:00,539] [INFO] [logging.py:60:log_dist] [Rank 0] step=13100, skipped=29, lr=[0.0001277992450410641, 0.0001277992450410641], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 16:35:00,591] [INFO] [timer.py:157:stop] 0/13100, SamplesPerSec=48.247293236707534
Tencent01:  iteration    13100/   50000 | elapsed time per iteration (ms): 3196.3 | learning rate 1.278E-04 | lm loss 2.919733E+00 | loss scale 16384.0 |
Tencent01: time (ms) | forward: 576.55 | backward: 1958.99 | optimizer: 120.78 | batch generator: 2.27 | data loader: 0.40
Tencent01: [2020-11-11 16:39:25,221] [INFO] [logging.py:60:log_dist] [Rank 0] step=13200, skipped=29, lr=[0.00012746352647707677, 0.00012746352647707677], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 16:39:25,273] [INFO] [timer.py:157:stop] 0/13200, SamplesPerSec=48.24835887243812
Tencent01:  iteration    13200/   50000 | elapsed time per iteration (ms): 2646.8 | learning rate 1.275E-04 | lm loss 2.916445E+00 | loss scale 16384.0 |
Tencent01: time (ms) | forward: 576.67 | backward: 1948.79 | optimizer: 121.09 | batch generator: 1.25 | data loader: 0.28
Tencent01: [2020-11-11 16:43:52,172] [INFO] [logging.py:60:log_dist] [Rank 0] step=13300, skipped=29, lr=[0.00012712573674289606, 0.00012712573674289606], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 16:43:52,223] [INFO] [timer.py:157:stop] 0/13300, SamplesPerSec=48.24630782717836
Tencent01:  iteration    13300/   50000 | elapsed time per iteration (ms): 2669.5 | learning rate 1.271E-04 | lm loss 2.949277E+00 | loss scale 16384.0 |
Tencent01: time (ms) | forward: 576.80 | backward: 1971.68 | optimizer: 120.73 | batch generator: 1.24 | data loader: 0.27
Tencent01: [2020-11-11 16:48:16,842] [INFO] [logging.py:60:log_dist] [Rank 0] step=13400, skipped=29, lr=[0.00012678588917388228, 0.00012678588917388228], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 16:48:16,894] [INFO] [timer.py:157:stop] 0/13400, SamplesPerSec=48.24738339535372
Tencent01:  iteration    13400/   50000 | elapsed time per iteration (ms): 2646.7 | learning rate 1.268E-04 | lm loss 2.930447E+00 | loss scale 16384.0 |
Tencent01: time (ms) | forward: 576.90 | backward: 1948.41 | optimizer: 121.13 | batch generator: 1.26 | data loader: 0.29
Tencent01: [2020-11-11 16:51:30,005] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 16:51:30,005] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent02: [2020-11-11 16:51:30,004] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 16:51:30,004] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent02: [2020-11-11 16:51:30,004] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 16:51:30,004] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent02: [2020-11-11 16:51:30,004] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 16:51:30,004] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent01: [2020-11-11 16:51:30,005] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 16:51:30,005] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 16:51:30,005] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent01: [2020-11-11 16:51:30,005] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent02: [2020-11-11 16:51:30,004] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 16:51:30,004] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 16:51:30,004] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent01: [2020-11-11 16:51:30,005] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 16:51:30,005] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent02: [2020-11-11 16:51:30,005] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent02: [2020-11-11 16:51:30,005] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 16:51:30,005] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 16:51:30,005] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent02: [2020-11-11 16:51:30,005] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent01: [2020-11-11 16:51:30,005] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 16:51:30,005] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 16:51:30,006] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent01: [2020-11-11 16:51:30,006] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent01: [2020-11-11 16:51:30,006] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 16:51:30,006] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent02: [2020-11-11 16:51:30,006] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 16:51:30,006] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent01: [2020-11-11 16:51:30,008] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 16:51:30,008] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Tencent01: [2020-11-11 16:52:41,508] [INFO] [logging.py:60:log_dist] [Rank 0] step=13500, skipped=29, lr=[0.00012644399718663557, 0.00012644399718663557], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 16:52:41,560] [INFO] [timer.py:157:stop] 0/13500, SamplesPerSec=48.2484460169065
Tencent01:  iteration    13500/   50000 | elapsed time per iteration (ms): 2646.7 | learning rate 1.264E-04 | lm loss 2.923296E+00 | loss scale 32768.0 |
Tencent01: time (ms) | forward: 576.71 | backward: 1948.58 | optimizer: 121.11 | batch generator: 1.25 | data loader: 0.28
Tencent01: [2020-11-11 16:57:06,156] [INFO] [logging.py:60:log_dist] [Rank 0] step=13600, skipped=29, lr=[0.0001261000742784662, 0.0001261000742784662], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 16:57:06,208] [INFO] [timer.py:157:stop] 0/13600, SamplesPerSec=48.24951108518879
Tencent01:  iteration    13600/   50000 | elapsed time per iteration (ms): 2646.5 | learning rate 1.261E-04 | lm loss 2.917451E+00 | loss scale 32768.0 |
Tencent01: time (ms) | forward: 576.64 | backward: 1948.48 | optimizer: 121.11 | batch generator: 1.22 | data loader: 0.28
Tencent01: [2020-11-11 17:01:31,741] [INFO] [logging.py:60:log_dist] [Rank 0] step=13700, skipped=29, lr=[0.00012575413402686163, 0.00012575413402686163], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 17:01:31,793] [INFO] [timer.py:157:stop] 0/13700, SamplesPerSec=48.249316523441365
Tencent01:  iteration    13700/   50000 | elapsed time per iteration (ms): 2655.9 | learning rate 1.258E-04 | lm loss 2.926061E+00 | loss scale 32768.0 |
Tencent01: time (ms) | forward: 576.56 | backward: 1958.31 | optimizer: 120.71 | batch generator: 1.21 | data loader: 0.28
Tencent01: [2020-11-11 17:05:57,810] [INFO] [logging.py:60:log_dist] [Rank 0] step=13800, skipped=29, lr=[0.00012540619008895065, 0.00012540619008895065], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 17:05:57,862] [INFO] [timer.py:157:stop] 0/13800, SamplesPerSec=48.248485571098975
Tencent01:  iteration    13800/   50000 | elapsed time per iteration (ms): 2660.7 | learning rate 1.254E-04 | lm loss 2.926025E+00 | loss scale 32768.0 |
Tencent01: time (ms) | forward: 576.81 | backward: 1963.36 | optimizer: 120.27 | batch generator: 1.21 | data loader: 0.27
Tencent01: [2020-11-11 17:10:22,446] [INFO] [logging.py:60:log_dist] [Rank 0] step=13900, skipped=29, lr=[0.00012505625620096422, 0.00012505625620096422], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 17:10:22,498] [INFO] [timer.py:157:stop] 0/13900, SamplesPerSec=48.249534744232136
Tencent01:  iteration    13900/   50000 | elapsed time per iteration (ms): 2646.4 | learning rate 1.251E-04 | lm loss 2.909499E+00 | loss scale 32768.0 |
Tencent01: time (ms) | forward: 576.65 | backward: 1949.16 | optimizer: 120.32 | batch generator: 1.17 | data loader: 0.26
Tencent01: [2020-11-11 17:14:47,136] [INFO] [logging.py:60:log_dist] [Rank 0] step=14000, skipped=29, lr=[0.00012470434617769302, 0.00012470434617769302], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 17:14:47,189] [INFO] [timer.py:157:stop] 0/14000, SamplesPerSec=48.25049898594511
Tencent01:  iteration    14000/   50000 | elapsed time per iteration (ms): 2646.9 | learning rate 1.247E-04 | lm loss 2.887686E+00 | loss scale 32768.0 |
Tencent01: time (ms) | forward: 576.64 | backward: 1949.74 | optimizer: 120.27 | batch generator: 1.17 | data loader: 0.26
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: -----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 14000 | LM loss: 2.926990E+00 | LM PPL: 1.867135E+01
Tencent01: -----------------------------------------------------------------------------------
Tencent01: [2020-11-11 17:20:06,887] [INFO] [logging.py:60:log_dist] [Rank 0] step=14100, skipped=29, lr=[0.00012435047391194217, 0.00012435047391194217], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 17:20:06,939] [INFO] [timer.py:157:stop] 0/14100, SamplesPerSec=48.25002735907032
Tencent01:  iteration    14100/   50000 | elapsed time per iteration (ms): 3197.5 | learning rate 1.243E-04 | lm loss 2.915962E+00 | loss scale 32768.0 |
Tencent01: time (ms) | forward: 576.38 | backward: 1961.03 | optimizer: 120.25 | batch generator: 2.16 | data loader: 0.40
Tencent01: [2020-11-11 17:24:33,942] [INFO] [logging.py:60:log_dist] [Rank 0] step=14200, skipped=29, lr=[0.00012399465337398284, 0.00012399465337398284], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 17:24:33,994] [INFO] [timer.py:157:stop] 0/14200, SamplesPerSec=48.24794421141147
Tencent01:  iteration    14200/   50000 | elapsed time per iteration (ms): 2670.5 | learning rate 1.240E-04 | lm loss 2.921399E+00 | loss scale 32768.0 |
Tencent01: time (ms) | forward: 576.78 | backward: 1973.25 | optimizer: 120.28 | batch generator: 1.16 | data loader: 0.26
Tencent01: [2020-11-11 17:28:58,734] [INFO] [logging.py:60:log_dist] [Rank 0] step=14300, skipped=29, lr=[0.00012363689861100057, 0.00012363689861100057], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 17:28:58,786] [INFO] [timer.py:157:stop] 0/14300, SamplesPerSec=48.24876543483965
Tencent01:  iteration    14300/   50000 | elapsed time per iteration (ms): 2647.9 | learning rate 1.236E-04 | lm loss 2.912597E+00 | loss scale 32768.0 |
Tencent01: time (ms) | forward: 576.80 | backward: 1950.60 | optimizer: 120.29 | batch generator: 1.15 | data loader: 0.27
Tencent01: [2020-11-11 17:33:23,429] [INFO] [logging.py:60:log_dist] [Rank 0] step=14400, skipped=29, lr=[0.00012327722374654083, 0.00012327722374654083], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 17:33:23,481] [INFO] [timer.py:157:stop] 0/14400, SamplesPerSec=48.249699762024214
Tencent01:  iteration    14400/   50000 | elapsed time per iteration (ms): 2646.9 | learning rate 1.233E-04 | lm loss 2.904917E+00 | loss scale 32768.0 |
Tencent01: time (ms) | forward: 576.52 | backward: 1949.95 | optimizer: 120.24 | batch generator: 1.15 | data loader: 0.26
Tencent01: [2020-11-11 17:36:36,668] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 17:36:36,668] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent01: [2020-11-11 17:36:36,668] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 17:36:36,668] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent02: [2020-11-11 17:36:36,667] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 17:36:36,668] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent02: [2020-11-11 17:36:36,668] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 17:36:36,668] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 17:36:36,668] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent02: [2020-11-11 17:36:36,668] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent01: [2020-11-11 17:36:36,669] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 17:36:36,669] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent02: [2020-11-11 17:36:36,668] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 17:36:36,669] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 17:36:36,669] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 17:36:36,668] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 17:36:36,669] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent02: [2020-11-11 17:36:36,668] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent01: [2020-11-11 17:36:36,669] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent02: [2020-11-11 17:36:36,668] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent02: [2020-11-11 17:36:36,668] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 17:36:36,668] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent01: [2020-11-11 17:36:36,669] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 17:36:36,669] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 17:36:36,669] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent01: [2020-11-11 17:36:36,669] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent01: [2020-11-11 17:36:36,669] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 17:36:36,668] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 17:36:36,669] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent02: [2020-11-11 17:36:36,668] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent02: [2020-11-11 17:36:36,668] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 17:36:36,668] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Tencent01: [2020-11-11 17:37:48,205] [INFO] [logging.py:60:log_dist] [Rank 0] step=14500, skipped=29, lr=[0.00012291564297995148, 0.00012291564297995148], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 17:37:48,258] [INFO] [timer.py:157:stop] 0/14500, SamplesPerSec=48.250518962160726
Tencent01:  iteration    14500/   50000 | elapsed time per iteration (ms): 2647.8 | learning rate 1.229E-04 | lm loss 2.902718E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.65 | backward: 1950.61 | optimizer: 120.27 | batch generator: 1.15 | data loader: 0.26
Tencent01: [2020-11-11 17:42:15,339] [INFO] [logging.py:60:log_dist] [Rank 0] step=14600, skipped=29, lr=[0.00012255217058582197, 0.00012255217058582197], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 17:42:15,391] [INFO] [timer.py:157:stop] 0/14600, SamplesPerSec=48.24839579277174
Tencent01:  iteration    14600/   50000 | elapsed time per iteration (ms): 2671.3 | learning rate 1.225E-04 | lm loss 2.910841E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 577.06 | backward: 1973.75 | optimizer: 120.29 | batch generator: 1.19 | data loader: 0.27
Tencent01: [2020-11-11 17:46:40,135] [INFO] [logging.py:60:log_dist] [Rank 0] step=14700, skipped=29, lr=[0.0001221868209134201, 0.0001221868209134201], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 17:46:40,187] [INFO] [timer.py:157:stop] 0/14700, SamplesPerSec=48.249189132596875
Tencent01:  iteration    14700/   50000 | elapsed time per iteration (ms): 2648.0 | learning rate 1.222E-04 | lm loss 2.899793E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.69 | backward: 1950.80 | optimizer: 120.24 | batch generator: 1.16 | data loader: 0.26
Tencent01: [2020-11-11 17:51:04,928] [INFO] [logging.py:60:log_dist] [Rank 0] step=14800, skipped=29, lr=[0.00012181960838612535, 0.00012181960838612535], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 17:51:04,981] [INFO] [timer.py:157:stop] 0/14800, SamplesPerSec=48.24998060490941
Tencent01:  iteration    14800/   50000 | elapsed time per iteration (ms): 2647.9 | learning rate 1.218E-04 | lm loss 2.886832E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.66 | backward: 1950.78 | optimizer: 120.26 | batch generator: 1.20 | data loader: 0.27
Tencent01: [2020-11-11 17:55:29,721] [INFO] [logging.py:60:log_dist] [Rank 0] step=14900, skipped=29, lr=[0.00012145054750085952, 0.00012145054750085952], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 17:55:29,772] [INFO] [timer.py:157:stop] 0/14900, SamplesPerSec=48.25076176532946
Tencent01:  iteration    14900/   50000 | elapsed time per iteration (ms): 2647.9 | learning rate 1.214E-04 | lm loss 2.882200E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.65 | backward: 1950.58 | optimizer: 120.45 | batch generator: 1.18 | data loader: 0.27
Tencent01: [2020-11-11 17:59:56,678] [INFO] [logging.py:60:log_dist] [Rank 0] step=15000, skipped=29, lr=[0.00012107965282751442, 0.00012107965282751442], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 17:59:56,730] [INFO] [timer.py:157:stop] 0/15000, SamplesPerSec=48.24890578964272
Tencent01:  iteration    15000/   50000 | elapsed time per iteration (ms): 2669.6 | learning rate 1.211E-04 | lm loss 2.902068E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.91 | backward: 1971.96 | optimizer: 120.46 | batch generator: 1.17 | data loader: 0.27
Tencent01: [2020-11-11 17:59:56,733] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: checkpoints/gpt-345M11-11-06-41/15000/mp_rank_00_model_states.pt
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: -----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 15000 | LM loss: 2.859603E+00 | LM PPL: 1.745459E+01
Tencent01: -----------------------------------------------------------------------------------
Tencent01: [2020-11-11 18:05:51,949] [INFO] [logging.py:60:log_dist] [Rank 0] step=15100, skipped=29, lr=[0.00012070693900837672, 0.00012070693900837672], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 18:05:52,001] [INFO] [timer.py:157:stop] 0/15100, SamplesPerSec=48.248497157193654
Tencent01:  iteration    15100/   50000 | elapsed time per iteration (ms): 3552.7 | learning rate 1.207E-04 | lm loss 2.908796E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.47 | backward: 1960.69 | optimizer: 120.38 | batch generator: 2.17 | data loader: 0.41
Tencent01: [2020-11-11 18:10:16,684] [INFO] [logging.py:60:log_dist] [Rank 0] step=15200, skipped=29, lr=[0.00012033242075754976, 0.00012033242075754976], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 18:10:16,736] [INFO] [timer.py:157:stop] 0/15200, SamplesPerSec=48.249347712844916
Tencent01:  iteration    15200/   50000 | elapsed time per iteration (ms): 2647.4 | learning rate 1.203E-04 | lm loss 2.877213E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.83 | backward: 1949.88 | optimizer: 120.39 | batch generator: 1.23 | data loader: 0.27
Tencent01: [2020-11-11 18:14:41,431] [INFO] [logging.py:60:log_dist] [Rank 0] step=15300, skipped=29, lr=[0.00011995611286037282, 0.00011995611286037282], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 18:14:41,483] [INFO] [timer.py:157:stop] 0/15300, SamplesPerSec=48.25017188903463
Tencent01:  iteration    15300/   50000 | elapsed time per iteration (ms): 2647.5 | learning rate 1.200E-04 | lm loss 2.882288E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.79 | backward: 1949.48 | optimizer: 120.91 | batch generator: 1.19 | data loader: 0.27
Tencent01: [2020-11-11 18:19:08,766] [INFO] [logging.py:60:log_dist] [Rank 0] step=15400, skipped=29, lr=[0.00011957803017283739, 0.00011957803017283739], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 18:19:08,818] [INFO] [timer.py:157:stop] 0/15400, SamplesPerSec=48.24793066665373
Tencent01:  iteration    15400/   50000 | elapsed time per iteration (ms): 2673.3 | learning rate 1.196E-04 | lm loss 2.889288E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.63 | backward: 1975.64 | optimizer: 120.80 | batch generator: 1.21 | data loader: 0.27
Tencent01: [2020-11-11 18:22:21,978] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 18:22:21,978] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-11 18:22:21,978] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 18:22:21,978] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-11 18:22:21,978] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 18:22:21,979] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-11 18:22:21,978] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 18:22:21,978] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-11 18:22:21,978] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 18:22:21,979] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 18:22:21,978] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-11 18:22:21,979] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-11 18:22:21,978] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 18:22:21,978] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-11 18:22:21,979] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 18:22:21,978] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 18:22:21,979] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-11 18:22:21,978] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-11 18:22:21,979] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 18:22:21,979] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 18:22:21,978] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 18:22:21,979] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-11 18:22:21,979] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-11 18:22:21,978] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-11 18:22:21,978] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 18:22:21,979] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-11 18:22:21,979] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 18:22:21,979] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 18:22:21,979] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 18:22:21,979] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-11 18:22:21,979] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-11 18:22:21,979] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-11 18:23:33,543] [INFO] [logging.py:60:log_dist] [Rank 0] step=15500, skipped=29, lr=[0.0001191981876210005, 0.0001191981876210005], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 18:23:33,595] [INFO] [timer.py:157:stop] 0/15500, SamplesPerSec=48.24872118436635
Tencent01:  iteration    15500/   50000 | elapsed time per iteration (ms): 2647.8 | learning rate 1.192E-04 | lm loss 2.878221E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 576.72 | backward: 1949.81 | optimizer: 120.97 | batch generator: 1.22 | data loader: 0.27
Tencent01: [2020-11-11 18:27:58,321] [INFO] [logging.py:60:log_dist] [Rank 0] step=15600, skipped=29, lr=[0.00011881660020039578, 0.00011881660020039578], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 18:27:58,373] [INFO] [timer.py:157:stop] 0/15600, SamplesPerSec=48.24949552730353
Tencent01:  iteration    15600/   50000 | elapsed time per iteration (ms): 2647.8 | learning rate 1.188E-04 | lm loss 2.869117E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 576.70 | backward: 1950.58 | optimizer: 120.26 | batch generator: 1.22 | data loader: 0.27
Tencent01: [2020-11-11 18:32:23,020] [INFO] [logging.py:60:log_dist] [Rank 0] step=15700, skipped=29, lr=[0.0001184332829754412, 0.0001184332829754412], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 18:32:23,072] [INFO] [timer.py:157:stop] 0/15700, SamplesPerSec=48.25034849264229
Tencent01:  iteration    15700/   50000 | elapsed time per iteration (ms): 2647.0 | learning rate 1.184E-04 | lm loss 2.910504E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 576.69 | backward: 1949.80 | optimizer: 120.26 | batch generator: 1.20 | data loader: 0.27
Tencent01: [2020-11-11 18:36:48,782] [INFO] [logging.py:60:log_dist] [Rank 0] step=15800, skipped=29, lr=[0.00011804825107884441, 0.00011804825107884441], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 18:36:48,834] [INFO] [timer.py:157:stop] 0/15800, SamplesPerSec=48.24996960056845
Tencent01:  iteration    15800/   50000 | elapsed time per iteration (ms): 2657.6 | learning rate 1.180E-04 | lm loss 2.875501E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 576.52 | backward: 1960.09 | optimizer: 120.76 | batch generator: 1.21 | data loader: 0.27
Tencent01: [2020-11-11 18:39:55,380] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 15869
Tencent01: [2020-11-11 18:39:55,380] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 15869
Tencent01: [2020-11-11 18:39:55,380] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-11 18:39:55,380] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 15869
Tencent01: [2020-11-11 18:39:55,380] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-11 18:39:55,380] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-11 18:39:55,380] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Tencent01: [2020-11-11 18:39:55,380] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 15869
Tencent01: [2020-11-11 18:39:55,380] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 15869
Tencent02: [2020-11-11 18:39:55,380] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-11 18:39:55,381] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: Grad overflow on iteration 15869
Tencent02: [2020-11-11 18:39:55,380] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-11 18:39:55,380] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 15869
Tencent02: [2020-11-11 18:39:55,380] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 15869
Tencent01: [2020-11-11 18:39:55,380] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: [2020-11-11 18:39:55,380] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 15869
Tencent01: Grad overflow on iteration 15869
Tencent01: [2020-11-11 18:39:55,381] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-11 18:39:55,380] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-11 18:39:55,380] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-11 18:39:55,381] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: [2020-11-11 18:39:55,380] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 15869
Tencent02: Grad overflow on iteration 15869
Tencent01: [2020-11-11 18:39:55,381] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-11 18:39:55,380] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-11 18:39:55,381] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-11 18:39:55,380] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-11 18:39:55,381] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: [2020-11-11 18:39:55,380] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 15869
Tencent02: Grad overflow on iteration 15869
Tencent01: [2020-11-11 18:39:55,381] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-11 18:39:55,380] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 15869
Tencent02: [2020-11-11 18:39:55,380] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-11 18:39:55,380] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-11 18:39:55,380] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 15869
Tencent02: [2020-11-11 18:39:55,380] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-11 18:41:14,700] [INFO] [logging.py:60:log_dist] [Rank 0] step=15900, skipped=30, lr=[0.000117665395386991, 0.000117665395386991], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 18:41:14,753] [INFO] [timer.py:157:stop] 0/15900, SamplesPerSec=48.24941158373634
Tencent01:  iteration    15900/   50000 | elapsed time per iteration (ms): 2659.2 | learning rate 1.177E-04 | lm loss 2.886260E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.39 | backward: 1963.24 | optimizer: 119.31 | batch generator: 1.18 | data loader: 0.26
Tencent01: [2020-11-11 18:45:39,437] [INFO] [logging.py:60:log_dist] [Rank 0] step=16000, skipped=30, lr=[0.00011727699658164668, 0.00011727699658164668], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 18:45:39,489] [INFO] [timer.py:157:stop] 0/16000, SamplesPerSec=48.25020851024156
Tencent01:  iteration    16000/   50000 | elapsed time per iteration (ms): 2647.4 | learning rate 1.173E-04 | lm loss 2.870802E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.81 | backward: 1949.92 | optimizer: 120.39 | batch generator: 1.20 | data loader: 0.27
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: -----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 16000 | LM loss: 2.889095E+00 | LM PPL: 1.797703E+01
Tencent01: -----------------------------------------------------------------------------------
Tencent01: [2020-11-11 18:50:59,157] [INFO] [logging.py:60:log_dist] [Rank 0] step=16100, skipped=30, lr=[0.00011688692875286713, 0.00011688692875286713], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 18:50:59,209] [INFO] [timer.py:157:stop] 0/16100, SamplesPerSec=48.24989106129124
Tencent01:  iteration    16100/   50000 | elapsed time per iteration (ms): 3197.2 | learning rate 1.169E-04 | lm loss 2.890375E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.56 | backward: 1959.73 | optimizer: 120.56 | batch generator: 2.13 | data loader: 0.40
Tencent01: [2020-11-11 18:55:23,819] [INFO] [logging.py:60:log_dist] [Rank 0] step=16200, skipped=30, lr=[0.00011649520729986231, 0.00011649520729986231], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 18:55:23,871] [INFO] [timer.py:157:stop] 0/16200, SamplesPerSec=48.25076502022694
Tencent01:  iteration    16200/   50000 | elapsed time per iteration (ms): 2646.6 | learning rate 1.165E-04 | lm loss 2.866797E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.57 | backward: 1948.62 | optimizer: 121.17 | batch generator: 1.24 | data loader: 0.28
Tencent01: [2020-11-11 18:59:51,242] [INFO] [logging.py:60:log_dist] [Rank 0] step=16300, skipped=30, lr=[0.00011610184768712447, 0.00011610184768712447], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 18:59:51,294] [INFO] [timer.py:157:stop] 0/16300, SamplesPerSec=48.248547293219715
Tencent01:  iteration    16300/   50000 | elapsed time per iteration (ms): 2674.2 | learning rate 1.161E-04 | lm loss 2.878550E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.81 | backward: 1976.02 | optimizer: 121.13 | batch generator: 1.25 | data loader: 0.28
Tencent01: [2020-11-11 19:04:16,032] [INFO] [logging.py:60:log_dist] [Rank 0] step=16400, skipped=30, lr=[0.00011570686544381758, 0.00011570686544381758], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 19:04:16,084] [INFO] [timer.py:157:stop] 0/16400, SamplesPerSec=48.24927613746291
Tencent01:  iteration    16400/   50000 | elapsed time per iteration (ms): 2647.9 | learning rate 1.157E-04 | lm loss 2.862120E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.87 | backward: 1949.62 | optimizer: 121.13 | batch generator: 1.23 | data loader: 0.28
Tencent01: [2020-11-11 19:08:40,758] [INFO] [logging.py:60:log_dist] [Rank 0] step=16500, skipped=30, lr=[0.00011531027616316428, 0.00011531027616316428], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 19:08:40,811] [INFO] [timer.py:157:stop] 0/16500, SamplesPerSec=48.25006577610852
Tencent01:  iteration    16500/   50000 | elapsed time per iteration (ms): 2647.3 | learning rate 1.153E-04 | lm loss 2.895078E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.64 | backward: 1949.34 | optimizer: 121.01 | batch generator: 1.22 | data loader: 0.27
Tencent01: [2020-11-11 19:13:05,466] [INFO] [logging.py:60:log_dist] [Rank 0] step=16600, skipped=30, lr=[0.00011491209550183032, 0.00011491209550183032], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 19:13:05,518] [INFO] [timer.py:157:stop] 0/16600, SamplesPerSec=48.25086237985922
Tencent01:  iteration    16600/   50000 | elapsed time per iteration (ms): 2647.1 | learning rate 1.149E-04 | lm loss 2.847147E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.59 | backward: 1949.99 | optimizer: 120.25 | batch generator: 1.22 | data loader: 0.27
Tencent01: [2020-11-11 19:17:32,569] [INFO] [logging.py:60:log_dist] [Rank 0] step=16700, skipped=30, lr=[0.00011451233917930639, 0.00011451233917930639], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 19:17:32,621] [INFO] [timer.py:157:stop] 0/16700, SamplesPerSec=48.249038504908505
Tencent01:  iteration    16700/   50000 | elapsed time per iteration (ms): 2671.0 | learning rate 1.145E-04 | lm loss 2.844468E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.68 | backward: 1973.85 | optimizer: 120.25 | batch generator: 1.20 | data loader: 0.27
Tencent01: [2020-11-11 19:21:57,387] [INFO] [logging.py:60:log_dist] [Rank 0] step=16800, skipped=30, lr=[0.00011411102297728763, 0.00011411102297728763], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 19:21:57,439] [INFO] [timer.py:157:stop] 0/16800, SamplesPerSec=48.24971364586634
Tencent01:  iteration    16800/   50000 | elapsed time per iteration (ms): 2648.2 | learning rate 1.141E-04 | lm loss 2.890192E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.69 | backward: 1950.34 | optimizer: 120.89 | batch generator: 1.21 | data loader: 0.27
Tencent01: [2020-11-11 19:25:05,284] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 19:25:05,284] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-11 19:25:05,284] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 19:25:05,284] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-11 19:25:05,284] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 19:25:05,284] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 19:25:05,284] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-11 19:25:05,285] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-11 19:25:05,285] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 19:25:05,285] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-11 19:25:05,285] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 19:25:05,285] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-11 19:25:05,285] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 19:25:05,285] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 19:25:05,285] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 19:25:05,285] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-11 19:25:05,285] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 19:25:05,285] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-11 19:25:05,285] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-11 19:25:05,285] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-11 19:25:05,285] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 19:25:05,285] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-11 19:25:05,285] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 19:25:05,285] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-11 19:25:05,285] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 19:25:05,285] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 19:25:05,285] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 19:25:05,285] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 19:25:05,285] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-11 19:25:05,285] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-11 19:25:05,285] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-11 19:25:05,285] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-11 19:26:22,139] [INFO] [logging.py:60:log_dist] [Rank 0] step=16900, skipped=30, lr=[0.00011370816273905052, 0.00011370816273905052], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 19:26:22,190] [INFO] [timer.py:157:stop] 0/16900, SamplesPerSec=48.25045016359487
Tencent01:  iteration    16900/   50000 | elapsed time per iteration (ms): 2647.5 | learning rate 1.137E-04 | lm loss 2.857526E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 576.50 | backward: 1949.69 | optimizer: 121.08 | batch generator: 1.21 | data loader: 0.27
Tencent01: [2020-11-11 19:30:46,948] [INFO] [logging.py:60:log_dist] [Rank 0] step=17000, skipped=30, lr=[0.00011330377436882748, 0.00011330377436882748], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 19:30:47,001] [INFO] [timer.py:157:stop] 0/17000, SamplesPerSec=48.25112335287736
Tencent01:  iteration    17000/   50000 | elapsed time per iteration (ms): 2648.1 | learning rate 1.133E-04 | lm loss 2.851276E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 576.81 | backward: 1950.25 | optimizer: 120.76 | batch generator: 1.25 | data loader: 0.27
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: -----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 17000 | LM loss: 2.862891E+00 | LM PPL: 1.751209E+01
Tencent01: -----------------------------------------------------------------------------------
Tencent01: [2020-11-11 19:35:29,604] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 17084
Tencent01: [2020-11-11 19:35:29,604] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 17084
Tencent01: [2020-11-11 19:35:29,604] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 17084
Tencent02: [2020-11-11 19:35:29,604] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 17084
Tencent01: [2020-11-11 19:35:29,604] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-11 19:35:29,604] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-11 19:35:29,604] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-11 19:35:29,604] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-11 19:35:29,604] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Tencent02: [2020-11-11 19:35:29,604] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-11 19:35:29,604] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 17084
Tencent01: Grad overflow on iteration 17084
Tencent02: [2020-11-11 19:35:29,604] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-11 19:35:29,604] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 17084
Tencent01: Grad overflow on iteration 17084
Tencent02: [2020-11-11 19:35:29,604] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-11 19:35:29,604] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 17084
Tencent01: [2020-11-11 19:35:29,604] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 17084
Tencent02: [2020-11-11 19:35:29,604] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-11 19:35:29,604] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-11 19:35:29,604] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-11 19:35:29,604] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-11 19:35:29,604] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 17084
Tencent01: Grad overflow on iteration 17084
Tencent02: [2020-11-11 19:35:29,604] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-11 19:35:29,604] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: Grad overflow on iteration 17084
Tencent01: [2020-11-11 19:35:29,604] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-11 19:35:29,604] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-11 19:35:29,604] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-11 19:35:29,604] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-11 19:35:29,604] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 17084
Tencent02: [2020-11-11 19:35:29,604] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-11 19:35:29,604] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 17084
Tencent02: [2020-11-11 19:35:29,604] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 17084
Tencent02: [2020-11-11 19:35:29,605] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-11 19:35:29,605] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-11 19:36:09,300] [INFO] [logging.py:60:log_dist] [Rank 0] step=17100, skipped=31, lr=[0.00011290194026921767, 0.00011290194026921767], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 19:36:09,352] [INFO] [timer.py:157:stop] 0/17100, SamplesPerSec=48.24796185624559
Tencent01:  iteration    17100/   50000 | elapsed time per iteration (ms): 3223.5 | learning rate 1.129E-04 | lm loss 2.842323E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.68 | backward: 1987.88 | optimizer: 119.22 | batch generator: 2.23 | data loader: 0.40
Tencent01: [2020-11-11 19:40:33,990] [INFO] [logging.py:60:log_dist] [Rank 0] step=17200, skipped=31, lr=[0.00011249455847031894, 0.00011249455847031894], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 19:40:34,043] [INFO] [timer.py:157:stop] 0/17200, SamplesPerSec=48.248763955573914
Tencent01:  iteration    17200/   50000 | elapsed time per iteration (ms): 2646.9 | learning rate 1.125E-04 | lm loss 2.837311E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.79 | backward: 1949.02 | optimizer: 120.84 | batch generator: 1.23 | data loader: 0.27
Tencent01: [2020-11-11 19:44:58,775] [INFO] [logging.py:60:log_dist] [Rank 0] step=17300, skipped=31, lr=[0.00011208569645045275, 0.00011208569645045275], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 19:44:58,828] [INFO] [timer.py:157:stop] 0/17300, SamplesPerSec=48.24945196425228
Tencent01:  iteration    17300/   50000 | elapsed time per iteration (ms): 2647.9 | learning rate 1.121E-04 | lm loss 2.830714E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.85 | backward: 1950.49 | optimizer: 120.28 | batch generator: 1.21 | data loader: 0.26
Tencent01: [2020-11-11 19:49:23,740] [INFO] [logging.py:60:log_dist] [Rank 0] step=17400, skipped=31, lr=[0.00011167537035079158, 0.00011167537035079158], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 19:49:23,792] [INFO] [timer.py:157:stop] 0/17400, SamplesPerSec=48.24994503290435
Tencent01:  iteration    17400/   50000 | elapsed time per iteration (ms): 2649.6 | learning rate 1.117E-04 | lm loss 2.872845E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.70 | backward: 1952.45 | optimizer: 120.26 | batch generator: 1.20 | data loader: 0.26
Tencent01: [2020-11-11 19:53:49,415] [INFO] [logging.py:60:log_dist] [Rank 0] step=17500, skipped=31, lr=[0.00011126359637030729, 0.00011126359637030729], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 19:53:49,467] [INFO] [timer.py:157:stop] 0/17500, SamplesPerSec=48.24969324783242
Tencent01:  iteration    17500/   50000 | elapsed time per iteration (ms): 2656.8 | learning rate 1.113E-04 | lm loss 2.814763E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.79 | backward: 1959.42 | optimizer: 120.29 | batch generator: 1.19 | data loader: 0.27
Tencent01: [2020-11-11 19:58:15,480] [INFO] [logging.py:60:log_dist] [Rank 0] step=17600, skipped=31, lr=[0.00011085039076513146, 0.00011085039076513146], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 19:58:15,532] [INFO] [timer.py:157:stop] 0/17600, SamplesPerSec=48.24904600558306
Tencent01:  iteration    17600/   50000 | elapsed time per iteration (ms): 2660.6 | learning rate 1.108E-04 | lm loss 2.832933E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.76 | backward: 1962.88 | optimizer: 120.74 | batch generator: 1.22 | data loader: 0.27
Tencent01: [2020-11-11 20:02:40,182] [INFO] [logging.py:60:log_dist] [Rank 0] step=17700, skipped=31, lr=[0.00011043576984791397, 0.00011043576984791397], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 20:02:40,234] [INFO] [timer.py:157:stop] 0/17700, SamplesPerSec=48.24980221198476
Tencent01:  iteration    17700/   50000 | elapsed time per iteration (ms): 2647.0 | learning rate 1.104E-04 | lm loss 2.862484E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.69 | backward: 1949.77 | optimizer: 120.31 | batch generator: 1.18 | data loader: 0.27
Tencent01: [2020-11-11 20:07:04,994] [INFO] [logging.py:60:log_dist] [Rank 0] step=17800, skipped=31, lr=[0.00011001974998717862, 0.00011001974998717862], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 20:07:05,047] [INFO] [timer.py:157:stop] 0/17800, SamplesPerSec=48.250440774477234
Tencent01:  iteration    17800/   50000 | elapsed time per iteration (ms): 2648.1 | learning rate 1.100E-04 | lm loss 2.869254E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.81 | backward: 1950.59 | optimizer: 120.44 | batch generator: 1.18 | data loader: 0.26
Tencent01: [2020-11-11 20:11:30,667] [INFO] [logging.py:60:log_dist] [Rank 0] step=17900, skipped=31, lr=[0.0001096023476066772, 0.0001096023476066772], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 20:11:30,719] [INFO] [timer.py:157:stop] 0/17900, SamplesPerSec=48.25019704465077
Tencent01:  iteration    17900/   50000 | elapsed time per iteration (ms): 2656.7 | learning rate 1.096E-04 | lm loss 2.831346E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.74 | backward: 1959.30 | optimizer: 120.40 | batch generator: 1.17 | data loader: 0.27
Tencent01: [2020-11-11 20:15:56,745] [INFO] [logging.py:60:log_dist] [Rank 0] step=18000, skipped=31, lr=[0.00010918357918474098, 0.00010918357918474098], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 20:15:56,798] [INFO] [timer.py:157:stop] 0/18000, SamplesPerSec=48.2495442996061
Tencent01:  iteration    18000/   50000 | elapsed time per iteration (ms): 2660.8 | learning rate 1.092E-04 | lm loss 2.872754E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 576.62 | backward: 1962.93 | optimizer: 120.97 | batch generator: 1.18 | data loader: 0.27
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: -----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 18000 | LM loss: 2.832039E+00 | LM PPL: 1.698004E+01
Tencent01: -----------------------------------------------------------------------------------
Tencent01: [2020-11-11 20:20:39,225] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 20:20:39,226] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-11 20:20:39,226] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 20:20:39,226] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-11 20:20:39,227] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 20:20:39,227] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-11 20:20:39,227] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 20:20:39,227] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 20:20:39,227] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 20:20:39,227] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 20:20:39,227] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 20:20:39,227] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-11 20:20:39,227] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-11 20:20:39,227] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-11 20:20:39,227] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 20:20:39,227] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-11 20:20:39,227] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-11 20:20:39,227] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 20:20:39,227] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-11 20:20:39,227] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 20:20:39,227] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-11 20:20:39,227] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-11 20:20:39,227] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 20:20:39,227] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 20:20:39,227] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-11 20:20:39,227] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 20:20:39,227] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-11 20:20:39,227] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-11 20:20:39,227] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 20:20:39,227] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-11 20:20:39,227] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 20:20:39,227] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-11 20:21:16,299] [INFO] [logging.py:60:log_dist] [Rank 0] step=18100, skipped=31, lr=[0.00010876346125363021, 0.00010876346125363021], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 20:21:16,351] [INFO] [timer.py:157:stop] 0/18100, SamplesPerSec=48.24939019564604
Tencent01:  iteration    18100/   50000 | elapsed time per iteration (ms): 3195.5 | learning rate 1.088E-04 | lm loss 2.840496E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 576.15 | backward: 1959.25 | optimizer: 120.20 | batch generator: 2.09 | data loader: 0.38
Tencent01: [2020-11-11 20:25:41,059] [INFO] [logging.py:60:log_dist] [Rank 0] step=18200, skipped=31, lr=[0.00010834201039888145, 0.00010834201039888145], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 20:25:41,110] [INFO] [timer.py:157:stop] 0/18200, SamplesPerSec=48.25006515183827
Tencent01:  iteration    18200/   50000 | elapsed time per iteration (ms): 2647.6 | learning rate 1.083E-04 | lm loss 2.839458E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 576.44 | backward: 1950.29 | optimizer: 120.63 | batch generator: 1.19 | data loader: 0.27
Tencent01: [2020-11-11 20:30:05,843] [INFO] [logging.py:60:log_dist] [Rank 0] step=18300, skipped=31, lr=[0.00010791924325865281, 0.00010791924325865281], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 20:30:05,895] [INFO] [timer.py:157:stop] 0/18300, SamplesPerSec=48.250722509029096
Tencent01:  iteration    18300/   50000 | elapsed time per iteration (ms): 2647.8 | learning rate 1.079E-04 | lm loss 2.818217E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 576.68 | backward: 1949.58 | optimizer: 121.28 | batch generator: 1.26 | data loader: 0.29
Tencent01: [2020-11-11 20:34:33,133] [INFO] [logging.py:60:log_dist] [Rank 0] step=18400, skipped=31, lr=[0.00010749517652306708, 0.00010749517652306708], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 20:34:33,185] [INFO] [timer.py:157:stop] 0/18400, SamplesPerSec=48.2488965412931
Tencent01:  iteration    18400/   50000 | elapsed time per iteration (ms): 2672.9 | learning rate 1.075E-04 | lm loss 2.804749E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 576.59 | backward: 1974.79 | optimizer: 121.22 | batch generator: 1.27 | data loader: 0.29
Tencent01: [2020-11-11 20:38:57,932] [INFO] [logging.py:60:log_dist] [Rank 0] step=18500, skipped=31, lr=[0.00010706982693355288, 0.00010706982693355288], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 20:38:57,984] [INFO] [timer.py:157:stop] 0/18500, SamplesPerSec=48.24952660326473
Tencent01:  iteration    18500/   50000 | elapsed time per iteration (ms): 2648.0 | learning rate 1.071E-04 | lm loss 2.844504E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 576.72 | backward: 1950.47 | optimizer: 120.54 | batch generator: 1.20 | data loader: 0.27
Tencent01: [2020-11-11 20:43:22,767] [INFO] [logging.py:60:log_dist] [Rank 0] step=18600, skipped=31, lr=[0.00010664321128218369, 0.00010664321128218369], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 20:43:22,820] [INFO] [timer.py:157:stop] 0/18600, SamplesPerSec=48.25011185889162
Tencent01:  iteration    18600/   50000 | elapsed time per iteration (ms): 2648.4 | learning rate 1.066E-04 | lm loss 2.832413E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 576.97 | backward: 1950.93 | optimizer: 120.22 | batch generator: 1.20 | data loader: 0.26
Tencent01: [2020-11-11 20:47:47,589] [INFO] [logging.py:60:log_dist] [Rank 0] step=18700, skipped=31, lr=[0.00010621534641101493, 0.00010621534641101493], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 20:47:47,642] [INFO] [timer.py:157:stop] 0/18700, SamplesPerSec=48.250700357262374
Tencent01:  iteration    18700/   50000 | elapsed time per iteration (ms): 2648.2 | learning rate 1.062E-04 | lm loss 2.822045E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 576.48 | backward: 1951.26 | optimizer: 120.25 | batch generator: 1.16 | data loader: 0.26
Tencent01: [2020-11-11 20:52:15,004] [INFO] [logging.py:60:log_dist] [Rank 0] step=18800, skipped=31, lr=[0.0001057862492114191, 0.0001057862492114191], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 20:52:15,051] [INFO] [timer.py:157:stop] 0/18800, SamplesPerSec=48.248800527395076
Tencent01:  iteration    18800/   50000 | elapsed time per iteration (ms): 2674.1 | learning rate 1.058E-04 | lm loss 2.853425E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 576.73 | backward: 1974.53 | optimizer: 122.53 | batch generator: 1.31 | data loader: 0.29
Tencent01: [2020-11-11 20:56:39,772] [INFO] [logging.py:60:log_dist] [Rank 0] step=18900, skipped=31, lr=[0.0001053559366234189, 0.0001053559366234189], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 20:56:39,824] [INFO] [timer.py:157:stop] 0/18900, SamplesPerSec=48.249462007685146
Tencent01:  iteration    18900/   50000 | elapsed time per iteration (ms): 2647.7 | learning rate 1.054E-04 | lm loss 2.813860E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 576.76 | backward: 1947.42 | optimizer: 123.23 | batch generator: 1.33 | data loader: 0.30
Tencent01: [2020-11-11 21:01:04,537] [INFO] [logging.py:60:log_dist] [Rank 0] step=19000, skipped=31, lr=[0.0001049244256350185, 0.0001049244256350185], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 21:01:04,589] [INFO] [timer.py:157:stop] 0/19000, SamplesPerSec=48.25011165111137
Tencent01:  iteration    19000/   50000 | elapsed time per iteration (ms): 2647.6 | learning rate 1.049E-04 | lm loss 2.826256E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 576.54 | backward: 1949.70 | optimizer: 121.13 | batch generator: 1.24 | data loader: 0.29
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: -----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 19000 | LM loss: 2.837269E+00 | LM PPL: 1.706908E+01
Tencent01: -----------------------------------------------------------------------------------
Tencent01: [2020-11-11 21:05:47,208] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 21:05:47,208] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-11 21:05:47,208] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 21:05:47,208] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 21:05:47,208] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-11 21:05:47,208] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-11 21:05:47,208] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 21:05:47,208] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-11 21:05:47,208] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 21:05:47,209] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-11 21:05:47,209] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 21:05:47,209] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 21:05:47,209] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-11 21:05:47,209] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-11 21:05:47,209] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 21:05:47,209] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 21:05:47,209] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-11 21:05:47,209] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-11 21:05:47,209] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 21:05:47,209] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-11 21:05:47,209] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 21:05:47,209] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 21:05:47,209] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-11 21:05:47,209] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-11 21:05:47,209] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-11 21:05:47,209] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-11 21:05:47,209] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 21:05:47,210] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-11 21:05:47,210] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 21:05:47,211] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-11 21:05:47,211] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-11 21:05:47,211] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-11 21:06:24,334] [INFO] [logging.py:60:log_dist] [Rank 0] step=19100, skipped=31, lr=[0.00010449173328153285, 0.00010449173328153285], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 21:06:24,385] [INFO] [timer.py:157:stop] 0/19100, SamplesPerSec=48.24978286934913
Tencent01:  iteration    19100/   50000 | elapsed time per iteration (ms): 3198.0 | learning rate 1.045E-04 | lm loss 2.821344E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 576.68 | backward: 1959.48 | optimizer: 121.46 | batch generator: 2.29 | data loader: 0.42
Tencent01: [2020-11-11 21:07:33,139] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 19125
Tencent01: [2020-11-11 21:07:33,139] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 19125
Tencent01: [2020-11-11 21:07:33,139] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-11 21:07:33,139] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-11 21:07:33,139] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 19125
Tencent01: [2020-11-11 21:07:33,139] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 19125
Tencent01: [2020-11-11 21:07:33,140] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-11 21:07:33,140] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-11 21:07:33,140] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 19125
Tencent02: [2020-11-11 21:07:33,140] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 19125
Tencent01: [2020-11-11 21:07:33,140] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-11 21:07:33,140] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-11 21:07:33,140] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 19125
Tencent02: [2020-11-11 21:07:33,140] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 19125
Tencent01: [2020-11-11 21:07:33,140] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Tencent02: [2020-11-11 21:07:33,140] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-11 21:07:33,140] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: [2020-11-11 21:07:33,140] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 19125
Tencent02: Grad overflow on iteration 19125
Tencent02: [2020-11-11 21:07:33,140] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 19125
Tencent01: [2020-11-11 21:07:33,140] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-11 21:07:33,140] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-11 21:07:33,140] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: Grad overflow on iteration 19125
Tencent01: [2020-11-11 21:07:33,140] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 19125
Tencent01: [2020-11-11 21:07:33,140] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-11 21:07:33,140] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-11 21:07:33,140] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 19125
Tencent02: [2020-11-11 21:07:33,140] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 19125
Tencent02: [2020-11-11 21:07:33,140] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-11 21:07:33,140] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-11 21:07:33,140] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-11 21:07:33,140] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 19125
Tencent02: [2020-11-11 21:07:33,140] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-11 21:07:33,140] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-11 21:10:51,416] [INFO] [logging.py:60:log_dist] [Rank 0] step=19200, skipped=32, lr=[0.00010406222091828113, 0.00010406222091828113], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-11 21:10:51,468] [INFO] [timer.py:157:stop] 0/19200, SamplesPerSec=48.24822765841954
Tencent01:  iteration    19200/   50000 | elapsed time per iteration (ms): 2670.8 | learning rate 1.041E-04 | lm loss 2.811620E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 576.73 | backward: 1973.84 | optimizer: 119.98 | batch generator: 1.24 | data loader: 0.28
Tencent01: 
Tencent01: VM-0-14-centos:109:174 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent01: 
Tencent01: VM-0-14-centos:113:185 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent01: 
Tencent01: VM-0-14-centos:108:172 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent01: 
Tencent01: VM-0-14-centos:106:182 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent01: 
Tencent01: VM-0-14-centos:112:176 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent01: 
Tencent01: VM-0-14-centos:110:180 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent01: 
Tencent01: VM-0-14-centos:107:178 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent01: 
Tencent01: VM-0-14-centos:111:200 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent01: 
Tencent01: VM-0-14-centos:109:174 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent01: 
Tencent01: VM-0-14-centos:108:172 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent01: 
Tencent01: VM-0-14-centos:112:176 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent01: 
Tencent01: VM-0-14-centos:107:178 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent01: 
Tencent01: VM-0-14-centos:110:180 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent01: 
Tencent01: VM-0-14-centos:113:185 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent01: 
Tencent01: VM-0-14-centos:106:182 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent01: 
Tencent01: VM-0-14-centos:111:200 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent01: 
Tencent01: VM-0-14-centos:108:172 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent01: 
Tencent01: VM-0-14-centos:107:178 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent01: 
Tencent01: VM-0-14-centos:109:174 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent01: 
Tencent01: VM-0-14-centos:113:185 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent01: 
Tencent01: VM-0-14-centos:106:182 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent01: 
Tencent01: VM-0-14-centos:110:180 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent01: 
Tencent01: VM-0-14-centos:111:200 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent01: 
Tencent01: VM-0-14-centos:112:176 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent01: 
Tencent01: VM-0-14-centos:108:172 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent01: 
Tencent01: VM-0-14-centos:112:176 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent01: 
Tencent01: VM-0-14-centos:113:185 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent01: 
Tencent01: VM-0-14-centos:109:174 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent01: 
Tencent01: VM-0-14-centos:111:200 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent01: 
Tencent01: VM-0-14-centos:106:182 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent01: 
Tencent01: VM-0-14-centos:107:178 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent01: 
Tencent01: VM-0-14-centos:110:180 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent01: 
Tencent01: VM-0-14-centos:110:595 [0] transport/net_ib.cc:816 NCCL WARN NET/IB : Got completion with error 12, opcode 0, len 32534, vendor err 129
Tencent01: VM-0-14-centos:110:595 [0] NCCL INFO include/net.h:28 -> 2
Tencent01: VM-0-14-centos:110:595 [0] NCCL INFO transport/net.cc:345 -> 2
Tencent01: VM-0-14-centos:110:595 [0] NCCL INFO transport.cc:179 -> 2 [Proxy Thread]
Tencent02: 
Tencent02: VM-0-9-centos:79:560 [0] transport/net_ib.cc:816 NCCL WARN NET/IB : Got completion with error 12, opcode 0, len 32504, vendor err 129
Tencent02: VM-0-9-centos:79:560 [0] NCCL INFO include/net.h:28 -> 2
Tencent02: VM-0-9-centos:79:560 [0] NCCL INFO transport/net.cc:345 -> 2
Tencent02: VM-0-9-centos:79:560 [0] NCCL INFO transport.cc:179 -> 2 [Proxy Thread]
Tencent02: 
Tencent02: VM-0-9-centos:80:562 [0] transport/net_ib.cc:816 NCCL WARN NET/IB : Got completion with error 12, opcode 0, len 32613, vendor err 129
Tencent02: VM-0-9-centos:80:562 [0] NCCL INFO include/net.h:28 -> 2
Tencent02: VM-0-9-centos:80:562 [0] NCCL INFO transport/net.cc:345 -> 2
Tencent02: VM-0-9-centos:80:562 [0] NCCL INFO transport.cc:179 -> 2 [Proxy Thread]
Tencent01: 
Tencent01: VM-0-14-centos:111:596 [0] transport/net_ib.cc:816 NCCL WARN NET/IB : Got completion with error 12, opcode 0, len 32519, vendor err 129
Tencent01: VM-0-14-centos:111:596 [0] NCCL INFO include/net.h:28 -> 2
Tencent01: VM-0-14-centos:111:596 [0] NCCL INFO transport/net.cc:345 -> 2
Tencent01: VM-0-14-centos:111:596 [0] NCCL INFO transport.cc:179 -> 2 [Proxy Thread]
Tencent02: 
Tencent02: VM-0-9-centos:81:145 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent02: 
Tencent02: VM-0-9-centos:76:141 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent02: 
Tencent02: VM-0-9-centos:79:149 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent02: 
Tencent02: VM-0-9-centos:75:147 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent02: 
Tencent02: VM-0-9-centos:82:143 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent02: 
Tencent02: VM-0-9-centos:80:167 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent02: 
Tencent02: VM-0-9-centos:78:138 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent02: 
Tencent02: VM-0-9-centos:77:163 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent02: 
Tencent02: VM-0-9-centos:76:141 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent02: 
Tencent02: VM-0-9-centos:81:145 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent02: 
Tencent02: VM-0-9-centos:82:143 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent02: 
Tencent02: VM-0-9-centos:75:147 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent02: 
Tencent02: VM-0-9-centos:78:138 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent02: 
Tencent02: VM-0-9-centos:80:167 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent02: 
Tencent02: VM-0-9-centos:79:149 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent02: 
Tencent02: VM-0-9-centos:77:163 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent02: 
Tencent02: VM-0-9-centos:76:141 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent02: 
Tencent02: VM-0-9-centos:75:147 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent02: 
Tencent02: VM-0-9-centos:82:143 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent02: 
Tencent02: VM-0-9-centos:78:138 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent02: 
Tencent02: VM-0-9-centos:81:145 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent02: 
Tencent02: VM-0-9-centos:79:149 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent02: 
Tencent02: VM-0-9-centos:77:163 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent02: 
Tencent02: VM-0-9-centos:80:167 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent02: 
Tencent02: VM-0-9-centos:76:141 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent02: 
Tencent02: VM-0-9-centos:82:143 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent02: 
Tencent02: VM-0-9-centos:75:147 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent02: 
Tencent02: VM-0-9-centos:78:138 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent02: 
Tencent02: VM-0-9-centos:81:145 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent02: 
Tencent02: VM-0-9-centos:79:149 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent02: 
Tencent02: VM-0-9-centos:77:163 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
Tencent02: 
Tencent02: VM-0-9-centos:80:167 [0] transport/net_ib.cc:73 NCCL WARN NET/IB : Got async event : GID table change
