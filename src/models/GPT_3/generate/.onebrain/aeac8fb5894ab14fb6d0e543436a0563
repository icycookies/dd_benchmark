file:evaluate_gpt2.py
file:.DS_Store
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:mpu/tests/test_cross_entropy.py
file:mpu/tests/test_layers.py
file:mpu/tests/__init__.py
file:mpu/tests/commons.py
file:mpu/tests/test_data.py
file:mpu/tests/test_initialize.py
file:mpu/tests/test_random.py
file:mpu/__pycache__/transformer.cpython-37.pyc
file:mpu/__pycache__/transformer.cpython-36.pyc
file:mpu/__pycache__/cross_entropy.cpython-36.pyc
file:mpu/__pycache__/cross_entropy.cpython-37.pyc
file:mpu/__pycache__/grads.cpython-37.pyc
file:mpu/__pycache__/initialize.cpython-37.pyc
file:mpu/__pycache__/initialize.cpython-36.pyc
file:mpu/__pycache__/grads.cpython-36.pyc
file:mpu/__pycache__/data.cpython-37.pyc
file:mpu/__pycache__/data.cpython-36.pyc
file:mpu/__pycache__/mappings.cpython-36.pyc
file:mpu/__pycache__/utils.cpython-36.pyc
file:mpu/__pycache__/layers.cpython-37.pyc
file:mpu/__pycache__/layers.cpython-36.pyc
file:mpu/__pycache__/mappings.cpython-37.pyc
file:mpu/__pycache__/utils.cpython-37.pyc
file:mpu/__pycache__/random.cpython-37.pyc
file:mpu/__pycache__/random.cpython-36.pyc
file:mpu/__pycache__/__init__.cpython-36.pyc
file:mpu/__pycache__/__init__.cpython-37.pyc
file:openwebtext/remove_group_duplicates.py
file:openwebtext/run_make_gpt2_dataset.sh
file:openwebtext/group_duplicates_url.py
file:openwebtext/make_gpt2_sizes.py
file:openwebtext/README.md
file:openwebtext/tokenizer.py
file:openwebtext/make_gpt2_dataset.py
file:openwebtext/merge_jsons.py
file:openwebtext/blacklist_urls.py
file:openwebtext/find_duplicates.py
file:openwebtext/cleanup_dataset.py
file:.pytorch_pretrained_bert/gpt2-merges.txt
file:.pytorch_pretrained_bert/gpt2-vocab.json
file:data_utils/tokenization_gpt2.py
file:data_utils/lazy_loader.py
file:data_utils/wordpiece.py
file:data_utils/datasets.py
file:data_utils/__init__.py
file:data_utils/tokenization.py
file:data_utils/tf_dl.py
file:data_utils/file_utils.py
file:data_utils/corpora.py
file:data_utils/samplers.py
file:data_utils/__pycache__/datasets.cpython-36.pyc
file:data_utils/__pycache__/datasets.cpython-37.pyc
file:data_utils/__pycache__/wordpiece.cpython-37.pyc
file:data_utils/__pycache__/wordpiece.cpython-36.pyc
file:data_utils/__pycache__/samplers.cpython-36.pyc
file:data_utils/__pycache__/samplers.cpython-37.pyc
file:data_utils/__pycache__/tokenization.cpython-36.pyc
file:data_utils/__pycache__/lazy_loader.cpython-37.pyc
file:data_utils/__pycache__/lazy_loader.cpython-36.pyc
file:data_utils/__pycache__/tokenization.cpython-37.pyc
file:data_utils/__pycache__/file_utils.cpython-36.pyc
file:data_utils/__pycache__/file_utils.cpython-37.pyc
file:data_utils/__pycache__/__init__.cpython-36.pyc
file:data_utils/__pycache__/corpora.cpython-37.pyc
file:data_utils/__pycache__/tokenization_gpt2.cpython-37.pyc
file:data_utils/__pycache__/tokenization_gpt2.cpython-36.pyc
file:data_utils/__pycache__/corpora.cpython-36.pyc
file:data_utils/__pycache__/__init__.cpython-37.pyc
file:__pycache__/gpt2_data_loader.cpython-37.pyc
file:__pycache__/gpt2_data_loader.cpython-36.pyc
file:__pycache__/learning_rates.cpython-37.pyc
file:__pycache__/learning_rates.cpython-36.pyc
file:__pycache__/configure_data.cpython-37.pyc
file:__pycache__/configure_data.cpython-36.pyc
file:__pycache__/utils.cpython-36.pyc
file:__pycache__/utils.cpython-37.pyc
file:__pycache__/arguments.cpython-37.pyc
file:__pycache__/arguments.cpython-36.pyc
file:.onebrain/.onebrain.ini
file:scripts/pretrain_bert_sentencepiece.sh
file:scripts/pretrain_bert_distributed.sh
file:scripts/pretrain_bert.sh
file:scripts/ds_checkpoint_check.sh
file:scripts/pretrain_gpt2_distributed.sh
file:scripts/ds_zero2_pretrain_gpt2_model_parallel.sh
file:scripts/ds_zero2_config.json
file:scripts/generate_text.sh
file:scripts/ds_zero-offload_config.json
file:scripts/ds_zero-offload_10B_config.json
file:scripts/ds_onebit_config.json
file:scripts/pretrain_gpt2_model_parallel.sh
file:scripts/ds_config.json
file:scripts/ds_zero-offload_pretrain_gpt2_model_parallel.sh
file:scripts/presplit_sentences_json.py
file:scripts/ds_zero-offload_10B_pretrain_gpt2_model_parallel.sh
file:scripts/split_json.py
file:scripts/ds_pretrain_gpt2.sh
file:scripts/pretrain_bert_model_parallel.sh
file:scripts/pretrain_bert_tfrecords_distributed.sh
file:scripts/run_gpt2_eval.py
file:scripts/pretrain_gpt2.sh
file:model/__init__.py
file:model/model.py
file:model/distributed.py
file:model/gpt2_modeling.py
file:model/modeling.py
file:model/__pycache__/model.cpython-37.pyc
file:model/__pycache__/gpt2_modeling.cpython-37.pyc
file:model/__pycache__/gpt2_modeling.cpython-36.pyc
file:model/__pycache__/model.cpython-36.pyc
file:model/__pycache__/modeling.cpython-37.pyc
file:model/__pycache__/modeling.cpython-36.pyc
file:model/__pycache__/distributed.cpython-37.pyc
file:model/__pycache__/distributed.cpython-36.pyc
file:model/__pycache__/__init__.cpython-36.pyc
file:model/__pycache__/__init__.cpython-37.pyc
file:fp16/fp16.py
file:fp16/__init__.py
file:fp16/fp16util.py
file:fp16/loss_scaler.py
file:fp16/__pycache__/fp16util.cpython-36.pyc
file:fp16/__pycache__/fp16util.cpython-37.pyc
file:fp16/__pycache__/loss_scaler.cpython-36.pyc
file:fp16/__pycache__/loss_scaler.cpython-37.pyc
file:fp16/__pycache__/fp16.cpython-37.pyc
file:fp16/__pycache__/fp16.cpython-36.pyc
file:fp16/__pycache__/__init__.cpython-36.pyc
file:fp16/__pycache__/__init__.cpython-37.pyc
file:evaluate_gpt2.py
file:.DS_Store
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:mpu/tests/test_cross_entropy.py
file:mpu/tests/test_layers.py
file:mpu/tests/__init__.py
file:mpu/tests/commons.py
file:mpu/tests/test_data.py
file:mpu/tests/test_initialize.py
file:mpu/tests/test_random.py
file:mpu/__pycache__/transformer.cpython-37.pyc
file:mpu/__pycache__/transformer.cpython-36.pyc
file:mpu/__pycache__/cross_entropy.cpython-36.pyc
file:mpu/__pycache__/cross_entropy.cpython-37.pyc
file:mpu/__pycache__/grads.cpython-37.pyc
file:mpu/__pycache__/initialize.cpython-37.pyc
file:mpu/__pycache__/initialize.cpython-36.pyc
file:mpu/__pycache__/grads.cpython-36.pyc
file:mpu/__pycache__/data.cpython-37.pyc
file:mpu/__pycache__/data.cpython-36.pyc
file:mpu/__pycache__/mappings.cpython-36.pyc
file:mpu/__pycache__/utils.cpython-36.pyc
file:mpu/__pycache__/layers.cpython-37.pyc
file:mpu/__pycache__/layers.cpython-36.pyc
file:mpu/__pycache__/mappings.cpython-37.pyc
file:mpu/__pycache__/utils.cpython-37.pyc
file:mpu/__pycache__/random.cpython-37.pyc
file:mpu/__pycache__/random.cpython-36.pyc
file:mpu/__pycache__/__init__.cpython-36.pyc
file:mpu/__pycache__/__init__.cpython-37.pyc
file:openwebtext/remove_group_duplicates.py
file:openwebtext/run_make_gpt2_dataset.sh
file:openwebtext/group_duplicates_url.py
file:openwebtext/make_gpt2_sizes.py
file:openwebtext/README.md
file:openwebtext/tokenizer.py
file:openwebtext/make_gpt2_dataset.py
file:openwebtext/merge_jsons.py
file:openwebtext/blacklist_urls.py
file:openwebtext/find_duplicates.py
file:openwebtext/cleanup_dataset.py
file:.pytorch_pretrained_bert/gpt2-merges.txt
file:.pytorch_pretrained_bert/gpt2-vocab.json
file:data_utils/tokenization_gpt2.py
file:data_utils/lazy_loader.py
file:data_utils/wordpiece.py
file:data_utils/datasets.py
file:data_utils/__init__.py
file:data_utils/tokenization.py
file:data_utils/tf_dl.py
file:data_utils/file_utils.py
file:data_utils/corpora.py
file:data_utils/samplers.py
file:data_utils/__pycache__/datasets.cpython-36.pyc
file:data_utils/__pycache__/datasets.cpython-37.pyc
file:data_utils/__pycache__/wordpiece.cpython-37.pyc
file:data_utils/__pycache__/wordpiece.cpython-36.pyc
file:data_utils/__pycache__/samplers.cpython-36.pyc
file:data_utils/__pycache__/samplers.cpython-37.pyc
file:data_utils/__pycache__/tokenization.cpython-36.pyc
file:data_utils/__pycache__/lazy_loader.cpython-37.pyc
file:data_utils/__pycache__/lazy_loader.cpython-36.pyc
file:data_utils/__pycache__/tokenization.cpython-37.pyc
file:data_utils/__pycache__/file_utils.cpython-36.pyc
file:data_utils/__pycache__/file_utils.cpython-37.pyc
file:data_utils/__pycache__/__init__.cpython-36.pyc
file:data_utils/__pycache__/corpora.cpython-37.pyc
file:data_utils/__pycache__/tokenization_gpt2.cpython-37.pyc
file:data_utils/__pycache__/tokenization_gpt2.cpython-36.pyc
file:data_utils/__pycache__/corpora.cpython-36.pyc
file:data_utils/__pycache__/__init__.cpython-37.pyc
file:__pycache__/gpt2_data_loader.cpython-37.pyc
file:__pycache__/gpt2_data_loader.cpython-36.pyc
file:__pycache__/learning_rates.cpython-37.pyc
file:__pycache__/learning_rates.cpython-36.pyc
file:__pycache__/configure_data.cpython-37.pyc
file:__pycache__/configure_data.cpython-36.pyc
file:__pycache__/utils.cpython-36.pyc
file:__pycache__/utils.cpython-37.pyc
file:__pycache__/arguments.cpython-37.pyc
file:__pycache__/arguments.cpython-36.pyc
file:.onebrain/aeac8fb5894ab14fb6d0e543436a0563
file:.onebrain/.onebrain.ini
file:scripts/pretrain_bert_sentencepiece.sh
file:scripts/pretrain_bert_distributed.sh
file:scripts/pretrain_bert.sh
file:scripts/ds_checkpoint_check.sh
file:scripts/pretrain_gpt2_distributed.sh
file:scripts/ds_zero2_pretrain_gpt2_model_parallel.sh
file:scripts/ds_zero2_config.json
file:scripts/generate_text.sh
file:scripts/ds_zero-offload_config.json
file:scripts/ds_zero-offload_10B_config.json
file:scripts/ds_onebit_config.json
file:scripts/pretrain_gpt2_model_parallel.sh
file:scripts/ds_config.json
file:scripts/ds_zero-offload_pretrain_gpt2_model_parallel.sh
file:scripts/presplit_sentences_json.py
file:scripts/ds_zero-offload_10B_pretrain_gpt2_model_parallel.sh
file:scripts/split_json.py
file:scripts/ds_pretrain_gpt2.sh
file:scripts/pretrain_bert_model_parallel.sh
file:scripts/pretrain_bert_tfrecords_distributed.sh
file:scripts/run_gpt2_eval.py
file:scripts/pretrain_gpt2.sh
file:model/__init__.py
file:model/model.py
file:model/distributed.py
file:model/gpt2_modeling.py
file:model/modeling.py
file:model/__pycache__/model.cpython-37.pyc
file:model/__pycache__/gpt2_modeling.cpython-37.pyc
file:model/__pycache__/gpt2_modeling.cpython-36.pyc
file:model/__pycache__/model.cpython-36.pyc
file:model/__pycache__/modeling.cpython-37.pyc
file:model/__pycache__/modeling.cpython-36.pyc
file:model/__pycache__/distributed.cpython-37.pyc
file:model/__pycache__/distributed.cpython-36.pyc
file:model/__pycache__/__init__.cpython-36.pyc
file:model/__pycache__/__init__.cpython-37.pyc
file:fp16/fp16.py
file:fp16/__init__.py
file:fp16/fp16util.py
file:fp16/loss_scaler.py
file:fp16/__pycache__/fp16util.cpython-36.pyc
file:fp16/__pycache__/fp16util.cpython-37.pyc
file:fp16/__pycache__/loss_scaler.cpython-36.pyc
file:fp16/__pycache__/loss_scaler.cpython-37.pyc
file:fp16/__pycache__/fp16.cpython-37.pyc
file:fp16/__pycache__/fp16.cpython-36.pyc
file:fp16/__pycache__/__init__.cpython-36.pyc
file:fp16/__pycache__/__init__.cpython-37.pyc
file:evaluate_gpt2.py
file:.DS_Store
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:mpu/tests/test_cross_entropy.py
file:mpu/tests/test_layers.py
file:mpu/tests/__init__.py
file:mpu/tests/commons.py
file:mpu/tests/test_data.py
file:mpu/tests/test_initialize.py
file:mpu/tests/test_random.py
file:openwebtext/remove_group_duplicates.py
file:openwebtext/run_make_gpt2_dataset.sh
file:openwebtext/group_duplicates_url.py
file:openwebtext/make_gpt2_sizes.py
file:openwebtext/README.md
file:openwebtext/tokenizer.py
file:openwebtext/make_gpt2_dataset.py
file:openwebtext/merge_jsons.py
file:openwebtext/blacklist_urls.py
file:openwebtext/find_duplicates.py
file:openwebtext/cleanup_dataset.py
file:.pytorch_pretrained_bert/gpt2-merges.txt
file:.pytorch_pretrained_bert/gpt2-vocab.json
file:data_utils/tokenization_gpt2.py
file:data_utils/lazy_loader.py
file:data_utils/wordpiece.py
file:data_utils/datasets.py
file:data_utils/__init__.py
file:data_utils/tokenization.py
file:data_utils/tf_dl.py
file:data_utils/file_utils.py
file:data_utils/corpora.py
file:data_utils/samplers.py
file:data_utils/__pycache__/datasets.cpython-36.pyc
file:data_utils/__pycache__/datasets.cpython-37.pyc
file:data_utils/__pycache__/wordpiece.cpython-37.pyc
file:data_utils/__pycache__/wordpiece.cpython-36.pyc
file:data_utils/__pycache__/samplers.cpython-36.pyc
file:data_utils/__pycache__/samplers.cpython-37.pyc
file:data_utils/__pycache__/tokenization.cpython-36.pyc
file:data_utils/__pycache__/lazy_loader.cpython-37.pyc
file:data_utils/__pycache__/lazy_loader.cpython-36.pyc
file:data_utils/__pycache__/tokenization.cpython-37.pyc
file:data_utils/__pycache__/file_utils.cpython-36.pyc
file:data_utils/__pycache__/file_utils.cpython-37.pyc
file:data_utils/__pycache__/__init__.cpython-36.pyc
file:data_utils/__pycache__/corpora.cpython-37.pyc
file:data_utils/__pycache__/tokenization_gpt2.cpython-37.pyc
file:data_utils/__pycache__/tokenization_gpt2.cpython-36.pyc
file:data_utils/__pycache__/corpora.cpython-36.pyc
file:data_utils/__pycache__/__init__.cpython-37.pyc
file:.onebrain/aeac8fb5894ab14fb6d0e543436a0563
file:.onebrain/.onebrain.ini
file:scripts/pretrain_bert_sentencepiece.sh
file:scripts/pretrain_bert_distributed.sh
file:scripts/pretrain_bert.sh
file:scripts/ds_checkpoint_check.sh
file:scripts/pretrain_gpt2_distributed.sh
file:scripts/ds_zero2_pretrain_gpt2_model_parallel.sh
file:scripts/ds_zero2_config.json
file:scripts/generate_text.sh
file:scripts/ds_zero-offload_config.json
file:scripts/ds_zero-offload_10B_config.json
file:scripts/ds_onebit_config.json
file:scripts/pretrain_gpt2_model_parallel.sh
file:scripts/ds_config.json
file:scripts/ds_zero-offload_pretrain_gpt2_model_parallel.sh
file:scripts/presplit_sentences_json.py
file:scripts/ds_zero-offload_10B_pretrain_gpt2_model_parallel.sh
file:scripts/split_json.py
file:scripts/ds_pretrain_gpt2.sh
file:scripts/pretrain_bert_model_parallel.sh
file:scripts/pretrain_bert_tfrecords_distributed.sh
file:scripts/run_gpt2_eval.py
file:scripts/pretrain_gpt2.sh
file:model/__init__.py
file:model/model.py
file:model/distributed.py
file:model/gpt2_modeling.py
file:model/modeling.py
file:fp16/fp16.py
file:fp16/__init__.py
file:fp16/fp16util.py
file:fp16/loss_scaler.py
file:fp16/__pycache__/fp16util.cpython-36.pyc
file:fp16/__pycache__/fp16util.cpython-37.pyc
file:fp16/__pycache__/loss_scaler.cpython-36.pyc
file:fp16/__pycache__/loss_scaler.cpython-37.pyc
file:fp16/__pycache__/fp16.cpython-37.pyc
file:fp16/__pycache__/fp16.cpython-36.pyc
file:fp16/__pycache__/__init__.cpython-36.pyc
file:fp16/__pycache__/__init__.cpython-37.pyc
file:evaluate_gpt2.py
file:.DS_Store
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:mpu/tests/test_cross_entropy.py
file:mpu/tests/test_layers.py
file:mpu/tests/__init__.py
file:mpu/tests/commons.py
file:mpu/tests/test_data.py
file:mpu/tests/test_initialize.py
file:mpu/tests/test_random.py
file:openwebtext/remove_group_duplicates.py
file:openwebtext/run_make_gpt2_dataset.sh
file:openwebtext/group_duplicates_url.py
file:openwebtext/make_gpt2_sizes.py
file:openwebtext/README.md
file:openwebtext/tokenizer.py
file:openwebtext/make_gpt2_dataset.py
file:openwebtext/merge_jsons.py
file:openwebtext/blacklist_urls.py
file:openwebtext/find_duplicates.py
file:openwebtext/cleanup_dataset.py
file:.pytorch_pretrained_bert/gpt2-merges.txt
file:.pytorch_pretrained_bert/gpt2-vocab.json
file:data_utils/tokenization_gpt2.py
file:data_utils/lazy_loader.py
file:data_utils/wordpiece.py
file:data_utils/datasets.py
file:data_utils/__init__.py
file:data_utils/tokenization.py
file:data_utils/tf_dl.py
file:data_utils/file_utils.py
file:data_utils/corpora.py
file:data_utils/samplers.py
file:data_utils/__pycache__/datasets.cpython-36.pyc
file:data_utils/__pycache__/datasets.cpython-37.pyc
file:data_utils/__pycache__/wordpiece.cpython-37.pyc
file:data_utils/__pycache__/wordpiece.cpython-36.pyc
file:data_utils/__pycache__/samplers.cpython-36.pyc
file:data_utils/__pycache__/samplers.cpython-37.pyc
file:data_utils/__pycache__/tokenization.cpython-36.pyc
file:data_utils/__pycache__/lazy_loader.cpython-37.pyc
file:data_utils/__pycache__/lazy_loader.cpython-36.pyc
file:data_utils/__pycache__/tokenization.cpython-37.pyc
file:data_utils/__pycache__/file_utils.cpython-36.pyc
file:data_utils/__pycache__/file_utils.cpython-37.pyc
file:data_utils/__pycache__/__init__.cpython-36.pyc
file:data_utils/__pycache__/corpora.cpython-37.pyc
file:data_utils/__pycache__/tokenization_gpt2.cpython-37.pyc
file:data_utils/__pycache__/tokenization_gpt2.cpython-36.pyc
file:data_utils/__pycache__/corpora.cpython-36.pyc
file:data_utils/__pycache__/__init__.cpython-37.pyc
file:.onebrain/aeac8fb5894ab14fb6d0e543436a0563
file:.onebrain/.onebrain.ini
file:scripts/pretrain_bert_sentencepiece.sh
file:scripts/pretrain_bert_distributed.sh
file:scripts/pretrain_bert.sh
file:scripts/ds_checkpoint_check.sh
file:scripts/pretrain_gpt2_distributed.sh
file:scripts/ds_zero2_pretrain_gpt2_model_parallel.sh
file:scripts/ds_zero2_config.json
file:scripts/generate_text.sh
file:scripts/ds_zero-offload_config.json
file:scripts/ds_zero-offload_10B_config.json
file:scripts/ds_onebit_config.json
file:scripts/pretrain_gpt2_model_parallel.sh
file:scripts/ds_config.json
file:scripts/ds_zero-offload_pretrain_gpt2_model_parallel.sh
file:scripts/presplit_sentences_json.py
file:scripts/ds_zero-offload_10B_pretrain_gpt2_model_parallel.sh
file:scripts/split_json.py
file:scripts/ds_pretrain_gpt2.sh
file:scripts/pretrain_bert_model_parallel.sh
file:scripts/pretrain_bert_tfrecords_distributed.sh
file:scripts/run_gpt2_eval.py
file:scripts/pretrain_gpt2.sh
file:model/__init__.py
file:model/model.py
file:model/distributed.py
file:model/gpt2_modeling.py
file:model/modeling.py
file:fp16/fp16.py
file:fp16/__init__.py
file:fp16/fp16util.py
file:fp16/loss_scaler.py
file:fp16/__pycache__/fp16util.cpython-36.pyc
file:fp16/__pycache__/fp16util.cpython-37.pyc
file:fp16/__pycache__/loss_scaler.cpython-36.pyc
file:fp16/__pycache__/loss_scaler.cpython-37.pyc
file:fp16/__pycache__/fp16.cpython-37.pyc
file:fp16/__pycache__/fp16.cpython-36.pyc
file:fp16/__pycache__/__init__.cpython-36.pyc
file:fp16/__pycache__/__init__.cpython-37.pyc
file:evaluate_gpt2.py
file:.DS_Store
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:mpu/tests/test_cross_entropy.py
file:mpu/tests/test_layers.py
file:mpu/tests/__init__.py
file:mpu/tests/commons.py
file:mpu/tests/test_data.py
file:mpu/tests/test_initialize.py
file:mpu/tests/test_random.py
file:openwebtext/remove_group_duplicates.py
file:openwebtext/run_make_gpt2_dataset.sh
file:openwebtext/group_duplicates_url.py
file:openwebtext/make_gpt2_sizes.py
file:openwebtext/README.md
file:openwebtext/tokenizer.py
file:openwebtext/make_gpt2_dataset.py
file:openwebtext/merge_jsons.py
file:openwebtext/blacklist_urls.py
file:openwebtext/find_duplicates.py
file:openwebtext/cleanup_dataset.py
file:.pytorch_pretrained_bert/gpt2-merges.txt
file:.pytorch_pretrained_bert/gpt2-vocab.json
file:data_utils/tokenization_gpt2.py
file:data_utils/lazy_loader.py
file:data_utils/wordpiece.py
file:data_utils/datasets.py
file:data_utils/__init__.py
file:data_utils/tokenization.py
file:data_utils/tf_dl.py
file:data_utils/file_utils.py
file:data_utils/corpora.py
file:data_utils/samplers.py
file:.onebrain/aeac8fb5894ab14fb6d0e543436a0563
file:.onebrain/.onebrain.ini
file:scripts/pretrain_bert_sentencepiece.sh
file:scripts/pretrain_bert_distributed.sh
file:scripts/pretrain_bert.sh
file:scripts/ds_checkpoint_check.sh
file:scripts/pretrain_gpt2_distributed.sh
file:scripts/ds_zero2_pretrain_gpt2_model_parallel.sh
file:scripts/ds_zero2_config.json
file:scripts/generate_text.sh
file:scripts/ds_zero-offload_config.json
file:scripts/ds_zero-offload_10B_config.json
file:scripts/ds_onebit_config.json
file:scripts/pretrain_gpt2_model_parallel.sh
file:scripts/extract_hostfile.py
file:scripts/ds_config.json
file:scripts/ds_zero-offload_pretrain_gpt2_model_parallel.sh
file:scripts/presplit_sentences_json.py
file:scripts/ds_zero-offload_10B_pretrain_gpt2_model_parallel.sh
file:scripts/split_json.py
file:scripts/ds_pretrain_gpt2.sh
file:scripts/pretrain_bert_model_parallel.sh
file:scripts/pretrain_bert_tfrecords_distributed.sh
file:scripts/run_gpt2_eval.py
file:scripts/pretrain_gpt2.sh
file:model/__init__.py
file:model/model.py
file:model/distributed.py
file:model/gpt2_modeling.py
file:model/modeling.py
file:fp16/fp16.py
file:fp16/__init__.py
file:fp16/fp16util.py
file:fp16/loss_scaler.py
file:evaluate_gpt2.py
file:.DS_Store
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:evaluate_gpt2.py
file:.DS_Store
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:mpu/tests/test_cross_entropy.py
file:mpu/tests/test_layers.py
file:mpu/tests/__init__.py
file:mpu/tests/commons.py
file:mpu/tests/test_data.py
file:mpu/tests/test_initialize.py
file:mpu/tests/test_random.py
file:openwebtext/remove_group_duplicates.py
file:openwebtext/run_make_gpt2_dataset.sh
file:openwebtext/group_duplicates_url.py
file:openwebtext/make_gpt2_sizes.py
file:openwebtext/README.md
file:openwebtext/tokenizer.py
file:openwebtext/make_gpt2_dataset.py
file:openwebtext/merge_jsons.py
file:openwebtext/blacklist_urls.py
file:openwebtext/find_duplicates.py
file:evaluate_gpt2.py
file:.DS_Store
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:mpu/tests/test_cross_entropy.py
file:mpu/tests/test_layers.py
file:mpu/tests/__init__.py
file:mpu/tests/commons.py
file:mpu/tests/test_data.py
file:mpu/tests/test_initialize.py
file:mpu/tests/test_random.py
file:openwebtext/remove_group_duplicates.py
file:openwebtext/run_make_gpt2_dataset.sh
file:openwebtext/group_duplicates_url.py
file:openwebtext/make_gpt2_sizes.py
file:openwebtext/README.md
file:openwebtext/tokenizer.py
file:openwebtext/make_gpt2_dataset.py
file:openwebtext/merge_jsons.py
file:openwebtext/blacklist_urls.py
file:openwebtext/find_duplicates.py
file:openwebtext/cleanup_dataset.py
file:.pytorch_pretrained_bert/gpt2-merges.txt
file:.pytorch_pretrained_bert/gpt2-vocab.json
file:data_utils/tokenization_gpt2.py
file:data_utils/lazy_loader.py
file:data_utils/wordpiece.py
file:data_utils/datasets.py
file:data_utils/__init__.py
file:data_utils/tokenization.py
file:data_utils/tf_dl.py
file:data_utils/file_utils.py
file:data_utils/corpora.py
file:data_utils/samplers.py
file:.onebrain/aeac8fb5894ab14fb6d0e543436a0563
file:.onebrain/.onebrain.ini
file:scripts/pretrain_bert_sentencepiece.sh
file:scripts/pretrain_bert_distributed.sh
file:scripts/pretrain_bert.sh
file:scripts/ds_checkpoint_check.sh
file:scripts/pretrain_gpt2_distributed.sh
file:scripts/ds_zero2_pretrain_gpt2_model_parallel.sh
file:scripts/ds_zero2_config.json
file:scripts/generate_text.sh
file:scripts/ds_zero-offload_config.json
file:scripts/ds_zero-offload_10B_config.json
file:scripts/ds_onebit_config.json
file:scripts/pretrain_gpt2_model_parallel.sh
file:scripts/extract_hostfile.py
file:scripts/ds_config.json
file:scripts/ds_zero-offload_pretrain_gpt2_model_parallel.sh
file:scripts/presplit_sentences_json.py
file:scripts/ds_zero-offload_10B_pretrain_gpt2_model_parallel.sh
file:scripts/split_json.py
file:scripts/ds_pretrain_gpt2.sh
file:scripts/pretrain_bert_model_parallel.sh
file:scripts/pretrain_bert_tfrecords_distributed.sh
file:scripts/run_gpt2_eval.py
file:scripts/pretrain_gpt2.sh
file:model/__init__.py
file:model/model.py
file:model/distributed.py
file:model/gpt2_modeling.py
file:model/modeling.py
file:fp16/fp16.py
file:fp16/__init__.py
file:fp16/fp16util.py
file:fp16/loss_scaler.py
file:evaluate_gpt2.py
file:.DS_Store
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:mpu/tests/test_cross_entropy.py
file:mpu/tests/test_layers.py
file:mpu/tests/__init__.py
file:mpu/tests/commons.py
file:mpu/tests/test_data.py
file:mpu/tests/test_initialize.py
file:mpu/tests/test_random.py
file:openwebtext/remove_group_duplicates.py
file:openwebtext/run_make_gpt2_dataset.sh
file:openwebtext/group_duplicates_url.py
file:openwebtext/make_gpt2_sizes.py
file:openwebtext/README.md
file:openwebtext/tokenizer.py
file:openwebtext/make_gpt2_dataset.py
file:openwebtext/merge_jsons.py
file:openwebtext/blacklist_urls.py
file:openwebtext/find_duplicates.py
file:openwebtext/cleanup_dataset.py
file:.pytorch_pretrained_bert/gpt2-merges.txt
file:.pytorch_pretrained_bert/gpt2-vocab.json
file:data_utils/tokenization_gpt2.py
file:data_utils/lazy_loader.py
file:data_utils/wordpiece.py
file:data_utils/datasets.py
file:data_utils/__init__.py
file:data_utils/tokenization.py
file:data_utils/tf_dl.py
file:data_utils/file_utils.py
file:data_utils/corpora.py
file:data_utils/samplers.py
file:.onebrain/aeac8fb5894ab14fb6d0e543436a0563
file:.onebrain/.onebrain.ini
file:scripts/pretrain_bert_sentencepiece.sh
file:scripts/pretrain_bert_distributed.sh
file:scripts/pretrain_bert.sh
file:scripts/ds_checkpoint_check.sh
file:scripts/pretrain_gpt2_distributed.sh
file:scripts/ds_zero2_pretrain_gpt2_model_parallel.sh
file:scripts/ds_zero2_config.json
file:scripts/generate_text.sh
file:scripts/ds_zero-offload_config.json
file:scripts/ds_zero-offload_10B_config.json
file:scripts/ds_onebit_config.json
file:scripts/pretrain_gpt2_model_parallel.sh
file:scripts/extract_hostfile.py
file:scripts/ds_config.json
file:scripts/ds_zero-offload_pretrain_gpt2_model_parallel.sh
file:scripts/presplit_sentences_json.py
file:scripts/ds_zero-offload_10B_pretrain_gpt2_model_parallel.sh
file:scripts/split_json.py
file:scripts/ds_pretrain_gpt2.sh
file:scripts/pretrain_bert_model_parallel.sh
file:scripts/pretrain_bert_tfrecords_distributed.sh
file:scripts/run_gpt2_eval.py
file:scripts/pretrain_gpt2.sh
file:model/__init__.py
file:model/model.py
file:model/distributed.py
file:model/gpt2_modeling.py
file:model/modeling.py
file:fp16/fp16.py
file:fp16/__init__.py
file:fp16/fp16util.py
file:fp16/loss_scaler.py
file:evaluate_gpt2.py
file:.DS_Store
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:mpu/tests/test_cross_entropy.py
file:mpu/tests/test_layers.py
file:mpu/tests/__init__.py
file:mpu/tests/commons.py
file:mpu/tests/test_data.py
file:mpu/tests/test_initialize.py
file:mpu/tests/test_random.py
file:openwebtext/remove_group_duplicates.py
file:openwebtext/run_make_gpt2_dataset.sh
file:openwebtext/group_duplicates_url.py
file:openwebtext/make_gpt2_sizes.py
file:openwebtext/README.md
file:openwebtext/tokenizer.py
file:openwebtext/make_gpt2_dataset.py
file:openwebtext/merge_jsons.py
file:openwebtext/blacklist_urls.py
file:openwebtext/find_duplicates.py
file:openwebtext/cleanup_dataset.py
file:.pytorch_pretrained_bert/gpt2-merges.txt
file:.pytorch_pretrained_bert/gpt2-vocab.json
file:data_utils/tokenization_gpt2.py
file:data_utils/lazy_loader.py
file:data_utils/wordpiece.py
file:data_utils/datasets.py
file:data_utils/__init__.py
file:data_utils/tokenization.py
file:data_utils/tf_dl.py
file:data_utils/file_utils.py
file:data_utils/corpora.py
file:data_utils/samplers.py
file:.onebrain/aeac8fb5894ab14fb6d0e543436a0563
file:.onebrain/.onebrain.ini
file:scripts/pretrain_bert_sentencepiece.sh
file:scripts/pretrain_bert_distributed.sh
file:scripts/pretrain_bert.sh
file:scripts/ds_checkpoint_check.sh
file:scripts/pretrain_gpt2_distributed.sh
file:scripts/ds_zero2_pretrain_gpt2_model_parallel.sh
file:scripts/ds_zero2_config.json
file:scripts/generate_text.sh
file:scripts/ds_zero-offload_config.json
file:scripts/ds_zero-offload_10B_config.json
file:scripts/ds_onebit_config.json
file:scripts/pretrain_gpt2_model_parallel.sh
file:scripts/extract_hostfile.py
file:scripts/ds_config.json
file:scripts/ds_zero-offload_pretrain_gpt2_model_parallel.sh
file:scripts/presplit_sentences_json.py
file:scripts/ds_zero-offload_10B_pretrain_gpt2_model_parallel.sh
file:scripts/split_json.py
file:scripts/ds_pretrain_gpt2.sh
file:scripts/pretrain_bert_model_parallel.sh
file:scripts/pretrain_bert_tfrecords_distributed.sh
file:scripts/run_gpt2_eval.py
file:scripts/pretrain_gpt2.sh
file:model/__init__.py
file:model/model.py
file:model/distributed.py
file:model/gpt2_modeling.py
file:model/modeling.py
file:fp16/fp16.py
file:fp16/__init__.py
file:fp16/fp16util.py
file:fp16/loss_scaler.py
file:evaluate_gpt2.py
file:.DS_Store
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:mpu/tests/test_cross_entropy.py
file:mpu/tests/test_layers.py
file:mpu/tests/__init__.py
file:mpu/tests/commons.py
file:mpu/tests/test_data.py
file:mpu/tests/test_initialize.py
file:mpu/tests/test_random.py
file:openwebtext/remove_group_duplicates.py
file:openwebtext/run_make_gpt2_dataset.sh
file:openwebtext/group_duplicates_url.py
file:openwebtext/make_gpt2_sizes.py
file:openwebtext/README.md
file:openwebtext/tokenizer.py
file:openwebtext/make_gpt2_dataset.py
file:openwebtext/merge_jsons.py
file:openwebtext/blacklist_urls.py
file:openwebtext/find_duplicates.py
file:openwebtext/cleanup_dataset.py
file:.pytorch_pretrained_bert/gpt2-merges.txt
file:.pytorch_pretrained_bert/gpt2-vocab.json
file:data_utils/tokenization_gpt2.py
file:data_utils/lazy_loader.py
file:data_utils/wordpiece.py
file:data_utils/datasets.py
file:data_utils/__init__.py
file:data_utils/tokenization.py
file:data_utils/tf_dl.py
file:data_utils/file_utils.py
file:data_utils/corpora.py
file:data_utils/samplers.py
file:.onebrain/aeac8fb5894ab14fb6d0e543436a0563
file:.onebrain/.onebrain.ini
file:scripts/pretrain_bert_sentencepiece.sh
file:scripts/pretrain_bert_distributed.sh
file:scripts/pretrain_bert.sh
file:scripts/ds_checkpoint_check.sh
file:scripts/pretrain_gpt2_distributed.sh
file:scripts/ds_zero2_pretrain_gpt2_model_parallel.sh
file:scripts/ds_zero2_config.json
file:scripts/generate_text.sh
file:scripts/ds_zero-offload_config.json
file:scripts/ds_zero-offload_10B_config.json
file:scripts/ds_onebit_config.json
file:scripts/pretrain_gpt2_model_parallel.sh
file:scripts/extract_hostfile.py
file:scripts/ds_config.json
file:scripts/ds_zero-offload_pretrain_gpt2_model_parallel.sh
file:scripts/presplit_sentences_json.py
file:scripts/ds_zero-offload_10B_pretrain_gpt2_model_parallel.sh
file:scripts/split_json.py
file:scripts/ds_pretrain_gpt2.sh
file:scripts/pretrain_bert_model_parallel.sh
file:scripts/pretrain_bert_tfrecords_distributed.sh
file:scripts/run_gpt2_eval.py
file:scripts/pretrain_gpt2.sh
file:model/__init__.py
file:model/model.py
file:model/distributed.py
file:model/gpt2_modeling.py
file:model/modeling.py
file:fp16/fp16.py
file:fp16/__init__.py
file:fp16/fp16util.py
file:fp16/loss_scaler.py
file:evaluate_gpt2.py
file:.DS_Store
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:mpu/tests/test_cross_entropy.py
file:mpu/tests/test_layers.py
file:mpu/tests/__init__.py
file:mpu/tests/commons.py
file:mpu/tests/test_data.py
file:mpu/tests/test_initialize.py
file:mpu/tests/test_random.py
file:openwebtext/remove_group_duplicates.py
file:openwebtext/run_make_gpt2_dataset.sh
file:openwebtext/group_duplicates_url.py
file:openwebtext/make_gpt2_sizes.py
file:openwebtext/README.md
file:openwebtext/tokenizer.py
file:openwebtext/make_gpt2_dataset.py
file:openwebtext/merge_jsons.py
file:openwebtext/blacklist_urls.py
file:openwebtext/find_duplicates.py
file:openwebtext/cleanup_dataset.py
file:.pytorch_pretrained_bert/gpt2-merges.txt
file:.pytorch_pretrained_bert/gpt2-vocab.json
file:data_utils/tokenization_gpt2.py
file:data_utils/lazy_loader.py
file:data_utils/wordpiece.py
file:data_utils/datasets.py
file:data_utils/__init__.py
file:data_utils/tokenization.py
file:data_utils/tf_dl.py
file:data_utils/file_utils.py
file:data_utils/corpora.py
file:data_utils/samplers.py
file:.onebrain/aeac8fb5894ab14fb6d0e543436a0563
file:.onebrain/.onebrain.ini
file:scripts/pretrain_bert_sentencepiece.sh
file:scripts/pretrain_bert_distributed.sh
file:scripts/pretrain_bert.sh
file:scripts/ds_checkpoint_check.sh
file:scripts/pretrain_gpt2_distributed.sh
file:scripts/ds_zero2_pretrain_gpt2_model_parallel.sh
file:scripts/ds_zero2_config.json
file:scripts/generate_text.sh
file:scripts/ds_zero-offload_config.json
file:scripts/ds_zero-offload_10B_config.json
file:scripts/ds_onebit_config.json
file:scripts/pretrain_gpt2_model_parallel.sh
file:scripts/extract_hostfile.py
file:scripts/ds_config.json
file:scripts/ds_zero-offload_pretrain_gpt2_model_parallel.sh
file:scripts/presplit_sentences_json.py
file:scripts/ds_zero-offload_10B_pretrain_gpt2_model_parallel.sh
file:scripts/split_json.py
file:scripts/ds_pretrain_gpt2.sh
file:scripts/pretrain_bert_model_parallel.sh
file:scripts/pretrain_bert_tfrecords_distributed.sh
file:scripts/run_gpt2_eval.py
file:scripts/pretrain_gpt2.sh
file:model/__init__.py
file:model/model.py
file:model/distributed.py
file:model/gpt2_modeling.py
file:model/modeling.py
file:fp16/fp16.py
file:fp16/__init__.py
file:fp16/fp16util.py
file:fp16/loss_scaler.py
file:evaluate_gpt2.py
file:.DS_Store
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:mpu/tests/test_cross_entropy.py
file:mpu/tests/test_layers.py
file:mpu/tests/__init__.py
file:mpu/tests/commons.py
file:mpu/tests/test_data.py
file:mpu/tests/test_initialize.py
file:mpu/tests/test_random.py
file:openwebtext/remove_group_duplicates.py
file:openwebtext/run_make_gpt2_dataset.sh
file:openwebtext/group_duplicates_url.py
file:openwebtext/make_gpt2_sizes.py
file:openwebtext/README.md
file:openwebtext/tokenizer.py
file:openwebtext/make_gpt2_dataset.py
file:openwebtext/merge_jsons.py
file:openwebtext/blacklist_urls.py
file:openwebtext/find_duplicates.py
file:openwebtext/cleanup_dataset.py
file:.pytorch_pretrained_bert/gpt2-merges.txt
file:.pytorch_pretrained_bert/gpt2-vocab.json
file:data_utils/tokenization_gpt2.py
file:data_utils/lazy_loader.py
file:data_utils/wordpiece.py
file:data_utils/datasets.py
file:data_utils/__init__.py
file:data_utils/tokenization.py
file:data_utils/tf_dl.py
file:data_utils/file_utils.py
file:data_utils/corpora.py
file:data_utils/samplers.py
file:.onebrain/aeac8fb5894ab14fb6d0e543436a0563
file:.onebrain/.onebrain.ini
file:scripts/pretrain_bert_sentencepiece.sh
file:scripts/pretrain_bert_distributed.sh
file:scripts/pretrain_bert.sh
file:scripts/ds_checkpoint_check.sh
file:scripts/pretrain_gpt2_distributed.sh
file:scripts/ds_zero2_pretrain_gpt2_model_parallel.sh
file:scripts/ds_zero2_config.json
file:scripts/generate_text.sh
file:scripts/ds_zero-offload_config.json
file:scripts/ds_zero-offload_10B_config.json
file:scripts/ds_onebit_config.json
file:scripts/pretrain_gpt2_model_parallel.sh
file:scripts/extract_hostfile.py
file:scripts/ds_config.json
file:scripts/ds_zero-offload_pretrain_gpt2_model_parallel.sh
file:scripts/presplit_sentences_json.py
file:scripts/ds_zero-offload_10B_pretrain_gpt2_model_parallel.sh
file:scripts/split_json.py
file:scripts/ds_pretrain_gpt2.sh
file:scripts/pretrain_bert_model_parallel.sh
file:scripts/pretrain_bert_tfrecords_distributed.sh
file:scripts/run_gpt2_eval.py
file:scripts/pretrain_gpt2.sh
file:model/__init__.py
file:model/model.py
file:model/distributed.py
file:model/gpt2_modeling.py
file:model/modeling.py
file:fp16/fp16.py
file:fp16/__init__.py
file:fp16/fp16util.py
file:fp16/loss_scaler.py
file:evaluate_gpt2.py
file:.DS_Store
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:mpu/tests/test_cross_entropy.py
file:mpu/tests/test_layers.py
file:mpu/tests/__init__.py
file:mpu/tests/commons.py
file:mpu/tests/test_data.py
file:mpu/tests/test_initialize.py
file:mpu/tests/test_random.py
file:openwebtext/remove_group_duplicates.py
file:openwebtext/run_make_gpt2_dataset.sh
file:openwebtext/group_duplicates_url.py
file:openwebtext/make_gpt2_sizes.py
file:openwebtext/README.md
file:openwebtext/tokenizer.py
file:openwebtext/make_gpt2_dataset.py
file:openwebtext/merge_jsons.py
file:openwebtext/blacklist_urls.py
file:openwebtext/find_duplicates.py
file:openwebtext/cleanup_dataset.py
file:.pytorch_pretrained_bert/gpt2-merges.txt
file:.pytorch_pretrained_bert/gpt2-vocab.json
file:data_utils/tokenization_gpt2.py
file:data_utils/lazy_loader.py
file:data_utils/wordpiece.py
file:data_utils/datasets.py
file:data_utils/__init__.py
file:data_utils/tokenization.py
file:data_utils/tf_dl.py
file:data_utils/file_utils.py
file:data_utils/corpora.py
file:data_utils/samplers.py
file:.onebrain/aeac8fb5894ab14fb6d0e543436a0563
file:.onebrain/.onebrain.ini
file:scripts/pretrain_bert_sentencepiece.sh
file:scripts/pretrain_bert_distributed.sh
file:scripts/pretrain_bert.sh
file:scripts/ds_checkpoint_check.sh
file:scripts/pretrain_gpt2_distributed.sh
file:scripts/ds_zero2_pretrain_gpt2_model_parallel.sh
file:scripts/ds_zero2_config.json
file:scripts/generate_text.sh
file:scripts/ds_zero-offload_config.json
file:scripts/ds_zero-offload_10B_config.json
file:scripts/ds_onebit_config.json
file:scripts/pretrain_gpt2_model_parallel.sh
file:scripts/extract_hostfile.py
file:scripts/ds_config.json
file:scripts/ds_zero-offload_pretrain_gpt2_model_parallel.sh
file:scripts/presplit_sentences_json.py
file:scripts/ds_zero-offload_10B_pretrain_gpt2_model_parallel.sh
file:scripts/split_json.py
file:scripts/ds_pretrain_gpt2.sh
file:scripts/pretrain_bert_model_parallel.sh
file:scripts/pretrain_bert_tfrecords_distributed.sh
file:scripts/run_gpt2_eval.py
file:scripts/pretrain_gpt2.sh
file:model/__init__.py
file:model/model.py
file:model/distributed.py
file:model/gpt2_modeling.py
file:model/modeling.py
file:fp16/fp16.py
file:fp16/__init__.py
file:fp16/fp16util.py
file:fp16/loss_scaler.py
file:evaluate_gpt2.py
file:.DS_Store
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:mpu/tests/test_cross_entropy.py
file:mpu/tests/test_layers.py
file:mpu/tests/__init__.py
file:mpu/tests/commons.py
file:mpu/tests/test_data.py
file:mpu/tests/test_initialize.py
file:mpu/tests/test_random.py
file:openwebtext/remove_group_duplicates.py
file:openwebtext/run_make_gpt2_dataset.sh
file:openwebtext/group_duplicates_url.py
file:openwebtext/make_gpt2_sizes.py
file:openwebtext/README.md
file:openwebtext/tokenizer.py
file:openwebtext/make_gpt2_dataset.py
file:openwebtext/merge_jsons.py
file:openwebtext/blacklist_urls.py
file:openwebtext/find_duplicates.py
file:openwebtext/cleanup_dataset.py
file:.pytorch_pretrained_bert/gpt2-merges.txt
file:.pytorch_pretrained_bert/gpt2-vocab.json
file:data_utils/tokenization_gpt2.py
file:data_utils/lazy_loader.py
file:data_utils/wordpiece.py
file:data_utils/datasets.py
file:data_utils/__init__.py
file:data_utils/tokenization.py
file:data_utils/tf_dl.py
file:data_utils/file_utils.py
file:data_utils/corpora.py
file:data_utils/samplers.py
file:.onebrain/aeac8fb5894ab14fb6d0e543436a0563
file:.onebrain/.onebrain.ini
file:scripts/pretrain_bert_sentencepiece.sh
file:scripts/show_rocev2_ipv4_gids.sh
file:scripts/pretrain_bert_distributed.sh
file:scripts/pretrain_bert.sh
file:scripts/ds_checkpoint_check.sh
file:scripts/pretrain_gpt2_distributed.sh
file:scripts/ds_zero2_pretrain_gpt2_model_parallel.sh
file:scripts/ds_zero2_config.json
file:scripts/generate_text.sh
file:scripts/ds_zero-offload_config.json
file:scripts/ds_zero-offload_10B_config.json
file:scripts/ds_onebit_config.json
file:scripts/pretrain_gpt2_model_parallel.sh
file:scripts/extract_hostfile.py
file:scripts/ds_config.json
file:scripts/ds_zero-offload_pretrain_gpt2_model_parallel.sh
file:scripts/presplit_sentences_json.py
file:scripts/ds_zero-offload_10B_pretrain_gpt2_model_parallel.sh
file:scripts/split_json.py
file:scripts/ds_pretrain_gpt2.sh
file:scripts/pretrain_bert_model_parallel.sh
file:scripts/pretrain_bert_tfrecords_distributed.sh
file:scripts/run_gpt2_eval.py
file:scripts/pretrain_gpt2.sh
file:model/__init__.py
file:model/model.py
file:model/distributed.py
file:model/gpt2_modeling.py
file:model/modeling.py
file:fp16/fp16.py
file:fp16/__init__.py
file:fp16/fp16util.py
file:fp16/loss_scaler.py
file:evaluate_gpt2.py
file:.DS_Store
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:mpu/tests/test_cross_entropy.py
file:mpu/tests/test_layers.py
file:mpu/tests/__init__.py
file:mpu/tests/commons.py
file:mpu/tests/test_data.py
file:mpu/tests/test_initialize.py
file:mpu/tests/test_random.py
file:openwebtext/remove_group_duplicates.py
file:openwebtext/run_make_gpt2_dataset.sh
file:openwebtext/group_duplicates_url.py
file:openwebtext/make_gpt2_sizes.py
file:openwebtext/README.md
file:openwebtext/tokenizer.py
file:openwebtext/make_gpt2_dataset.py
file:openwebtext/merge_jsons.py
file:openwebtext/blacklist_urls.py
file:openwebtext/find_duplicates.py
file:openwebtext/cleanup_dataset.py
file:.pytorch_pretrained_bert/gpt2-merges.txt
file:.pytorch_pretrained_bert/gpt2-vocab.json
file:data_utils/tokenization_gpt2.py
file:data_utils/lazy_loader.py
file:data_utils/wordpiece.py
file:data_utils/datasets.py
file:data_utils/__init__.py
file:data_utils/tokenization.py
file:data_utils/tf_dl.py
file:data_utils/file_utils.py
file:data_utils/corpora.py
file:data_utils/samplers.py
file:.onebrain/aeac8fb5894ab14fb6d0e543436a0563
file:.onebrain/.onebrain.ini
file:scripts/pretrain_bert_sentencepiece.sh
file:scripts/show_rocev2_ipv4_gids.sh
file:scripts/pretrain_bert_distributed.sh
file:scripts/pretrain_bert.sh
file:scripts/ds_checkpoint_check.sh
file:scripts/pretrain_gpt2_distributed.sh
file:scripts/ds_zero2_pretrain_gpt2_model_parallel.sh
file:scripts/ds_zero2_config.json
file:scripts/generate_text.sh
file:scripts/ds_zero-offload_config.json
file:scripts/ds_zero-offload_10B_config.json
file:scripts/ds_onebit_config.json
file:scripts/pretrain_gpt2_model_parallel.sh
file:scripts/extract_hostfile.py
file:scripts/ds_config.json
file:scripts/ds_zero-offload_pretrain_gpt2_model_parallel.sh
file:scripts/presplit_sentences_json.py
file:scripts/ds_zero-offload_10B_pretrain_gpt2_model_parallel.sh
file:scripts/split_json.py
file:scripts/ds_pretrain_gpt2.sh
file:scripts/pretrain_bert_model_parallel.sh
file:scripts/pretrain_bert_tfrecords_distributed.sh
file:scripts/run_gpt2_eval.py
file:scripts/pretrain_gpt2.sh
file:model/__init__.py
file:model/model.py
file:model/distributed.py
file:model/gpt2_modeling.py
file:model/modeling.py
file:fp16/fp16.py
file:fp16/__init__.py
file:fp16/fp16util.py
file:fp16/loss_scaler.py
file:evaluate_gpt2.py
file:.DS_Store
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:mpu/tests/test_cross_entropy.py
file:mpu/tests/test_layers.py
file:mpu/tests/__init__.py
file:mpu/tests/commons.py
file:mpu/tests/test_data.py
file:mpu/tests/test_initialize.py
file:mpu/tests/test_random.py
file:openwebtext/remove_group_duplicates.py
file:openwebtext/run_make_gpt2_dataset.sh
file:openwebtext/group_duplicates_url.py
file:openwebtext/make_gpt2_sizes.py
file:openwebtext/README.md
file:openwebtext/tokenizer.py
file:openwebtext/make_gpt2_dataset.py
file:openwebtext/merge_jsons.py
file:openwebtext/blacklist_urls.py
file:openwebtext/find_duplicates.py
file:openwebtext/cleanup_dataset.py
file:.pytorch_pretrained_bert/gpt2-merges.txt
file:.pytorch_pretrained_bert/gpt2-vocab.json
file:data_utils/tokenization_gpt2.py
file:data_utils/lazy_loader.py
file:data_utils/wordpiece.py
file:data_utils/datasets.py
file:data_utils/__init__.py
file:data_utils/tokenization.py
file:data_utils/tf_dl.py
file:data_utils/file_utils.py
file:data_utils/corpora.py
file:data_utils/samplers.py
file:.onebrain/aeac8fb5894ab14fb6d0e543436a0563
file:.onebrain/.onebrain.ini
file:scripts/pretrain_bert_sentencepiece.sh
file:scripts/show_rocev2_ipv4_gids.sh
file:scripts/pretrain_bert_distributed.sh
file:scripts/pretrain_bert.sh
file:scripts/ds_checkpoint_check.sh
file:scripts/pretrain_gpt2_distributed.sh
file:scripts/ds_zero2_pretrain_gpt2_model_parallel.sh
file:scripts/ds_zero2_config.json
file:scripts/generate_text.sh
file:scripts/ds_zero-offload_config.json
file:scripts/ds_zero-offload_10B_config.json
file:scripts/ds_onebit_config.json
file:scripts/pretrain_gpt2_model_parallel.sh
file:scripts/extract_hostfile.py
file:scripts/ds_config.json
file:scripts/ds_zero-offload_pretrain_gpt2_model_parallel.sh
file:scripts/presplit_sentences_json.py
file:scripts/ds_zero-offload_10B_pretrain_gpt2_model_parallel.sh
file:scripts/split_json.py
file:scripts/ds_pretrain_gpt2.sh
file:scripts/pretrain_bert_model_parallel.sh
file:scripts/pretrain_bert_tfrecords_distributed.sh
file:scripts/run_gpt2_eval.py
file:scripts/pretrain_gpt2.sh
file:model/__init__.py
file:model/model.py
file:model/distributed.py
file:model/gpt2_modeling.py
file:model/modeling.py
file:fp16/fp16.py
file:fp16/__init__.py
file:fp16/fp16util.py
file:fp16/loss_scaler.py
file:evaluate_gpt2.py
file:.DS_Store
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:evaluate_gpt2.py
file:.DS_Store
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:evaluate_gpt2.py
file:.DS_Store
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:mpu/tests/test_cross_entropy.py
file:mpu/tests/test_layers.py
file:mpu/tests/__init__.py
file:mpu/tests/commons.py
file:mpu/tests/test_data.py
file:mpu/tests/test_initialize.py
file:mpu/tests/test_random.py
file:chinese_sentencepiece/cog-pretrain.model
file:chinese_sentencepiece/cog-pretrain.vocab
file:openwebtext/remove_group_duplicates.py
file:openwebtext/run_make_gpt2_dataset.sh
file:openwebtext/group_duplicates_url.py
file:openwebtext/make_gpt2_sizes.py
file:openwebtext/README.md
file:openwebtext/tokenizer.py
file:openwebtext/make_gpt2_dataset.py
file:openwebtext/merge_jsons.py
file:openwebtext/blacklist_urls.py
file:openwebtext/find_duplicates.py
file:openwebtext/cleanup_dataset.py
file:.pytorch_pretrained_bert/gpt2-merges.txt
file:.pytorch_pretrained_bert/gpt2-vocab.json
file:data_utils/tokenization_gpt2.py
file:data_utils/lazy_loader.py
file:evaluate_gpt2.py
file:.DS_Store
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:mpu/tests/test_cross_entropy.py
file:mpu/tests/test_layers.py
file:mpu/tests/__init__.py
file:mpu/tests/commons.py
file:mpu/tests/test_data.py
file:mpu/tests/test_initialize.py
file:mpu/tests/test_random.py
file:chinese_sentencepiece/cog-pretrain.model
file:chinese_sentencepiece/cog-pretrain.vocab
file:openwebtext/remove_group_duplicates.py
file:openwebtext/run_make_gpt2_dataset.sh
file:openwebtext/group_duplicates_url.py
file:openwebtext/make_gpt2_sizes.py
file:openwebtext/README.md
file:openwebtext/tokenizer.py
file:openwebtext/make_gpt2_dataset.py
file:openwebtext/merge_jsons.py
file:openwebtext/blacklist_urls.py
file:openwebtext/find_duplicates.py
file:openwebtext/cleanup_dataset.py
file:.pytorch_pretrained_bert/gpt2-merges.txt
file:.pytorch_pretrained_bert/gpt2-vocab.json
file:data_utils/tokenization_gpt2.py
file:data_utils/lazy_loader.py
file:data_utils/wordpiece.py
file:data_utils/datasets.py
file:data_utils/__init__.py
file:data_utils/tokenization.py
file:data_utils/tf_dl.py
file:data_utils/file_utils.py
file:data_utils/sp_tokenizer.py
file:data_utils/corpora.py
file:data_utils/samplers.py
file:data_utils/__pycache__/corpora.cpython-38.pyc
file:data_utils/__pycache__/tokenization_gpt2.cpython-38.pyc
file:data_utils/__pycache__/__init__.cpython-38.pyc
file:data_utils/__pycache__/lazy_loader.cpython-38.pyc
file:data_utils/__pycache__/tokenization.cpython-38.pyc
file:data_utils/__pycache__/file_utils.cpython-38.pyc
file:data_utils/__pycache__/sp_tokenizer.cpython-38.pyc
file:data_utils/__pycache__/wordpiece.cpython-38.pyc
file:data_utils/__pycache__/samplers.cpython-38.pyc
file:data_utils/__pycache__/datasets.cpython-38.pyc
file:.onebrain/aeac8fb5894ab14fb6d0e543436a0563
file:.onebrain/.onebrain.ini
file:scripts/pretrain_bert_sentencepiece.sh
file:scripts/show_rocev2_ipv4_gids.sh
file:scripts/pretrain_bert_distributed.sh
file:scripts/pretrain_bert.sh
file:scripts/ds_checkpoint_check.sh
file:scripts/pretrain_gpt2_distributed.sh
file:scripts/ds_zero2_pretrain_gpt2_model_parallel.sh
file:scripts/ds_zero2_config.json
file:scripts/generate_text.sh
file:scripts/ds_zero-offload_config.json
file:scripts/ds_zero-offload_10B_config.json
file:scripts/ds_onebit_config.json
file:scripts/pretrain_gpt2_model_parallel.sh
file:scripts/extract_hostfile.py
file:scripts/ds_config.json
file:scripts/ds_zero-offload_pretrain_gpt2_model_parallel.sh
file:scripts/presplit_sentences_json.py
file:scripts/ds_zero-offload_10B_pretrain_gpt2_model_parallel.sh
file:scripts/split_json.py
file:scripts/ds_pretrain_gpt2.sh
file:scripts/pretrain_bert_model_parallel.sh
file:scripts/pretrain_bert_tfrecords_distributed.sh
file:scripts/run_gpt2_eval.py
file:scripts/pretrain_gpt2.sh
file:model/__init__.py
file:model/model.py
file:model/distributed.py
file:model/gpt2_modeling.py
file:model/modeling.py
file:fp16/fp16.py
file:fp16/__init__.py
file:fp16/fp16util.py
file:fp16/loss_scaler.py
file:evaluate_gpt2.py
file:.DS_Store
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:mpu/tests/test_cross_entropy.py
file:mpu/tests/test_layers.py
file:mpu/tests/__init__.py
file:mpu/tests/commons.py
file:mpu/tests/test_data.py
file:mpu/tests/test_initialize.py
file:mpu/tests/test_random.py
file:chinese_sentencepiece/cog-pretrain.model
file:chinese_sentencepiece/cog-pretrain.vocab
file:openwebtext/remove_group_duplicates.py
file:openwebtext/run_make_gpt2_dataset.sh
file:openwebtext/group_duplicates_url.py
file:openwebtext/make_gpt2_sizes.py
file:openwebtext/README.md
file:openwebtext/tokenizer.py
file:openwebtext/make_gpt2_dataset.py
file:openwebtext/merge_jsons.py
file:openwebtext/blacklist_urls.py
file:openwebtext/find_duplicates.py
file:openwebtext/cleanup_dataset.py
file:.pytorch_pretrained_bert/gpt2-merges.txt
file:.pytorch_pretrained_bert/gpt2-vocab.json
file:data_utils/tokenization_gpt2.py
file:data_utils/lazy_loader.py
file:data_utils/wordpiece.py
file:data_utils/datasets.py
file:data_utils/__init__.py
file:data_utils/tokenization.py
file:data_utils/tf_dl.py
file:data_utils/file_utils.py
file:data_utils/sp_tokenizer.py
file:data_utils/corpora.py
file:data_utils/samplers.py
file:data_utils/__pycache__/corpora.cpython-38.pyc
file:data_utils/__pycache__/tokenization_gpt2.cpython-38.pyc
file:data_utils/__pycache__/__init__.cpython-38.pyc
file:data_utils/__pycache__/lazy_loader.cpython-38.pyc
file:data_utils/__pycache__/tokenization.cpython-38.pyc
file:data_utils/__pycache__/file_utils.cpython-38.pyc
file:data_utils/__pycache__/sp_tokenizer.cpython-38.pyc
file:data_utils/__pycache__/wordpiece.cpython-38.pyc
file:data_utils/__pycache__/samplers.cpython-38.pyc
file:data_utils/__pycache__/datasets.cpython-38.pyc
file:.onebrain/aeac8fb5894ab14fb6d0e543436a0563
file:.onebrain/.onebrain.ini
file:scripts/pretrain_bert_sentencepiece.sh
file:scripts/show_rocev2_ipv4_gids.sh
file:scripts/pretrain_bert_distributed.sh
file:scripts/pretrain_bert.sh
file:scripts/ds_checkpoint_check.sh
file:scripts/pretrain_gpt2_distributed.sh
file:scripts/ds_zero2_pretrain_gpt2_model_parallel.sh
file:scripts/ds_zero2_config.json
file:scripts/generate_text.sh
file:scripts/ds_zero-offload_config.json
file:scripts/ds_zero-offload_10B_config.json
file:scripts/ds_onebit_config.json
file:scripts/pretrain_gpt2_model_parallel.sh
file:scripts/extract_hostfile.py
file:scripts/ds_config.json
file:scripts/ds_zero-offload_pretrain_gpt2_model_parallel.sh
file:scripts/presplit_sentences_json.py
file:scripts/ds_zero-offload_10B_pretrain_gpt2_model_parallel.sh
file:scripts/split_json.py
file:scripts/ds_pretrain_gpt2.sh
file:scripts/pretrain_bert_model_parallel.sh
file:scripts/pretrain_bert_tfrecords_distributed.sh
file:scripts/run_gpt2_eval.py
file:scripts/pretrain_gpt2.sh
file:model/__init__.py
file:model/model.py
file:model/distributed.py
file:model/gpt2_modeling.py
file:model/modeling.py
file:fp16/fp16.py
file:fp16/__init__.py
file:fp16/fp16util.py
file:fp16/loss_scaler.py
file:evaluate_gpt2.py
file:.DS_Store
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:mpu/tests/test_cross_entropy.py
file:mpu/tests/test_layers.py
file:mpu/tests/__init__.py
file:mpu/tests/commons.py
file:mpu/tests/test_data.py
file:mpu/tests/test_initialize.py
file:mpu/tests/test_random.py
file:chinese_sentencepiece/cog-pretrain.model
file:chinese_sentencepiece/cog-pretrain.vocab
file:openwebtext/remove_group_duplicates.py
file:openwebtext/run_make_gpt2_dataset.sh
file:openwebtext/group_duplicates_url.py
file:openwebtext/make_gpt2_sizes.py
file:openwebtext/README.md
file:openwebtext/tokenizer.py
file:openwebtext/make_gpt2_dataset.py
file:openwebtext/merge_jsons.py
file:openwebtext/blacklist_urls.py
file:openwebtext/find_duplicates.py
file:openwebtext/cleanup_dataset.py
file:.pytorch_pretrained_bert/gpt2-merges.txt
file:.pytorch_pretrained_bert/gpt2-vocab.json
file:data_utils/tokenization_gpt2.py
file:data_utils/lazy_loader.py
file:data_utils/wordpiece.py
file:data_utils/datasets.py
file:data_utils/__init__.py
file:data_utils/tokenization.py
file:data_utils/tf_dl.py
file:data_utils/file_utils.py
file:data_utils/sp_tokenizer.py
file:data_utils/corpora.py
file:data_utils/samplers.py
file:data_utils/__pycache__/corpora.cpython-38.pyc
file:data_utils/__pycache__/tokenization_gpt2.cpython-38.pyc
file:data_utils/__pycache__/__init__.cpython-38.pyc
file:data_utils/__pycache__/lazy_loader.cpython-38.pyc
file:data_utils/__pycache__/tokenization.cpython-38.pyc
file:data_utils/__pycache__/file_utils.cpython-38.pyc
file:data_utils/__pycache__/sp_tokenizer.cpython-38.pyc
file:data_utils/__pycache__/wordpiece.cpython-38.pyc
file:data_utils/__pycache__/samplers.cpython-38.pyc
file:data_utils/__pycache__/datasets.cpython-38.pyc
file:.onebrain/aeac8fb5894ab14fb6d0e543436a0563
file:.onebrain/.onebrain.ini
file:scripts/pretrain_bert_sentencepiece.sh
file:scripts/show_rocev2_ipv4_gids.sh
file:scripts/pretrain_bert_distributed.sh
file:scripts/pretrain_bert.sh
file:scripts/ds_checkpoint_check.sh
file:scripts/pretrain_gpt2_distributed.sh
file:scripts/ds_zero2_pretrain_gpt2_model_parallel.sh
file:scripts/ds_zero2_config.json
file:scripts/generate_text.sh
file:scripts/ds_zero-offload_config.json
file:scripts/ds_zero-offload_10B_config.json
file:scripts/ds_onebit_config.json
file:scripts/pretrain_gpt2_model_parallel.sh
file:scripts/extract_hostfile.py
file:scripts/ds_config.json
file:scripts/ds_zero-offload_pretrain_gpt2_model_parallel.sh
file:scripts/presplit_sentences_json.py
file:scripts/ds_zero-offload_10B_pretrain_gpt2_model_parallel.sh
file:scripts/split_json.py
file:scripts/ds_pretrain_gpt2.sh
file:scripts/pretrain_bert_model_parallel.sh
file:scripts/pretrain_bert_tfrecords_distributed.sh
file:scripts/run_gpt2_eval.py
file:scripts/pretrain_gpt2.sh
file:model/__init__.py
file:model/model.py
file:model/distributed.py
file:model/gpt2_modeling.py
file:model/modeling.py
file:fp16/fp16.py
file:fp16/__init__.py
file:fp16/fp16util.py
file:fp16/loss_scaler.py
file:evaluate_gpt2.py
file:.DS_Store
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:mpu/tests/test_cross_entropy.py
file:mpu/tests/test_layers.py
file:mpu/tests/__init__.py
file:mpu/tests/commons.py
file:mpu/tests/test_data.py
file:mpu/tests/test_initialize.py
file:mpu/tests/test_random.py
file:chinese_sentencepiece/cog-pretrain.model
file:chinese_sentencepiece/cog-pretrain.vocab
file:openwebtext/remove_group_duplicates.py
file:openwebtext/run_make_gpt2_dataset.sh
file:openwebtext/group_duplicates_url.py
file:openwebtext/make_gpt2_sizes.py
file:openwebtext/README.md
file:openwebtext/tokenizer.py
file:openwebtext/make_gpt2_dataset.py
file:openwebtext/merge_jsons.py
file:openwebtext/blacklist_urls.py
file:openwebtext/find_duplicates.py
file:openwebtext/cleanup_dataset.py
file:.pytorch_pretrained_bert/gpt2-merges.txt
file:.pytorch_pretrained_bert/gpt2-vocab.json
file:data_utils/tokenization_gpt2.py
file:data_utils/lazy_loader.py
file:data_utils/wordpiece.py
file:data_utils/datasets.py
file:data_utils/__init__.py
file:data_utils/tokenization.py
file:data_utils/tf_dl.py
file:data_utils/file_utils.py
file:data_utils/sp_tokenizer.py
file:data_utils/corpora.py
file:data_utils/samplers.py
file:data_utils/__pycache__/corpora.cpython-38.pyc
file:data_utils/__pycache__/tokenization_gpt2.cpython-38.pyc
file:data_utils/__pycache__/__init__.cpython-38.pyc
file:data_utils/__pycache__/lazy_loader.cpython-38.pyc
file:data_utils/__pycache__/tokenization.cpython-38.pyc
file:data_utils/__pycache__/file_utils.cpython-38.pyc
file:data_utils/__pycache__/sp_tokenizer.cpython-38.pyc
file:data_utils/__pycache__/wordpiece.cpython-38.pyc
file:data_utils/__pycache__/samplers.cpython-38.pyc
file:data_utils/__pycache__/datasets.cpython-38.pyc
file:.onebrain/aeac8fb5894ab14fb6d0e543436a0563
file:.onebrain/.onebrain.ini
file:scripts/pretrain_bert_sentencepiece.sh
file:scripts/show_rocev2_ipv4_gids.sh
file:scripts/pretrain_bert_distributed.sh
file:scripts/pretrain_bert.sh
file:scripts/ds_checkpoint_check.sh
file:scripts/pretrain_gpt2_distributed.sh
file:scripts/ds_zero2_pretrain_gpt2_model_parallel.sh
file:scripts/ds_zero2_config.json
file:scripts/generate_text.sh
file:scripts/ds_zero-offload_config.json
file:scripts/ds_zero-offload_10B_config.json
file:scripts/ds_onebit_config.json
file:scripts/pretrain_gpt2_model_parallel.sh
file:scripts/extract_hostfile.py
file:scripts/ds_config.json
file:scripts/ds_zero-offload_pretrain_gpt2_model_parallel.sh
file:scripts/presplit_sentences_json.py
file:scripts/ds_zero-offload_10B_pretrain_gpt2_model_parallel.sh
file:scripts/split_json.py
file:scripts/ds_pretrain_gpt2.sh
file:scripts/pretrain_bert_model_parallel.sh
file:scripts/pretrain_bert_tfrecords_distributed.sh
file:scripts/run_gpt2_eval.py
file:scripts/pretrain_gpt2.sh
file:model/__init__.py
file:model/model.py
file:model/distributed.py
file:model/gpt2_modeling.py
file:model/modeling.py
file:fp16/fp16.py
file:fp16/__init__.py
file:fp16/fp16util.py
file:fp16/loss_scaler.py
file:evaluate_gpt2.py
file:.DS_Store
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:mpu/tests/test_cross_entropy.py
file:mpu/tests/test_layers.py
file:mpu/tests/__init__.py
file:mpu/tests/commons.py
file:mpu/tests/test_data.py
file:mpu/tests/test_initialize.py
file:mpu/tests/test_random.py
file:chinese_sentencepiece/cog-pretrain.model
file:chinese_sentencepiece/cog-pretrain.vocab
file:openwebtext/remove_group_duplicates.py
file:openwebtext/run_make_gpt2_dataset.sh
file:openwebtext/group_duplicates_url.py
file:openwebtext/make_gpt2_sizes.py
file:openwebtext/README.md
file:openwebtext/tokenizer.py
file:openwebtext/make_gpt2_dataset.py
file:openwebtext/merge_jsons.py
file:openwebtext/blacklist_urls.py
file:openwebtext/find_duplicates.py
file:openwebtext/cleanup_dataset.py
file:.pytorch_pretrained_bert/gpt2-merges.txt
file:.pytorch_pretrained_bert/gpt2-vocab.json
file:data_utils/tokenization_gpt2.py
file:data_utils/lazy_loader.py
file:data_utils/wordpiece.py
file:data_utils/datasets.py
file:data_utils/__init__.py
file:data_utils/tokenization.py
file:data_utils/tf_dl.py
file:data_utils/file_utils.py
file:data_utils/sp_tokenizer.py
file:data_utils/corpora.py
file:data_utils/samplers.py
file:data_utils/__pycache__/corpora.cpython-38.pyc
file:data_utils/__pycache__/tokenization_gpt2.cpython-38.pyc
file:data_utils/__pycache__/__init__.cpython-38.pyc
file:data_utils/__pycache__/lazy_loader.cpython-38.pyc
file:data_utils/__pycache__/tokenization.cpython-38.pyc
file:data_utils/__pycache__/file_utils.cpython-38.pyc
file:data_utils/__pycache__/sp_tokenizer.cpython-38.pyc
file:data_utils/__pycache__/wordpiece.cpython-38.pyc
file:data_utils/__pycache__/samplers.cpython-38.pyc
file:data_utils/__pycache__/datasets.cpython-38.pyc
file:.onebrain/aeac8fb5894ab14fb6d0e543436a0563
file:.onebrain/.onebrain.ini
file:scripts/pretrain_bert_sentencepiece.sh
file:scripts/show_rocev2_ipv4_gids.sh
file:scripts/pretrain_bert_distributed.sh
file:scripts/pretrain_bert.sh
file:scripts/ds_checkpoint_check.sh
file:scripts/pretrain_gpt2_distributed.sh
file:scripts/ds_zero2_pretrain_gpt2_model_parallel.sh
file:scripts/ds_zero2_config.json
file:scripts/generate_text.sh
file:scripts/ds_zero-offload_config.json
file:scripts/ds_zero-offload_10B_config.json
file:scripts/ds_onebit_config.json
file:scripts/pretrain_gpt2_model_parallel.sh
file:scripts/extract_hostfile.py
file:scripts/ds_config.json
file:scripts/ds_zero-offload_pretrain_gpt2_model_parallel.sh
file:scripts/presplit_sentences_json.py
file:scripts/ds_zero-offload_10B_pretrain_gpt2_model_parallel.sh
file:scripts/split_json.py
file:scripts/ds_pretrain_gpt2.sh
file:scripts/pretrain_bert_model_parallel.sh
file:scripts/pretrain_bert_tfrecords_distributed.sh
file:scripts/run_gpt2_eval.py
file:scripts/pretrain_gpt2.sh
file:model/__init__.py
file:model/model.py
file:model/distributed.py
file:model/gpt2_modeling.py
file:model/modeling.py
file:fp16/fp16.py
file:fp16/__init__.py
file:fp16/fp16util.py
file:fp16/loss_scaler.py
file:evaluate_gpt2.py
file:.DS_Store
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:mpu/tests/test_cross_entropy.py
file:mpu/tests/test_layers.py
file:mpu/tests/__init__.py
file:mpu/tests/commons.py
file:mpu/tests/test_data.py
file:mpu/tests/test_initialize.py
file:mpu/tests/test_random.py
file:chinese_sentencepiece/cog-pretrain.model
file:chinese_sentencepiece/cog-pretrain.vocab
file:openwebtext/remove_group_duplicates.py
file:openwebtext/run_make_gpt2_dataset.sh
file:openwebtext/group_duplicates_url.py
file:openwebtext/make_gpt2_sizes.py
file:openwebtext/README.md
file:openwebtext/tokenizer.py
file:openwebtext/make_gpt2_dataset.py
file:openwebtext/merge_jsons.py
file:openwebtext/blacklist_urls.py
file:openwebtext/find_duplicates.py
file:openwebtext/cleanup_dataset.py
file:.pytorch_pretrained_bert/gpt2-merges.txt
file:.pytorch_pretrained_bert/gpt2-vocab.json
file:data_utils/tokenization_gpt2.py
file:data_utils/lazy_loader.py
file:data_utils/wordpiece.py
file:data_utils/datasets.py
file:data_utils/__init__.py
file:data_utils/tokenization.py
file:data_utils/tf_dl.py
file:data_utils/file_utils.py
file:data_utils/sp_tokenizer.py
file:data_utils/corpora.py
file:data_utils/samplers.py
file:data_utils/__pycache__/corpora.cpython-38.pyc
file:data_utils/__pycache__/tokenization_gpt2.cpython-38.pyc
file:data_utils/__pycache__/__init__.cpython-38.pyc
file:data_utils/__pycache__/lazy_loader.cpython-38.pyc
file:data_utils/__pycache__/tokenization.cpython-38.pyc
file:data_utils/__pycache__/file_utils.cpython-38.pyc
file:data_utils/__pycache__/sp_tokenizer.cpython-38.pyc
file:data_utils/__pycache__/wordpiece.cpython-38.pyc
file:data_utils/__pycache__/samplers.cpython-38.pyc
file:data_utils/__pycache__/datasets.cpython-38.pyc
file:.onebrain/aeac8fb5894ab14fb6d0e543436a0563
file:.onebrain/.onebrain.ini
file:scripts/pretrain_bert_sentencepiece.sh
file:scripts/show_rocev2_ipv4_gids.sh
file:scripts/pretrain_bert_distributed.sh
file:scripts/pretrain_bert.sh
file:scripts/ds_checkpoint_check.sh
file:scripts/pretrain_gpt2_distributed.sh
file:scripts/ds_zero2_pretrain_gpt2_model_parallel.sh
file:scripts/ds_zero2_config.json
file:scripts/generate_text.sh
file:scripts/ds_zero-offload_config.json
file:scripts/ds_zero-offload_10B_config.json
file:scripts/ds_onebit_config.json
file:scripts/pretrain_gpt2_model_parallel.sh
file:scripts/extract_hostfile.py
file:scripts/ds_config.json
file:scripts/ds_zero-offload_pretrain_gpt2_model_parallel.sh
file:scripts/presplit_sentences_json.py
file:scripts/ds_zero-offload_10B_pretrain_gpt2_model_parallel.sh
file:scripts/split_json.py
file:scripts/ds_pretrain_gpt2.sh
file:scripts/pretrain_bert_model_parallel.sh
file:scripts/pretrain_bert_tfrecords_distributed.sh
file:scripts/run_gpt2_eval.py
file:scripts/pretrain_gpt2.sh
file:model/__init__.py
file:model/model.py
file:model/distributed.py
file:model/gpt2_modeling.py
file:model/modeling.py
file:fp16/fp16.py
file:fp16/__init__.py
file:fp16/fp16util.py
file:fp16/loss_scaler.py
file:evaluate_gpt2.py
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:mpu/tests/test_cross_entropy.py
file:mpu/tests/test_layers.py
file:mpu/tests/__init__.py
file:mpu/tests/commons.py
file:mpu/tests/test_data.py
file:mpu/tests/test_initialize.py
file:mpu/tests/test_random.py
file:chinese_sentencepiece/cog-pretrain.model
file:chinese_sentencepiece/cog-pretrain.vocab
file:openwebtext/remove_group_duplicates.py
file:openwebtext/run_make_gpt2_dataset.sh
file:openwebtext/group_duplicates_url.py
file:openwebtext/make_gpt2_sizes.py
file:openwebtext/README.md
file:openwebtext/tokenizer.py
file:openwebtext/make_gpt2_dataset.py
file:openwebtext/merge_jsons.py
file:openwebtext/blacklist_urls.py
file:openwebtext/find_duplicates.py
file:openwebtext/cleanup_dataset.py
file:.pytorch_pretrained_bert/gpt2-merges.txt
file:.pytorch_pretrained_bert/gpt2-vocab.json
file:data_utils/tokenization_gpt2.py
file:data_utils/lazy_loader.py
file:data_utils/wordpiece.py
file:data_utils/datasets.py
file:data_utils/__init__.py
file:data_utils/tokenization.py
file:data_utils/tf_dl.py
file:data_utils/file_utils.py
file:data_utils/sp_tokenizer.py
file:data_utils/corpora.py
file:data_utils/samplers.py
file:data_utils/__pycache__/corpora.cpython-38.pyc
file:data_utils/__pycache__/tokenization_gpt2.cpython-38.pyc
file:data_utils/__pycache__/__init__.cpython-38.pyc
file:data_utils/__pycache__/lazy_loader.cpython-38.pyc
file:data_utils/__pycache__/tokenization.cpython-38.pyc
file:data_utils/__pycache__/file_utils.cpython-38.pyc
file:data_utils/__pycache__/sp_tokenizer.cpython-38.pyc
file:data_utils/__pycache__/wordpiece.cpython-38.pyc
file:data_utils/__pycache__/samplers.cpython-38.pyc
file:data_utils/__pycache__/datasets.cpython-38.pyc
file:.onebrain/aeac8fb5894ab14fb6d0e543436a0563
file:.onebrain/.onebrain.ini
file:scripts/pretrain_bert_sentencepiece.sh
file:scripts/show_rocev2_ipv4_gids.sh
file:scripts/pretrain_bert_distributed.sh
file:scripts/pretrain_bert.sh
file:scripts/ds_checkpoint_check.sh
file:scripts/pretrain_gpt2_distributed.sh
file:scripts/ds_zero2_pretrain_gpt2_model_parallel.sh
file:scripts/ds_zero2_config.json
file:scripts/generate_text.sh
file:scripts/ds_zero-offload_config.json
file:scripts/ds_zero-offload_10B_config.json
file:scripts/ds_onebit_config.json
file:scripts/pretrain_gpt2_model_parallel.sh
file:scripts/extract_hostfile.py
file:scripts/ds_config.json
file:scripts/ds_zero-offload_pretrain_gpt2_model_parallel.sh
file:scripts/presplit_sentences_json.py
file:scripts/ds_zero-offload_10B_pretrain_gpt2_model_parallel.sh
file:scripts/split_json.py
file:scripts/ds_pretrain_gpt2.sh
file:scripts/pretrain_bert_model_parallel.sh
file:scripts/pretrain_bert_tfrecords_distributed.sh
file:scripts/run_gpt2_eval.py
file:scripts/pretrain_gpt2.sh
file:model/__init__.py
file:model/model.py
file:model/distributed.py
file:model/gpt2_modeling.py
file:model/modeling.py
file:fp16/fp16.py
file:fp16/__init__.py
file:fp16/fp16util.py
file:fp16/loss_scaler.py
file:evaluate_gpt2.py
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:mpu/tests/test_cross_entropy.py
file:mpu/tests/test_layers.py
file:mpu/tests/__init__.py
file:mpu/tests/commons.py
file:mpu/tests/test_data.py
file:mpu/tests/test_initialize.py
file:mpu/tests/test_random.py
file:chinese_sentencepiece/cog-pretrain.model
file:chinese_sentencepiece/cog-pretrain.vocab
file:openwebtext/remove_group_duplicates.py
file:openwebtext/run_make_gpt2_dataset.sh
file:openwebtext/group_duplicates_url.py
file:openwebtext/make_gpt2_sizes.py
file:openwebtext/README.md
file:openwebtext/tokenizer.py
file:openwebtext/make_gpt2_dataset.py
file:openwebtext/merge_jsons.py
file:openwebtext/blacklist_urls.py
file:openwebtext/find_duplicates.py
file:openwebtext/cleanup_dataset.py
file:.pytorch_pretrained_bert/gpt2-merges.txt
file:.pytorch_pretrained_bert/gpt2-vocab.json
file:data_utils/tokenization_gpt2.py
file:data_utils/lazy_loader.py
file:data_utils/wordpiece.py
file:data_utils/datasets.py
file:data_utils/__init__.py
file:data_utils/tokenization.py
file:data_utils/tf_dl.py
file:data_utils/file_utils.py
file:data_utils/sp_tokenizer.py
file:data_utils/corpora.py
file:data_utils/samplers.py
file:data_utils/__pycache__/corpora.cpython-38.pyc
file:data_utils/__pycache__/tokenization_gpt2.cpython-38.pyc
file:data_utils/__pycache__/__init__.cpython-38.pyc
file:data_utils/__pycache__/lazy_loader.cpython-38.pyc
file:data_utils/__pycache__/tokenization.cpython-38.pyc
file:data_utils/__pycache__/file_utils.cpython-38.pyc
file:data_utils/__pycache__/sp_tokenizer.cpython-38.pyc
file:data_utils/__pycache__/wordpiece.cpython-38.pyc
file:data_utils/__pycache__/samplers.cpython-38.pyc
file:data_utils/__pycache__/datasets.cpython-38.pyc
file:.onebrain/aeac8fb5894ab14fb6d0e543436a0563
file:.onebrain/.onebrain.ini
file:scripts/pretrain_bert_sentencepiece.sh
file:scripts/show_rocev2_ipv4_gids.sh
file:scripts/pretrain_bert_distributed.sh
file:scripts/pretrain_bert.sh
file:scripts/ds_checkpoint_check.sh
file:scripts/pretrain_gpt2_distributed.sh
file:scripts/ds_zero2_pretrain_gpt2_model_parallel.sh
file:scripts/ds_zero2_config.json
file:scripts/generate_text.sh
file:scripts/ds_zero-offload_config.json
file:scripts/ds_zero-offload_10B_config.json
file:scripts/ds_onebit_config.json
file:scripts/pretrain_gpt2_model_parallel.sh
file:scripts/extract_hostfile.py
file:scripts/ds_config.json
file:scripts/ds_zero-offload_pretrain_gpt2_model_parallel.sh
file:scripts/presplit_sentences_json.py
file:scripts/ds_zero-offload_10B_pretrain_gpt2_model_parallel.sh
file:scripts/split_json.py
file:scripts/ds_pretrain_gpt2.sh
file:scripts/pretrain_bert_model_parallel.sh
file:scripts/pretrain_bert_tfrecords_distributed.sh
file:scripts/run_gpt2_eval.py
file:scripts/pretrain_gpt2.sh
file:model/__init__.py
file:model/model.py
file:model/distributed.py
file:model/gpt2_modeling.py
file:model/modeling.py
file:fp16/fp16.py
file:fp16/__init__.py
file:fp16/fp16util.py
file:fp16/loss_scaler.py
file:evaluate_gpt2.py
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:mpu/tests/test_cross_entropy.py
file:mpu/tests/test_layers.py
file:mpu/tests/__init__.py
file:mpu/tests/commons.py
file:mpu/tests/test_data.py
file:mpu/tests/test_initialize.py
file:mpu/tests/test_random.py
file:chinese_sentencepiece/cog-pretrain.model
file:chinese_sentencepiece/cog-pretrain.vocab
file:openwebtext/remove_group_duplicates.py
file:openwebtext/run_make_gpt2_dataset.sh
file:openwebtext/group_duplicates_url.py
file:openwebtext/make_gpt2_sizes.py
file:openwebtext/README.md
file:openwebtext/tokenizer.py
file:openwebtext/make_gpt2_dataset.py
file:openwebtext/merge_jsons.py
file:openwebtext/blacklist_urls.py
file:openwebtext/find_duplicates.py
file:openwebtext/cleanup_dataset.py
file:.pytorch_pretrained_bert/gpt2-merges.txt
file:.pytorch_pretrained_bert/gpt2-vocab.json
file:data_utils/tokenization_gpt2.py
file:data_utils/lazy_loader.py
file:data_utils/wordpiece.py
file:data_utils/datasets.py
file:data_utils/__init__.py
file:data_utils/tokenization.py
file:data_utils/tf_dl.py
file:data_utils/file_utils.py
file:data_utils/sp_tokenizer.py
file:data_utils/corpora.py
file:data_utils/samplers.py
file:data_utils/__pycache__/corpora.cpython-38.pyc
file:data_utils/__pycache__/tokenization_gpt2.cpython-38.pyc
file:data_utils/__pycache__/__init__.cpython-38.pyc
file:data_utils/__pycache__/lazy_loader.cpython-38.pyc
file:data_utils/__pycache__/tokenization.cpython-38.pyc
file:data_utils/__pycache__/file_utils.cpython-38.pyc
file:data_utils/__pycache__/sp_tokenizer.cpython-38.pyc
file:data_utils/__pycache__/wordpiece.cpython-38.pyc
file:data_utils/__pycache__/samplers.cpython-38.pyc
file:data_utils/__pycache__/datasets.cpython-38.pyc
file:.onebrain/aeac8fb5894ab14fb6d0e543436a0563
file:.onebrain/.onebrain.ini
file:scripts/pretrain_bert_sentencepiece.sh
file:scripts/show_rocev2_ipv4_gids.sh
file:scripts/pretrain_bert_distributed.sh
file:scripts/pretrain_bert.sh
file:scripts/ds_checkpoint_check.sh
file:scripts/pretrain_gpt2_distributed.sh
file:scripts/ds_zero2_pretrain_gpt2_model_parallel.sh
file:scripts/ds_zero2_config.json
file:scripts/generate_text.sh
file:scripts/ds_zero-offload_config.json
file:scripts/ds_zero-offload_10B_config.json
file:scripts/ds_onebit_config.json
file:scripts/pretrain_gpt2_model_parallel.sh
file:scripts/extract_hostfile.py
file:scripts/ds_config.json
file:scripts/ds_zero-offload_pretrain_gpt2_model_parallel.sh
file:scripts/presplit_sentences_json.py
file:scripts/ds_zero-offload_10B_pretrain_gpt2_model_parallel.sh
file:scripts/split_json.py
file:scripts/ds_pretrain_gpt2.sh
file:scripts/ds_pretrain_gpt2_chinese.sh
file:scripts/pretrain_bert_model_parallel.sh
file:scripts/pretrain_bert_tfrecords_distributed.sh
file:scripts/run_gpt2_eval.py
file:scripts/pretrain_gpt2.sh
file:model/__init__.py
file:model/model.py
file:model/distributed.py
file:model/gpt2_modeling.py
file:model/modeling.py
file:fp16/fp16.py
file:fp16/__init__.py
file:fp16/fp16util.py
file:fp16/loss_scaler.py
file:evaluate_gpt2.py
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:mpu/tests/test_cross_entropy.py
file:mpu/tests/test_layers.py
file:mpu/tests/__init__.py
file:mpu/tests/commons.py
file:mpu/tests/test_data.py
file:mpu/tests/test_initialize.py
file:mpu/tests/test_random.py
file:chinese_sentencepiece/cog-pretrain.model
file:chinese_sentencepiece/cog-pretrain.vocab
file:openwebtext/remove_group_duplicates.py
file:openwebtext/run_make_gpt2_dataset.sh
file:openwebtext/group_duplicates_url.py
file:openwebtext/make_gpt2_sizes.py
file:openwebtext/README.md
file:openwebtext/tokenizer.py
file:openwebtext/make_gpt2_dataset.py
file:openwebtext/merge_jsons.py
file:openwebtext/blacklist_urls.py
file:openwebtext/find_duplicates.py
file:openwebtext/cleanup_dataset.py
file:.pytorch_pretrained_bert/gpt2-merges.txt
file:.pytorch_pretrained_bert/gpt2-vocab.json
file:data_utils/tokenization_gpt2.py
file:data_utils/lazy_loader.py
file:data_utils/wordpiece.py
file:data_utils/datasets.py
file:data_utils/__init__.py
file:data_utils/tokenization.py
file:data_utils/tf_dl.py
file:data_utils/file_utils.py
file:data_utils/sp_tokenizer.py
file:data_utils/corpora.py
file:data_utils/samplers.py
file:data_utils/__pycache__/corpora.cpython-38.pyc
file:data_utils/__pycache__/tokenization_gpt2.cpython-38.pyc
file:data_utils/__pycache__/__init__.cpython-38.pyc
file:data_utils/__pycache__/lazy_loader.cpython-38.pyc
file:data_utils/__pycache__/tokenization.cpython-38.pyc
file:data_utils/__pycache__/file_utils.cpython-38.pyc
file:data_utils/__pycache__/sp_tokenizer.cpython-38.pyc
file:data_utils/__pycache__/wordpiece.cpython-38.pyc
file:data_utils/__pycache__/samplers.cpython-38.pyc
file:data_utils/__pycache__/datasets.cpython-38.pyc
file:.onebrain/aeac8fb5894ab14fb6d0e543436a0563
file:.onebrain/.onebrain.ini
file:scripts/pretrain_bert_sentencepiece.sh
file:scripts/show_rocev2_ipv4_gids.sh
file:scripts/pretrain_bert_distributed.sh
file:scripts/pretrain_bert.sh
file:scripts/ds_checkpoint_check.sh
file:scripts/pretrain_gpt2_distributed.sh
file:scripts/ds_zero2_pretrain_gpt2_model_parallel.sh
file:scripts/ds_zero2_config.json
file:scripts/generate_text.sh
file:scripts/ds_zero-offload_config.json
file:scripts/ds_zero-offload_10B_config.json
file:scripts/ds_onebit_config.json
file:scripts/pretrain_gpt2_model_parallel.sh
file:scripts/extract_hostfile.py
file:scripts/ds_config.json
file:scripts/ds_zero-offload_pretrain_gpt2_model_parallel.sh
file:scripts/presplit_sentences_json.py
file:scripts/ds_zero-offload_10B_pretrain_gpt2_model_parallel.sh
file:scripts/split_json.py
file:scripts/ds_pretrain_gpt2.sh
file:scripts/ds_pretrain_gpt2_chinese.sh
file:scripts/pretrain_bert_model_parallel.sh
file:scripts/pretrain_bert_tfrecords_distributed.sh
file:scripts/run_gpt2_eval.py
file:scripts/pretrain_gpt2.sh
file:model/__init__.py
file:model/model.py
file:model/distributed.py
file:model/gpt2_modeling.py
file:model/modeling.py
file:fp16/fp16.py
file:fp16/__init__.py
file:fp16/fp16util.py
file:fp16/loss_scaler.py
file:evaluate_gpt2.py
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:mpu/tests/test_cross_entropy.py
file:mpu/tests/test_layers.py
file:mpu/tests/__init__.py
file:mpu/tests/commons.py
file:mpu/tests/test_data.py
file:mpu/tests/test_initialize.py
file:mpu/tests/test_random.py
file:chinese_sentencepiece/cog-pretrain.model
file:chinese_sentencepiece/cog-pretrain.vocab
file:openwebtext/remove_group_duplicates.py
file:openwebtext/run_make_gpt2_dataset.sh
file:openwebtext/group_duplicates_url.py
file:openwebtext/make_gpt2_sizes.py
file:openwebtext/README.md
file:openwebtext/tokenizer.py
file:openwebtext/make_gpt2_dataset.py
file:openwebtext/merge_jsons.py
file:openwebtext/blacklist_urls.py
file:openwebtext/find_duplicates.py
file:openwebtext/cleanup_dataset.py
file:.pytorch_pretrained_bert/gpt2-merges.txt
file:.pytorch_pretrained_bert/gpt2-vocab.json
file:data_utils/tokenization_gpt2.py
file:data_utils/lazy_loader.py
file:data_utils/wordpiece.py
file:data_utils/datasets.py
file:data_utils/__init__.py
file:data_utils/tokenization.py
file:data_utils/tf_dl.py
file:data_utils/file_utils.py
file:data_utils/sp_tokenizer.py
file:data_utils/corpora.py
file:data_utils/samplers.py
file:data_utils/__pycache__/corpora.cpython-38.pyc
file:data_utils/__pycache__/tokenization_gpt2.cpython-38.pyc
file:data_utils/__pycache__/__init__.cpython-38.pyc
file:data_utils/__pycache__/lazy_loader.cpython-38.pyc
file:data_utils/__pycache__/tokenization.cpython-38.pyc
file:data_utils/__pycache__/file_utils.cpython-38.pyc
file:data_utils/__pycache__/sp_tokenizer.cpython-38.pyc
file:data_utils/__pycache__/wordpiece.cpython-38.pyc
file:data_utils/__pycache__/samplers.cpython-38.pyc
file:data_utils/__pycache__/datasets.cpython-38.pyc
file:.onebrain/aeac8fb5894ab14fb6d0e543436a0563
file:.onebrain/.onebrain.ini
file:scripts/pretrain_bert_sentencepiece.sh
file:scripts/show_rocev2_ipv4_gids.sh
file:scripts/pretrain_bert_distributed.sh
file:scripts/pretrain_bert.sh
file:scripts/ds_checkpoint_check.sh
file:scripts/pretrain_gpt2_distributed.sh
file:scripts/ds_zero2_pretrain_gpt2_model_parallel.sh
file:scripts/ds_zero2_config.json
file:scripts/generate_text.sh
file:scripts/ds_zero-offload_config.json
file:scripts/ds_zero-offload_10B_config.json
file:scripts/ds_onebit_config.json
file:scripts/pretrain_gpt2_model_parallel.sh
file:scripts/extract_hostfile.py
file:scripts/ds_config.json
file:scripts/ds_zero-offload_pretrain_gpt2_model_parallel.sh
file:scripts/presplit_sentences_json.py
file:scripts/ds_zero-offload_10B_pretrain_gpt2_model_parallel.sh
file:scripts/split_json.py
file:scripts/ds_pretrain_gpt2.sh
file:scripts/ds_pretrain_gpt2_chinese.sh
file:scripts/pretrain_bert_model_parallel.sh
file:scripts/pretrain_bert_tfrecords_distributed.sh
file:scripts/run_gpt2_eval.py
file:scripts/pretrain_gpt2.sh
file:model/__init__.py
file:model/model.py
file:model/distributed.py
file:model/gpt2_modeling.py
file:model/modeling.py
file:fp16/fp16.py
file:fp16/__init__.py
file:fp16/fp16util.py
file:fp16/loss_scaler.py
file:evaluate_gpt2.py
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:mpu/tests/test_cross_entropy.py
file:mpu/tests/test_layers.py
file:mpu/tests/__init__.py
file:mpu/tests/commons.py
file:mpu/tests/test_data.py
file:mpu/tests/test_initialize.py
file:mpu/tests/test_random.py
file:chinese_sentencepiece/cog-pretrain.model
file:chinese_sentencepiece/cog-pretrain.vocab
file:openwebtext/remove_group_duplicates.py
file:openwebtext/run_make_gpt2_dataset.sh
file:openwebtext/group_duplicates_url.py
file:openwebtext/make_gpt2_sizes.py
file:openwebtext/README.md
file:openwebtext/tokenizer.py
file:openwebtext/make_gpt2_dataset.py
file:openwebtext/merge_jsons.py
file:openwebtext/blacklist_urls.py
file:openwebtext/find_duplicates.py
file:openwebtext/cleanup_dataset.py
file:.pytorch_pretrained_bert/gpt2-merges.txt
file:.pytorch_pretrained_bert/gpt2-vocab.json
file:data_utils/tokenization_gpt2.py
file:data_utils/lazy_loader.py
file:data_utils/wordpiece.py
file:data_utils/datasets.py
file:data_utils/__init__.py
file:data_utils/tokenization.py
file:data_utils/tf_dl.py
file:data_utils/file_utils.py
file:data_utils/sp_tokenizer.py
file:data_utils/corpora.py
file:data_utils/samplers.py
file:data_utils/__pycache__/corpora.cpython-38.pyc
file:data_utils/__pycache__/tokenization_gpt2.cpython-38.pyc
file:data_utils/__pycache__/__init__.cpython-38.pyc
file:data_utils/__pycache__/lazy_loader.cpython-38.pyc
file:data_utils/__pycache__/tokenization.cpython-38.pyc
file:data_utils/__pycache__/file_utils.cpython-38.pyc
file:data_utils/__pycache__/sp_tokenizer.cpython-38.pyc
file:data_utils/__pycache__/wordpiece.cpython-38.pyc
file:data_utils/__pycache__/samplers.cpython-38.pyc
file:data_utils/__pycache__/datasets.cpython-38.pyc
file:.onebrain/aeac8fb5894ab14fb6d0e543436a0563
file:.onebrain/.onebrain.ini
file:scripts/pretrain_bert_sentencepiece.sh
file:scripts/show_rocev2_ipv4_gids.sh
file:scripts/pretrain_bert_distributed.sh
file:scripts/pretrain_bert.sh
file:scripts/ds_checkpoint_check.sh
file:scripts/pretrain_gpt2_distributed.sh
file:scripts/ds_zero2_pretrain_gpt2_model_parallel.sh
file:scripts/ds_zero2_config.json
file:scripts/generate_text.sh
file:scripts/ds_zero-offload_config.json
file:scripts/ds_zero-offload_10B_config.json
file:scripts/ds_onebit_config.json
file:scripts/pretrain_gpt2_model_parallel.sh
file:scripts/extract_hostfile.py
file:scripts/ds_config.json
file:scripts/ds_zero-offload_pretrain_gpt2_model_parallel.sh
file:scripts/presplit_sentences_json.py
file:scripts/ds_zero-offload_10B_pretrain_gpt2_model_parallel.sh
file:scripts/split_json.py
file:scripts/ds_pretrain_gpt2.sh
file:scripts/ds_pretrain_gpt2_chinese.sh
file:scripts/pretrain_bert_model_parallel.sh
file:scripts/pretrain_bert_tfrecords_distributed.sh
file:scripts/run_gpt2_eval.py
file:scripts/pretrain_gpt2.sh
file:model/__init__.py
file:model/model.py
file:model/distributed.py
file:model/gpt2_modeling.py
file:model/modeling.py
file:fp16/fp16.py
file:fp16/__init__.py
file:fp16/fp16util.py
file:fp16/loss_scaler.py
file:evaluate_gpt2.py
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:mpu/tests/test_cross_entropy.py
file:mpu/tests/test_layers.py
file:mpu/tests/__init__.py
file:mpu/tests/commons.py
file:mpu/tests/test_data.py
file:mpu/tests/test_initialize.py
file:mpu/tests/test_random.py
file:chinese_sentencepiece/cog-pretrain.model
file:chinese_sentencepiece/cog-pretrain.vocab
file:openwebtext/remove_group_duplicates.py
file:openwebtext/run_make_gpt2_dataset.sh
file:openwebtext/group_duplicates_url.py
file:openwebtext/make_gpt2_sizes.py
file:openwebtext/README.md
file:openwebtext/tokenizer.py
file:openwebtext/make_gpt2_dataset.py
file:openwebtext/merge_jsons.py
file:openwebtext/blacklist_urls.py
file:openwebtext/find_duplicates.py
file:openwebtext/cleanup_dataset.py
file:.pytorch_pretrained_bert/gpt2-merges.txt
file:.pytorch_pretrained_bert/gpt2-vocab.json
file:data_utils/tokenization_gpt2.py
file:data_utils/lazy_loader.py
file:data_utils/wordpiece.py
file:data_utils/datasets.py
file:data_utils/__init__.py
file:data_utils/tokenization.py
file:data_utils/tf_dl.py
file:data_utils/file_utils.py
file:data_utils/sp_tokenizer.py
file:data_utils/corpora.py
file:data_utils/samplers.py
file:data_utils/__pycache__/corpora.cpython-38.pyc
file:data_utils/__pycache__/tokenization_gpt2.cpython-38.pyc
file:data_utils/__pycache__/__init__.cpython-38.pyc
file:data_utils/__pycache__/lazy_loader.cpython-38.pyc
file:data_utils/__pycache__/tokenization.cpython-38.pyc
file:data_utils/__pycache__/file_utils.cpython-38.pyc
file:data_utils/__pycache__/sp_tokenizer.cpython-38.pyc
file:data_utils/__pycache__/wordpiece.cpython-38.pyc
file:data_utils/__pycache__/samplers.cpython-38.pyc
file:data_utils/__pycache__/datasets.cpython-38.pyc
file:.onebrain/aeac8fb5894ab14fb6d0e543436a0563
file:.onebrain/.onebrain.ini
file:scripts/pretrain_bert_sentencepiece.sh
file:scripts/show_rocev2_ipv4_gids.sh
file:scripts/pretrain_bert_distributed.sh
file:scripts/pretrain_bert.sh
file:scripts/ds_checkpoint_check.sh
file:scripts/pretrain_gpt2_distributed.sh
file:scripts/ds_zero2_pretrain_gpt2_model_parallel.sh
file:scripts/ds_zero2_config.json
file:scripts/generate_text.sh
file:scripts/ds_zero-offload_config.json
file:scripts/ds_zero-offload_10B_config.json
file:scripts/ds_onebit_config.json
file:scripts/pretrain_gpt2_model_parallel.sh
file:scripts/extract_hostfile.py
file:scripts/ds_config.json
file:scripts/ds_zero-offload_pretrain_gpt2_model_parallel.sh
file:scripts/presplit_sentences_json.py
file:scripts/ds_zero-offload_10B_pretrain_gpt2_model_parallel.sh
file:scripts/split_json.py
file:scripts/ds_pretrain_gpt2.sh
file:scripts/ds_pretrain_gpt2_chinese.sh
file:scripts/pretrain_bert_model_parallel.sh
file:scripts/pretrain_bert_tfrecords_distributed.sh
file:scripts/run_gpt2_eval.py
file:scripts/pretrain_gpt2.sh
file:model/__init__.py
file:model/model.py
file:model/distributed.py
file:model/gpt2_modeling.py
file:model/modeling.py
file:fp16/fp16.py
file:fp16/__init__.py
file:fp16/fp16util.py
file:fp16/loss_scaler.py
file:evaluate_gpt2.py
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:mpu/tests/test_cross_entropy.py
file:mpu/tests/test_layers.py
file:mpu/tests/__init__.py
file:mpu/tests/commons.py
file:mpu/tests/test_data.py
file:mpu/tests/test_initialize.py
file:mpu/tests/test_random.py
file:chinese_sentencepiece/cog-pretrain.model
file:chinese_sentencepiece/cog-pretrain.vocab
file:openwebtext/remove_group_duplicates.py
file:openwebtext/run_make_gpt2_dataset.sh
file:openwebtext/group_duplicates_url.py
file:openwebtext/make_gpt2_sizes.py
file:openwebtext/README.md
file:openwebtext/tokenizer.py
file:openwebtext/make_gpt2_dataset.py
file:openwebtext/merge_jsons.py
file:openwebtext/blacklist_urls.py
file:openwebtext/find_duplicates.py
file:openwebtext/cleanup_dataset.py
file:.pytorch_pretrained_bert/gpt2-merges.txt
file:.pytorch_pretrained_bert/gpt2-vocab.json
file:data_utils/tokenization_gpt2.py
file:data_utils/lazy_loader.py
file:data_utils/wordpiece.py
file:data_utils/datasets.py
file:data_utils/__init__.py
file:data_utils/tokenization.py
file:data_utils/tf_dl.py
file:data_utils/file_utils.py
file:data_utils/sp_tokenizer.py
file:data_utils/corpora.py
file:data_utils/samplers.py
file:data_utils/__pycache__/corpora.cpython-38.pyc
file:data_utils/__pycache__/tokenization_gpt2.cpython-38.pyc
file:data_utils/__pycache__/__init__.cpython-38.pyc
file:data_utils/__pycache__/lazy_loader.cpython-38.pyc
file:data_utils/__pycache__/tokenization.cpython-38.pyc
file:data_utils/__pycache__/file_utils.cpython-38.pyc
file:data_utils/__pycache__/sp_tokenizer.cpython-38.pyc
file:data_utils/__pycache__/wordpiece.cpython-38.pyc
file:data_utils/__pycache__/samplers.cpython-38.pyc
file:data_utils/__pycache__/datasets.cpython-38.pyc
file:.onebrain/aeac8fb5894ab14fb6d0e543436a0563
file:.onebrain/.onebrain.ini
file:scripts/pretrain_bert_sentencepiece.sh
file:scripts/show_rocev2_ipv4_gids.sh
file:scripts/pretrain_bert_distributed.sh
file:scripts/pretrain_bert.sh
file:scripts/ds_checkpoint_check.sh
file:scripts/pretrain_gpt2_distributed.sh
file:scripts/ds_zero2_pretrain_gpt2_model_parallel.sh
file:scripts/ds_zero2_config.json
file:scripts/generate_text.sh
file:scripts/ds_zero-offload_config.json
file:scripts/ds_zero-offload_10B_config.json
file:scripts/ds_onebit_config.json
file:scripts/pretrain_gpt2_model_parallel.sh
file:scripts/extract_hostfile.py
file:scripts/ds_config.json
file:scripts/ds_zero-offload_pretrain_gpt2_model_parallel.sh
file:scripts/presplit_sentences_json.py
file:scripts/ds_zero-offload_10B_pretrain_gpt2_model_parallel.sh
file:scripts/split_json.py
file:scripts/ds_pretrain_gpt2.sh
file:scripts/ds_pretrain_gpt2_chinese.sh
file:scripts/pretrain_bert_model_parallel.sh
file:scripts/pretrain_bert_tfrecords_distributed.sh
file:scripts/run_gpt2_eval.py
file:scripts/pretrain_gpt2.sh
file:model/__init__.py
file:model/model.py
file:model/distributed.py
file:model/gpt2_modeling.py
file:model/modeling.py
file:fp16/fp16.py
file:fp16/__init__.py
file:fp16/fp16util.py
file:fp16/loss_scaler.py
file:evaluate_gpt2.py
file:.DS_Store
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:mpu/tests/test_cross_entropy.py
file:mpu/tests/test_layers.py
file:mpu/tests/__init__.py
file:mpu/tests/commons.py
file:mpu/tests/test_data.py
file:mpu/tests/test_initialize.py
file:mpu/tests/test_random.py
file:chinese_sentencepiece/cog-pretrain.model
file:chinese_sentencepiece/cog-pretrain.vocab
file:openwebtext/remove_group_duplicates.py
file:openwebtext/run_make_gpt2_dataset.sh
file:openwebtext/group_duplicates_url.py
file:openwebtext/make_gpt2_sizes.py
file:openwebtext/README.md
file:openwebtext/tokenizer.py
file:openwebtext/make_gpt2_dataset.py
file:openwebtext/merge_jsons.py
file:openwebtext/blacklist_urls.py
file:openwebtext/find_duplicates.py
file:openwebtext/cleanup_dataset.py
file:.pytorch_pretrained_bert/gpt2-merges.txt
file:.pytorch_pretrained_bert/gpt2-vocab.json
file:data_utils/tokenization_gpt2.py
file:data_utils/lazy_loader.py
file:data_utils/wordpiece.py
file:data_utils/datasets.py
file:data_utils/__init__.py
file:data_utils/tokenization.py
file:data_utils/tf_dl.py
file:data_utils/file_utils.py
file:data_utils/sp_tokenizer.py
file:data_utils/corpora.py
file:data_utils/samplers.py
file:data_utils/__pycache__/corpora.cpython-38.pyc
file:data_utils/__pycache__/tokenization_gpt2.cpython-38.pyc
file:data_utils/__pycache__/__init__.cpython-38.pyc
file:data_utils/__pycache__/lazy_loader.cpython-38.pyc
file:data_utils/__pycache__/tokenization.cpython-38.pyc
file:data_utils/__pycache__/file_utils.cpython-38.pyc
file:data_utils/__pycache__/sp_tokenizer.cpython-38.pyc
file:data_utils/__pycache__/wordpiece.cpython-38.pyc
file:data_utils/__pycache__/samplers.cpython-38.pyc
file:data_utils/__pycache__/datasets.cpython-38.pyc
file:.onebrain/aeac8fb5894ab14fb6d0e543436a0563
file:.onebrain/.onebrain.ini
file:scripts/pretrain_bert_sentencepiece.sh
file:scripts/show_rocev2_ipv4_gids.sh
file:scripts/pretrain_bert_distributed.sh
file:scripts/pretrain_bert.sh
file:scripts/ds_checkpoint_check.sh
file:scripts/pretrain_gpt2_distributed.sh
file:scripts/ds_zero2_pretrain_gpt2_model_parallel.sh
file:scripts/ds_zero2_config.json
file:scripts/generate_text.sh
file:scripts/ds_zero-offload_config.json
file:scripts/ds_zero-offload_10B_config.json
file:scripts/ds_onebit_config.json
file:scripts/pretrain_gpt2_model_parallel.sh
file:scripts/extract_hostfile.py
file:scripts/ds_config.json
file:scripts/ds_zero-offload_pretrain_gpt2_model_parallel.sh
file:scripts/presplit_sentences_json.py
file:scripts/ds_zero-offload_10B_pretrain_gpt2_model_parallel.sh
file:scripts/split_json.py
file:scripts/ds_pretrain_gpt2.sh
file:scripts/ds_pretrain_gpt2_chinese.sh
file:scripts/pretrain_bert_model_parallel.sh
file:scripts/pretrain_bert_tfrecords_distributed.sh
file:scripts/run_gpt2_eval.py
file:scripts/pretrain_gpt2.sh
file:model/__init__.py
file:model/model.py
file:model/distributed.py
file:model/gpt2_modeling.py
file:model/modeling.py
file:fp16/fp16.py
file:fp16/__init__.py
file:fp16/fp16util.py
file:fp16/loss_scaler.py
file:evaluate_gpt2.py
file:.DS_Store
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:mpu/tests/test_cross_entropy.py
file:mpu/tests/test_layers.py
file:mpu/tests/__init__.py
file:mpu/tests/commons.py
file:mpu/tests/test_data.py
file:mpu/tests/test_initialize.py
file:mpu/tests/test_random.py
file:chinese_sentencepiece/cog-pretrain.model
file:chinese_sentencepiece/cog-pretrain.vocab
file:openwebtext/remove_group_duplicates.py
file:openwebtext/run_make_gpt2_dataset.sh
file:openwebtext/group_duplicates_url.py
file:openwebtext/make_gpt2_sizes.py
file:openwebtext/README.md
file:openwebtext/tokenizer.py
file:openwebtext/make_gpt2_dataset.py
file:openwebtext/merge_jsons.py
file:openwebtext/blacklist_urls.py
file:openwebtext/find_duplicates.py
file:openwebtext/cleanup_dataset.py
file:.pytorch_pretrained_bert/gpt2-merges.txt
file:.pytorch_pretrained_bert/gpt2-vocab.json
file:data_utils/tokenization_gpt2.py
file:data_utils/lazy_loader.py
file:data_utils/wordpiece.py
file:data_utils/datasets.py
file:data_utils/__init__.py
file:data_utils/tokenization.py
file:data_utils/tf_dl.py
file:data_utils/file_utils.py
file:data_utils/sp_tokenizer.py
file:data_utils/corpora.py
file:data_utils/samplers.py
file:data_utils/__pycache__/corpora.cpython-38.pyc
file:data_utils/__pycache__/tokenization_gpt2.cpython-38.pyc
file:data_utils/__pycache__/__init__.cpython-38.pyc
file:data_utils/__pycache__/lazy_loader.cpython-38.pyc
file:data_utils/__pycache__/tokenization.cpython-38.pyc
file:data_utils/__pycache__/file_utils.cpython-38.pyc
file:data_utils/__pycache__/sp_tokenizer.cpython-38.pyc
file:data_utils/__pycache__/wordpiece.cpython-38.pyc
file:data_utils/__pycache__/samplers.cpython-38.pyc
file:data_utils/__pycache__/datasets.cpython-38.pyc
file:.onebrain/aeac8fb5894ab14fb6d0e543436a0563
file:.onebrain/.onebrain.ini
file:scripts/pretrain_bert_sentencepiece.sh
file:scripts/show_rocev2_ipv4_gids.sh
file:scripts/pretrain_bert_distributed.sh
file:scripts/pretrain_bert.sh
file:scripts/ds_checkpoint_check.sh
file:scripts/pretrain_gpt2_distributed.sh
file:scripts/ds_zero2_pretrain_gpt2_model_parallel.sh
file:scripts/ds_zero2_config.json
file:scripts/generate_text.sh
file:scripts/ds_zero-offload_config.json
file:scripts/ds_zero-offload_10B_config.json
file:scripts/ds_onebit_config.json
file:scripts/pretrain_gpt2_model_parallel.sh
file:scripts/extract_hostfile.py
file:scripts/ds_config.json
file:scripts/ds_zero-offload_pretrain_gpt2_model_parallel.sh
file:scripts/presplit_sentences_json.py
file:scripts/ds_zero-offload_10B_pretrain_gpt2_model_parallel.sh
file:scripts/split_json.py
file:scripts/ds_pretrain_gpt2.sh
file:scripts/ds_pretrain_gpt2_chinese.sh
file:scripts/pretrain_bert_model_parallel.sh
file:scripts/pretrain_bert_tfrecords_distributed.sh
file:scripts/run_gpt2_eval.py
file:scripts/pretrain_gpt2.sh
file:model/__init__.py
file:model/model.py
file:model/distributed.py
file:model/gpt2_modeling.py
file:model/modeling.py
file:fp16/fp16.py
file:fp16/__init__.py
file:fp16/fp16util.py
file:fp16/loss_scaler.py
file:evaluate_gpt2.py
file:.DS_Store
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:mpu/tests/test_cross_entropy.py
file:mpu/tests/test_layers.py
file:mpu/tests/__init__.py
file:mpu/tests/commons.py
file:mpu/tests/test_data.py
file:mpu/tests/test_initialize.py
file:mpu/tests/test_random.py
file:chinese_sentencepiece/cog-pretrain.model
file:chinese_sentencepiece/cog-pretrain.vocab
file:openwebtext/remove_group_duplicates.py
file:openwebtext/run_make_gpt2_dataset.sh
file:openwebtext/group_duplicates_url.py
file:openwebtext/make_gpt2_sizes.py
file:openwebtext/README.md
file:openwebtext/tokenizer.py
file:openwebtext/make_gpt2_dataset.py
file:openwebtext/merge_jsons.py
file:openwebtext/blacklist_urls.py
file:openwebtext/find_duplicates.py
file:openwebtext/cleanup_dataset.py
file:.pytorch_pretrained_bert/gpt2-merges.txt
file:.pytorch_pretrained_bert/gpt2-vocab.json
file:data_utils/tokenization_gpt2.py
file:data_utils/lazy_loader.py
file:data_utils/wordpiece.py
file:data_utils/datasets.py
file:data_utils/__init__.py
file:data_utils/tokenization.py
file:data_utils/tf_dl.py
file:data_utils/file_utils.py
file:data_utils/sp_tokenizer.py
file:data_utils/corpora.py
file:data_utils/samplers.py
file:data_utils/__pycache__/corpora.cpython-38.pyc
file:data_utils/__pycache__/tokenization_gpt2.cpython-38.pyc
file:data_utils/__pycache__/__init__.cpython-38.pyc
file:data_utils/__pycache__/lazy_loader.cpython-38.pyc
file:data_utils/__pycache__/tokenization.cpython-38.pyc
file:data_utils/__pycache__/file_utils.cpython-38.pyc
file:data_utils/__pycache__/sp_tokenizer.cpython-38.pyc
file:data_utils/__pycache__/wordpiece.cpython-38.pyc
file:data_utils/__pycache__/samplers.cpython-38.pyc
file:data_utils/__pycache__/datasets.cpython-38.pyc
file:.onebrain/aeac8fb5894ab14fb6d0e543436a0563
file:.onebrain/.onebrain.ini
file:scripts/pretrain_bert_sentencepiece.sh
file:scripts/show_rocev2_ipv4_gids.sh
file:scripts/pretrain_bert_distributed.sh
file:scripts/pretrain_bert.sh
file:scripts/ds_checkpoint_check.sh
file:scripts/pretrain_gpt2_distributed.sh
file:scripts/ds_zero2_pretrain_gpt2_model_parallel.sh
file:scripts/ds_zero2_config.json
file:scripts/generate_text.sh
file:scripts/ds_zero-offload_config.json
file:scripts/ds_zero-offload_10B_config.json
file:scripts/ds_onebit_config.json
file:scripts/pretrain_gpt2_model_parallel.sh
file:scripts/extract_hostfile.py
file:scripts/ds_config.json
file:scripts/ds_zero-offload_pretrain_gpt2_model_parallel.sh
file:scripts/presplit_sentences_json.py
file:scripts/ds_zero-offload_10B_pretrain_gpt2_model_parallel.sh
file:scripts/split_json.py
file:scripts/ds_pretrain_gpt2.sh
file:scripts/ds_pretrain_gpt2_chinese.sh
file:scripts/pretrain_bert_model_parallel.sh
file:scripts/pretrain_bert_tfrecords_distributed.sh
file:scripts/run_gpt2_eval.py
file:scripts/pretrain_gpt2.sh
file:model/__init__.py
file:model/model.py
file:model/distributed.py
file:model/gpt2_modeling.py
file:model/modeling.py
file:fp16/fp16.py
file:fp16/__init__.py
file:fp16/fp16util.py
file:fp16/loss_scaler.py
file:evaluate_gpt2.py
file:.DS_Store
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:mpu/tests/test_cross_entropy.py
file:mpu/tests/test_layers.py
file:mpu/tests/__init__.py
file:mpu/tests/commons.py
file:mpu/tests/test_data.py
file:mpu/tests/test_initialize.py
file:mpu/tests/test_random.py
file:chinese_sentencepiece/cog-pretrain.model
file:chinese_sentencepiece/cog-pretrain.vocab
file:openwebtext/remove_group_duplicates.py
file:openwebtext/run_make_gpt2_dataset.sh
file:openwebtext/group_duplicates_url.py
file:openwebtext/make_gpt2_sizes.py
file:openwebtext/README.md
file:openwebtext/tokenizer.py
file:openwebtext/make_gpt2_dataset.py
file:openwebtext/merge_jsons.py
file:openwebtext/blacklist_urls.py
file:openwebtext/find_duplicates.py
file:openwebtext/cleanup_dataset.py
file:.pytorch_pretrained_bert/gpt2-merges.txt
file:.pytorch_pretrained_bert/gpt2-vocab.json
file:data_utils/tokenization_gpt2.py
file:data_utils/lazy_loader.py
file:data_utils/wordpiece.py
file:data_utils/datasets.py
file:data_utils/__init__.py
file:data_utils/tokenization.py
file:data_utils/tf_dl.py
file:data_utils/file_utils.py
file:data_utils/sp_tokenizer.py
file:data_utils/corpora.py
file:data_utils/samplers.py
file:data_utils/__pycache__/corpora.cpython-38.pyc
file:data_utils/__pycache__/tokenization_gpt2.cpython-38.pyc
file:data_utils/__pycache__/__init__.cpython-38.pyc
file:data_utils/__pycache__/lazy_loader.cpython-38.pyc
file:data_utils/__pycache__/tokenization.cpython-38.pyc
file:data_utils/__pycache__/file_utils.cpython-38.pyc
file:data_utils/__pycache__/sp_tokenizer.cpython-38.pyc
file:data_utils/__pycache__/wordpiece.cpython-38.pyc
file:data_utils/__pycache__/samplers.cpython-38.pyc
file:data_utils/__pycache__/datasets.cpython-38.pyc
file:.onebrain/aeac8fb5894ab14fb6d0e543436a0563
file:.onebrain/.onebrain.ini
file:scripts/pretrain_bert_sentencepiece.sh
file:scripts/show_rocev2_ipv4_gids.sh
file:scripts/pretrain_bert_distributed.sh
file:scripts/pretrain_bert.sh
file:scripts/ds_checkpoint_check.sh
file:scripts/pretrain_gpt2_distributed.sh
file:scripts/ds_zero2_pretrain_gpt2_model_parallel.sh
file:scripts/ds_zero2_config.json
file:scripts/generate_text.sh
file:scripts/ds_zero-offload_config.json
file:scripts/ds_zero-offload_10B_config.json
file:scripts/ds_onebit_config.json
file:scripts/pretrain_gpt2_model_parallel.sh
file:scripts/extract_hostfile.py
file:scripts/ds_config.json
file:scripts/ds_zero-offload_pretrain_gpt2_model_parallel.sh
file:scripts/presplit_sentences_json.py
file:scripts/ds_zero-offload_10B_pretrain_gpt2_model_parallel.sh
file:scripts/split_json.py
file:scripts/ds_pretrain_gpt2.sh
file:scripts/ds_pretrain_gpt2_chinese.sh
file:scripts/pretrain_bert_model_parallel.sh
file:scripts/pretrain_bert_tfrecords_distributed.sh
file:scripts/run_gpt2_eval.py
file:scripts/pretrain_gpt2.sh
file:model/__init__.py
file:model/model.py
file:model/distributed.py
file:model/gpt2_modeling.py
file:model/modeling.py
file:fp16/fp16.py
file:fp16/__init__.py
file:fp16/fp16util.py
file:fp16/loss_scaler.py
file:evaluate_gpt2.py
file:.DS_Store
file:LICENSE
file:requirements.txt
file:arguments.py
file:generate_samples.py
file:detokenizer.py
file:pretrain_gpt2.py
file:learning_rates.py
file:pretrain_bert.py
file:README.md
file:gpt2_data_loader.py
file:NOTICE.txt
file:utils.py
file:configure_data.py
file:baai-task.yaml
file:docker/requirements.txt
file:docker/Dockerfile
file:docker/README.md
file:mpu/cross_entropy.py
file:mpu/initialize.py
file:mpu/__init__.py
file:mpu/random.py
file:mpu/utils.py
file:mpu/transformer.py
file:mpu/grads.py
file:mpu/layers.py
file:mpu/data.py
file:mpu/mappings.py
file:mpu/tests/test_cross_entropy.py
file:mpu/tests/test_layers.py
file:mpu/tests/__init__.py
file:mpu/tests/commons.py
file:mpu/tests/test_data.py
file:mpu/tests/test_initialize.py
file:mpu/tests/test_random.py
file:chinese_sentencepiece/cog-pretrain.model
file:chinese_sentencepiece/cog-pretrain.vocab
file:openwebtext/remove_group_duplicates.py
file:openwebtext/run_make_gpt2_dataset.sh
file:openwebtext/group_duplicates_url.py
file:openwebtext/make_gpt2_sizes.py
file:openwebtext/README.md
file:openwebtext/tokenizer.py
file:openwebtext/make_gpt2_dataset.py
file:openwebtext/merge_jsons.py
file:openwebtext/blacklist_urls.py
file:openwebtext/find_duplicates.py
file:openwebtext/cleanup_dataset.py
file:.pytorch_pretrained_bert/gpt2-merges.txt
file:.pytorch_pretrained_bert/gpt2-vocab.json
file:data_utils/tokenization_gpt2.py
file:data_utils/lazy_loader.py
file:data_utils/wordpiece.py
file:data_utils/datasets.py
file:data_utils/__init__.py
file:data_utils/tokenization.py
file:data_utils/tf_dl.py
file:data_utils/file_utils.py
file:data_utils/sp_tokenizer.py
file:data_utils/corpora.py
file:data_utils/samplers.py
file:data_utils/__pycache__/corpora.cpython-38.pyc
file:data_utils/__pycache__/tokenization_gpt2.cpython-38.pyc
file:data_utils/__pycache__/__init__.cpython-38.pyc
file:data_utils/__pycache__/lazy_loader.cpython-38.pyc
file:data_utils/__pycache__/tokenization.cpython-38.pyc
file:data_utils/__pycache__/file_utils.cpython-38.pyc
file:data_utils/__pycache__/sp_tokenizer.cpython-38.pyc
file:data_utils/__pycache__/wordpiece.cpython-38.pyc
file:data_utils/__pycache__/samplers.cpython-38.pyc
file:data_utils/__pycache__/datasets.cpython-38.pyc
file:.onebrain/aeac8fb5894ab14fb6d0e543436a0563
file:.onebrain/.onebrain.ini
file:scripts/pretrain_bert_sentencepiece.sh
file:scripts/show_rocev2_ipv4_gids.sh
file:scripts/pretrain_bert_distributed.sh
file:scripts/pretrain_bert.sh
file:scripts/ds_checkpoint_check.sh
file:scripts/pretrain_gpt2_distributed.sh
file:scripts/ds_zero2_pretrain_gpt2_model_parallel.sh
file:scripts/ds_zero2_config.json
file:scripts/generate_text.sh
file:scripts/ds_zero-offload_config.json
file:scripts/ds_zero-offload_10B_config.json
file:scripts/ds_onebit_config.json
file:scripts/pretrain_gpt2_model_parallel.sh
file:scripts/extract_hostfile.py
file:scripts/ds_config.json
file:scripts/ds_zero-offload_pretrain_gpt2_model_parallel.sh
file:scripts/presplit_sentences_json.py
file:scripts/ds_zero-offload_10B_pretrain_gpt2_model_parallel.sh
file:scripts/split_json.py
file:scripts/ds_pretrain_gpt2.sh
file:scripts/ds_pretrain_gpt2_chinese.sh
file:scripts/pretrain_bert_model_parallel.sh
file:scripts/pretrain_bert_tfrecords_distributed.sh
file:scripts/run_gpt2_eval.py
file:scripts/pretrain_gpt2.sh
file:model/__init__.py
file:model/model.py
file:model/distributed.py
file:model/gpt2_modeling.py
file:model/modeling.py
file:fp16/fp16.py
file:fp16/__init__.py
file:fp16/fp16util.py
file:fp16/loss_scaler.py
