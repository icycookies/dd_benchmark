NCCL_DEBUG=info NCCL_IB_DISABLE=0 NCCL_SOCKET_IFNAME=bond0 NCCL_IB_GID_INDEX=3 NCCL_NET_GDR_LEVEL=0 deepspeed --num_nodes 2 --num_gpus 8 --hostfile /root/code/config/hostfile pretrain_gpt2.py --model-parallel-size 1 --num-layers 40 --hidden-size 1408 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --load /root/data/checkpoints/gpt-345M11-14-13-21 --save /root/data/checkpoints --save-interval 2000 --train-iters 320000 --resume-dataloader --train-data zhihu baike zhidao --lazy-loader --tokenizer-type ChineseSPTokenizer --pre-tokenize --split 949,50,1 --distributed-backend nccl --lr-decay-ratio 0.1 --lr-decay-style cosine --warmup .01 --checkpoint-activations --deepspeed-activation-checkpointing --fp16 --deepspeed --deepspeed_config /root/code/Megatron-LM/scripts/ds_config.json
Warning: Permanently added '[9.216.102.34]:2222' (ECDSA) to the list of known hosts.
[2020-11-23 11:47:13,102] [INFO] [runner.py:285:main] Using IP address of 10.16.0.14 for node Tencent01
[2020-11-23 11:47:13,103] [INFO] [multinode_runner.py:51:get_cmd] Running on the following workers: Tencent01,Tencent02
[2020-11-23 11:47:13,103] [INFO] [runner.py:355:main] cmd = pdsh -f 1024 -w Tencent01,Tencent02 export NCCL_DEBUG=info; export NCCL_IB_GID_INDEX=3; export NCCL_NET_GDR_LEVEL=0; export NCCL_SOCKET_IFNAME=bond0; export NCCL_IB_DISABLE=0; export NCCL_INCLUDE_DIR=/usr/include; export NCCL_VERSION=2.7.8; export NCCL_LIBRARY=/usr/lib/x86_64-linux-gnu; export PYTHONPATH=/root/code/Megatron-LM;  cd /root/code/Megatron-LM; /opt/conda/bin/python -u -m deepspeed.launcher.launch --world_info=eyJUZW5jZW50MDEiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN10sICJUZW5jZW50MDIiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --node_rank=%n --master_addr=10.16.0.14 --master_port=29500 pretrain_gpt2.py --model-parallel-size '1' --num-layers '40' --hidden-size '1408' --num-attention-heads '16' --seq-length '1024' --max-position-embeddings '1024' --load '/root/data/checkpoints/gpt-345M11-14-13-21' --save '/root/data/checkpoints' --save-interval '2000' --train-iters '320000' --resume-dataloader --train-data 'zhihu' 'baike' 'zhidao' --lazy-loader --tokenizer-type 'ChineseSPTokenizer' --pre-tokenize --split '949,50,1' --distributed-backend 'nccl' --lr-decay-ratio '0.1' --lr-decay-style 'cosine' --warmup '.01' --checkpoint-activations --deepspeed-activation-checkpointing --fp16 --deepspeed --deepspeed_config '/root/code/Megatron-LM/scripts/ds_config.json'
Tencent02: Warning: Permanently added '[9.216.102.2]:2222' (ECDSA) to the list of known hosts.
Tencent01: Warning: Permanently added '[9.216.102.34]:2222' (ECDSA) to the list of known hosts.
Tencent01: /opt/conda/lib/python3.7/site-packages/deepspeed/ops/sparse_attention/softmax.py:8: UserWarning: Unable to import triton, sparse attention will not be accessible
Tencent01:   warnings.warn("Unable to import triton, sparse attention will not be accessible")
Tencent01: /opt/conda/lib/python3.7/site-packages/deepspeed/ops/sparse_attention/matmul.py:8: UserWarning: Unable to import triton, sparse attention will not be accessible
Tencent01:   warnings.warn("Unable to import triton, sparse attention will not be accessible")
Tencent01: [2020-11-23 11:47:14,421] [INFO] [launch.py:71:main] 0 NCCL_INCLUDE_DIR /usr/include
Tencent01: [2020-11-23 11:47:14,421] [INFO] [launch.py:71:main] 0 NCCL_IB_GID_INDEX 3
Tencent01: [2020-11-23 11:47:14,421] [INFO] [launch.py:71:main] 0 NCCL_IB_DISABLE 0
Tencent01: [2020-11-23 11:47:14,421] [INFO] [launch.py:71:main] 0 NCCL_VERSION 2.7.8
Tencent01: [2020-11-23 11:47:14,421] [INFO] [launch.py:71:main] 0 NCCL_NET_GDR_LEVEL 0
Tencent01: [2020-11-23 11:47:14,421] [INFO] [launch.py:71:main] 0 NCCL_LIBRARY /usr/lib/x86_64-linux-gnu
Tencent01: [2020-11-23 11:47:14,421] [INFO] [launch.py:71:main] 0 NCCL_DEBUG info
Tencent01: [2020-11-23 11:47:14,421] [INFO] [launch.py:71:main] 0 NCCL_SOCKET_IFNAME bond0
Tencent01: [2020-11-23 11:47:14,421] [INFO] [launch.py:78:main] WORLD INFO DICT: {'Tencent01': [0, 1, 2, 3, 4, 5, 6, 7], 'Tencent02': [0, 1, 2, 3, 4, 5, 6, 7]}
Tencent01: [2020-11-23 11:47:14,421] [INFO] [launch.py:87:main] nnodes=2, num_local_procs=8, node_rank=0
Tencent01: [2020-11-23 11:47:14,422] [INFO] [launch.py:99:main] global_rank_mapping=defaultdict(<class 'list'>, {'Tencent01': [0, 1, 2, 3, 4, 5, 6, 7], 'Tencent02': [8, 9, 10, 11, 12, 13, 14, 15]})
Tencent01: [2020-11-23 11:47:14,422] [INFO] [launch.py:100:main] dist_world_size=16
Tencent01: [2020-11-23 11:47:14,422] [INFO] [launch.py:103:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
Tencent02: /opt/conda/lib/python3.7/site-packages/deepspeed/ops/sparse_attention/softmax.py:8: UserWarning: Unable to import triton, sparse attention will not be accessible
Tencent02:   warnings.warn("Unable to import triton, sparse attention will not be accessible")
Tencent02: /opt/conda/lib/python3.7/site-packages/deepspeed/ops/sparse_attention/matmul.py:8: UserWarning: Unable to import triton, sparse attention will not be accessible
Tencent02:   warnings.warn("Unable to import triton, sparse attention will not be accessible")
Tencent02: [2020-11-23 11:47:14,802] [INFO] [launch.py:71:main] 1 NCCL_INCLUDE_DIR /usr/include
Tencent02: [2020-11-23 11:47:14,802] [INFO] [launch.py:71:main] 1 NCCL_IB_GID_INDEX 3
Tencent02: [2020-11-23 11:47:14,802] [INFO] [launch.py:71:main] 1 NCCL_IB_DISABLE 0
Tencent02: [2020-11-23 11:47:14,802] [INFO] [launch.py:71:main] 1 NCCL_VERSION 2.7.8
Tencent02: [2020-11-23 11:47:14,802] [INFO] [launch.py:71:main] 1 NCCL_NET_GDR_LEVEL 0
Tencent02: [2020-11-23 11:47:14,802] [INFO] [launch.py:71:main] 1 NCCL_LIBRARY /usr/lib/x86_64-linux-gnu
Tencent02: [2020-11-23 11:47:14,802] [INFO] [launch.py:71:main] 1 NCCL_DEBUG info
Tencent02: [2020-11-23 11:47:14,802] [INFO] [launch.py:71:main] 1 NCCL_SOCKET_IFNAME bond0
Tencent02: [2020-11-23 11:47:14,802] [INFO] [launch.py:78:main] WORLD INFO DICT: {'Tencent01': [0, 1, 2, 3, 4, 5, 6, 7], 'Tencent02': [0, 1, 2, 3, 4, 5, 6, 7]}
Tencent02: [2020-11-23 11:47:14,802] [INFO] [launch.py:87:main] nnodes=2, num_local_procs=8, node_rank=1
Tencent02: [2020-11-23 11:47:14,802] [INFO] [launch.py:99:main] global_rank_mapping=defaultdict(<class 'list'>, {'Tencent01': [0, 1, 2, 3, 4, 5, 6, 7], 'Tencent02': [8, 9, 10, 11, 12, 13, 14, 15]})
Tencent02: [2020-11-23 11:47:14,802] [INFO] [launch.py:100:main] dist_world_size=16
Tencent02: [2020-11-23 11:47:14,802] [INFO] [launch.py:103:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
Tencent01: /opt/conda/lib/python3.7/site-packages/deepspeed/ops/sparse_attention/softmax.py:8: UserWarning: Unable to import triton, sparse attention will not be accessible
Tencent01:   warnings.warn("Unable to import triton, sparse attention will not be accessible")
Tencent01: /opt/conda/lib/python3.7/site-packages/deepspeed/ops/sparse_attention/softmax.py:8: UserWarning: Unable to import triton, sparse attention will not be accessible
Tencent01:   warnings.warn("Unable to import triton, sparse attention will not be accessible")
Tencent01: /opt/conda/lib/python3.7/site-packages/deepspeed/ops/sparse_attention/matmul.py:8: UserWarning: Unable to import triton, sparse attention will not be accessible
Tencent01:   warnings.warn("Unable to import triton, sparse attention will not be accessible")
Tencent01: /opt/conda/lib/python3.7/site-packages/deepspeed/ops/sparse_attention/matmul.py:8: UserWarning: Unable to import triton, sparse attention will not be accessible
Tencent01:   warnings.warn("Unable to import triton, sparse attention will not be accessible")
Tencent01: /opt/conda/lib/python3.7/site-packages/deepspeed/ops/sparse_attention/softmax.py:8: UserWarning: Unable to import triton, sparse attention will not be accessible
Tencent01:   warnings.warn("Unable to import triton, sparse attention will not be accessible")
Tencent01: /opt/conda/lib/python3.7/site-packages/deepspeed/ops/sparse_attention/softmax.py:8: UserWarning: Unable to import triton, sparse attention will not be accessible
Tencent01:   warnings.warn("Unable to import triton, sparse attention will not be accessible")
Tencent01: /opt/conda/lib/python3.7/site-packages/deepspeed/ops/sparse_attention/softmax.py:8: UserWarning: Unable to import triton, sparse attention will not be accessible
Tencent01:   warnings.warn("Unable to import triton, sparse attention will not be accessible")
Tencent01: /opt/conda/lib/python3.7/site-packages/deepspeed/ops/sparse_attention/matmul.py:8: UserWarning: Unable to import triton, sparse attention will not be accessible
Tencent01:   warnings.warn("Unable to import triton, sparse attention will not be accessible")
Tencent01: /opt/conda/lib/python3.7/site-packages/deepspeed/ops/sparse_attention/matmul.py:8: UserWarning: Unable to import triton, sparse attention will not be accessible
Tencent01:   warnings.warn("Unable to import triton, sparse attention will not be accessible")
Tencent01: /opt/conda/lib/python3.7/site-packages/deepspeed/ops/sparse_attention/matmul.py:8: UserWarning: Unable to import triton, sparse attention will not be accessible
Tencent01:   warnings.warn("Unable to import triton, sparse attention will not be accessible")
Tencent01: /opt/conda/lib/python3.7/site-packages/deepspeed/ops/sparse_attention/softmax.py:8: UserWarning: Unable to import triton, sparse attention will not be accessible
Tencent01:   warnings.warn("Unable to import triton, sparse attention will not be accessible")
Tencent01: /opt/conda/lib/python3.7/site-packages/deepspeed/ops/sparse_attention/matmul.py:8: UserWarning: Unable to import triton, sparse attention will not be accessible
Tencent01:   warnings.warn("Unable to import triton, sparse attention will not be accessible")
Tencent01: /opt/conda/lib/python3.7/site-packages/deepspeed/ops/sparse_attention/softmax.py:8: UserWarning: Unable to import triton, sparse attention will not be accessible
Tencent01:   warnings.warn("Unable to import triton, sparse attention will not be accessible")
Tencent01: /opt/conda/lib/python3.7/site-packages/deepspeed/ops/sparse_attention/matmul.py:8: UserWarning: Unable to import triton, sparse attention will not be accessible
Tencent01:   warnings.warn("Unable to import triton, sparse attention will not be accessible")
Tencent01: /opt/conda/lib/python3.7/site-packages/deepspeed/ops/sparse_attention/softmax.py:8: UserWarning: Unable to import triton, sparse attention will not be accessible
Tencent01:   warnings.warn("Unable to import triton, sparse attention will not be accessible")
Tencent01: /opt/conda/lib/python3.7/site-packages/deepspeed/ops/sparse_attention/matmul.py:8: UserWarning: Unable to import triton, sparse attention will not be accessible
Tencent01:   warnings.warn("Unable to import triton, sparse attention will not be accessible")
Tencent02: /opt/conda/lib/python3.7/site-packages/deepspeed/ops/sparse_attention/softmax.py:8: UserWarning: Unable to import triton, sparse attention will not be accessible
Tencent02:   warnings.warn("Unable to import triton, sparse attention will not be accessible")
Tencent02: /opt/conda/lib/python3.7/site-packages/deepspeed/ops/sparse_attention/softmax.py:8: UserWarning: Unable to import triton, sparse attention will not be accessible
Tencent02:   warnings.warn("Unable to import triton, sparse attention will not be accessible")
Tencent02: /opt/conda/lib/python3.7/site-packages/deepspeed/ops/sparse_attention/softmax.py:8: UserWarning: Unable to import triton, sparse attention will not be accessible
Tencent02:   warnings.warn("Unable to import triton, sparse attention will not be accessible")
Tencent02: /opt/conda/lib/python3.7/site-packages/deepspeed/ops/sparse_attention/softmax.py:8: UserWarning: Unable to import triton, sparse attention will not be accessible
Tencent02:   warnings.warn("Unable to import triton, sparse attention will not be accessible")
Tencent02: /opt/conda/lib/python3.7/site-packages/deepspeed/ops/sparse_attention/softmax.py:8: UserWarning: Unable to import triton, sparse attention will not be accessible
Tencent02:   warnings.warn("Unable to import triton, sparse attention will not be accessible")
Tencent02: /opt/conda/lib/python3.7/site-packages/deepspeed/ops/sparse_attention/softmax.py:8: UserWarning: Unable to import triton, sparse attention will not be accessible
Tencent02:   warnings.warn("Unable to import triton, sparse attention will not be accessible")
Tencent02: /opt/conda/lib/python3.7/site-packages/deepspeed/ops/sparse_attention/matmul.py:8: UserWarning: Unable to import triton, sparse attention will not be accessible
Tencent02:   warnings.warn("Unable to import triton, sparse attention will not be accessible")
Tencent02: /opt/conda/lib/python3.7/site-packages/deepspeed/ops/sparse_attention/matmul.py:8: UserWarning: Unable to import triton, sparse attention will not be accessible
Tencent02:   warnings.warn("Unable to import triton, sparse attention will not be accessible")
Tencent02: /opt/conda/lib/python3.7/site-packages/deepspeed/ops/sparse_attention/matmul.py:8: UserWarning: Unable to import triton, sparse attention will not be accessible
Tencent02:   warnings.warn("Unable to import triton, sparse attention will not be accessible")
Tencent02: /opt/conda/lib/python3.7/site-packages/deepspeed/ops/sparse_attention/matmul.py:8: UserWarning: Unable to import triton, sparse attention will not be accessible
Tencent02:   warnings.warn("Unable to import triton, sparse attention will not be accessible")
Tencent02: /opt/conda/lib/python3.7/site-packages/deepspeed/ops/sparse_attention/matmul.py:8: UserWarning: Unable to import triton, sparse attention will not be accessible
Tencent02:   warnings.warn("Unable to import triton, sparse attention will not be accessible")
Tencent02: /opt/conda/lib/python3.7/site-packages/deepspeed/ops/sparse_attention/matmul.py:8: UserWarning: Unable to import triton, sparse attention will not be accessible
Tencent02:   warnings.warn("Unable to import triton, sparse attention will not be accessible")
Tencent02: /opt/conda/lib/python3.7/site-packages/deepspeed/ops/sparse_attention/softmax.py:8: UserWarning: Unable to import triton, sparse attention will not be accessible
Tencent02:   warnings.warn("Unable to import triton, sparse attention will not be accessible")
Tencent02: /opt/conda/lib/python3.7/site-packages/deepspeed/ops/sparse_attention/matmul.py:8: UserWarning: Unable to import triton, sparse attention will not be accessible
Tencent02:   warnings.warn("Unable to import triton, sparse attention will not be accessible")
Tencent02: /opt/conda/lib/python3.7/site-packages/deepspeed/ops/sparse_attention/softmax.py:8: UserWarning: Unable to import triton, sparse attention will not be accessible
Tencent02:   warnings.warn("Unable to import triton, sparse attention will not be accessible")
Tencent02: /opt/conda/lib/python3.7/site-packages/deepspeed/ops/sparse_attention/matmul.py:8: UserWarning: Unable to import triton, sparse attention will not be accessible
Tencent02:   warnings.warn("Unable to import triton, sparse attention will not be accessible")
Tencent01: using world size: 16 and model-parallel size: 1 
Tencent01:  > using dynamic loss scaling
Tencent01: [2020-11-23 11:47:17,811] [INFO] [checkpointing.py:629:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
Tencent01: [2020-11-23 11:47:17,811] [INFO] [checkpointing.py:629:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
Tencent02: [2020-11-23 11:47:18,792] [INFO] [checkpointing.py:629:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
Tencent02: [2020-11-23 11:47:18,792] [INFO] [checkpointing.py:629:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
Tencent02: [2020-11-23 11:47:18,792] [INFO] [checkpointing.py:629:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
Tencent02: [2020-11-23 11:47:18,792] [INFO] [checkpointing.py:629:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
Tencent02: [2020-11-23 11:47:18,793] [INFO] [checkpointing.py:629:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
Tencent02: [2020-11-23 11:47:18,793] [INFO] [checkpointing.py:629:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
Tencent02: [2020-11-23 11:47:18,792] [INFO] [checkpointing.py:629:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
Tencent02: [2020-11-23 11:47:18,793] [INFO] [checkpointing.py:629:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
Tencent01: [2020-11-23 11:47:18,809] [INFO] [checkpointing.py:629:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
Tencent01: [2020-11-23 11:47:18,809] [INFO] [checkpointing.py:629:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
Tencent01: [2020-11-23 11:47:18,809] [INFO] [checkpointing.py:629:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
Tencent01: [2020-11-23 11:47:18,809] [INFO] [checkpointing.py:629:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
Tencent01: [2020-11-23 11:47:18,810] [INFO] [checkpointing.py:629:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
Tencent01: > initializing model parallel with size 1
Tencent01: [2020-11-23 11:47:18,817] [INFO] [checkpointing.py:629:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
Tencent01: [2020-11-23 11:47:18,817] [INFO] [checkpointing.py:256:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
Tencent01: configuring data
Tencent02: VM-0-9-centos:36:36 [3] NCCL INFO Bootstrap : Using [0]bond0:9.216.102.2<0>
Tencent02: VM-0-9-centos:36:36 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
Tencent02: VM-0-9-centos:36:36 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
Tencent02: VM-0-9-centos:36:36 [3] NCCL INFO NET/IB : Using [0]mlx5_bond_0:1/RoCE ; OOB bond0:9.216.102.2<0>
Tencent02: VM-0-9-centos:36:36 [3] NCCL INFO Using network IB
Tencent02: NCCL version 2.7.8+cuda10.1
Tencent02: VM-0-9-centos:36:92 [3] NCCL INFO Channel 00/32 :    0
Tencent02: VM-0-9-centos:36:92 [3] NCCL INFO Channel 01/32 :    0
Tencent02: VM-0-9-centos:36:92 [3] NCCL INFO Channel 02/32 :    0
Tencent02: VM-0-9-centos:36:92 [3] NCCL INFO Channel 03/32 :    0
Tencent02: VM-0-9-centos:36:92 [3] NCCL INFO Channel 04/32 :    0
Tencent02: VM-0-9-centos:36:92 [3] NCCL INFO Channel 05/32 :    0
Tencent02: VM-0-9-centos:36:92 [3] NCCL INFO Channel 06/32 :    0
Tencent02: VM-0-9-centos:36:92 [3] NCCL INFO Channel 07/32 :    0
Tencent02: VM-0-9-centos:36:92 [3] NCCL INFO Channel 08/32 :    0
Tencent02: VM-0-9-centos:36:92 [3] NCCL INFO Channel 09/32 :    0
Tencent02: VM-0-9-centos:36:92 [3] NCCL INFO Channel 10/32 :    0
Tencent02: VM-0-9-centos:36:92 [3] NCCL INFO Channel 11/32 :    0
Tencent02: VM-0-9-centos:36:92 [3] NCCL INFO Channel 12/32 :    0
Tencent02: VM-0-9-centos:36:92 [3] NCCL INFO Channel 13/32 :    0
Tencent02: VM-0-9-centos:36:92 [3] NCCL INFO Channel 14/32 :    0
Tencent02: VM-0-9-centos:36:92 [3] NCCL INFO Channel 15/32 :    0
Tencent02: VM-0-9-centos:36:92 [3] NCCL INFO Channel 16/32 :    0
Tencent02: VM-0-9-centos:36:92 [3] NCCL INFO Channel 17/32 :    0
Tencent02: VM-0-9-centos:36:92 [3] NCCL INFO Channel 18/32 :    0
Tencent02: VM-0-9-centos:36:92 [3] NCCL INFO Channel 19/32 :    0
Tencent02: VM-0-9-centos:36:92 [3] NCCL INFO Channel 20/32 :    0
Tencent02: VM-0-9-centos:36:92 [3] NCCL INFO Channel 21/32 :    0
Tencent02: VM-0-9-centos:36:92 [3] NCCL INFO Channel 22/32 :    0
Tencent02: VM-0-9-centos:36:92 [3] NCCL INFO Channel 23/32 :    0
Tencent02: VM-0-9-centos:36:92 [3] NCCL INFO Channel 24/32 :    0
Tencent02: VM-0-9-centos:36:92 [3] NCCL INFO Channel 25/32 :    0
Tencent02: VM-0-9-centos:36:92 [3] NCCL INFO Channel 26/32 :    0
Tencent02: VM-0-9-centos:36:92 [3] NCCL INFO Channel 27/32 :    0
Tencent02: VM-0-9-centos:36:92 [3] NCCL INFO Channel 28/32 :    0
Tencent02: VM-0-9-centos:36:92 [3] NCCL INFO Channel 29/32 :    0
Tencent02: VM-0-9-centos:36:92 [3] NCCL INFO Channel 30/32 :    0
Tencent02: VM-0-9-centos:36:92 [3] NCCL INFO Channel 31/32 :    0
Tencent02: VM-0-9-centos:36:92 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0->-1/
Tencent02: VM-0-9-centos:36:92 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff
Tencent02: VM-0-9-centos:36:92 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
Tencent02: VM-0-9-centos:36:92 [3] NCCL INFO comm 0x7fa38c006890 rank 0 nranks 1 cudaDev 3 busId 3e000 - Init COMPLETE
Tencent02: VM-0-9-centos:39:39 [6] NCCL INFO Bootstrap : Using [0]bond0:9.216.102.2<0>
Tencent02: VM-0-9-centos:39:39 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
Tencent02: VM-0-9-centos:39:39 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
Tencent02: VM-0-9-centos:39:39 [6] NCCL INFO NET/IB : Using [0]mlx5_bond_0:1/RoCE ; OOB bond0:9.216.102.2<0>
Tencent02: VM-0-9-centos:39:39 [6] NCCL INFO Using network IB
Tencent02: NCCL version 2.7.8+cuda10.1
Tencent02: VM-0-9-centos:39:97 [6] NCCL INFO Channel 00/32 :    0
Tencent02: VM-0-9-centos:39:97 [6] NCCL INFO Channel 01/32 :    0
Tencent02: VM-0-9-centos:39:97 [6] NCCL INFO Channel 02/32 :    0
Tencent02: VM-0-9-centos:39:97 [6] NCCL INFO Channel 03/32 :    0
Tencent02: VM-0-9-centos:39:97 [6] NCCL INFO Channel 04/32 :    0
Tencent02: VM-0-9-centos:39:97 [6] NCCL INFO Channel 05/32 :    0
Tencent02: VM-0-9-centos:39:97 [6] NCCL INFO Channel 06/32 :    0
Tencent02: VM-0-9-centos:39:97 [6] NCCL INFO Channel 07/32 :    0
Tencent02: VM-0-9-centos:39:97 [6] NCCL INFO Channel 08/32 :    0
Tencent02: VM-0-9-centos:39:97 [6] NCCL INFO Channel 09/32 :    0
Tencent02: VM-0-9-centos:39:97 [6] NCCL INFO Channel 10/32 :    0
Tencent02: VM-0-9-centos:39:97 [6] NCCL INFO Channel 11/32 :    0
Tencent02: VM-0-9-centos:39:97 [6] NCCL INFO Channel 12/32 :    0
Tencent02: VM-0-9-centos:39:97 [6] NCCL INFO Channel 13/32 :    0
Tencent02: VM-0-9-centos:39:97 [6] NCCL INFO Channel 14/32 :    0
Tencent02: VM-0-9-centos:39:97 [6] NCCL INFO Channel 15/32 :    0
Tencent02: VM-0-9-centos:39:97 [6] NCCL INFO Channel 16/32 :    0
Tencent02: VM-0-9-centos:39:97 [6] NCCL INFO Channel 17/32 :    0
Tencent02: VM-0-9-centos:39:97 [6] NCCL INFO Channel 18/32 :    0
Tencent02: VM-0-9-centos:39:97 [6] NCCL INFO Channel 19/32 :    0
Tencent02: VM-0-9-centos:39:97 [6] NCCL INFO Channel 20/32 :    0
Tencent02: VM-0-9-centos:39:97 [6] NCCL INFO Channel 21/32 :    0
Tencent02: VM-0-9-centos:39:97 [6] NCCL INFO Channel 22/32 :    0
Tencent02: VM-0-9-centos:39:97 [6] NCCL INFO Channel 23/32 :    0
Tencent02: VM-0-9-centos:39:97 [6] NCCL INFO Channel 24/32 :    0
Tencent02: VM-0-9-centos:39:97 [6] NCCL INFO Channel 25/32 :    0
Tencent02: VM-0-9-centos:39:97 [6] NCCL INFO Channel 26/32 :    0
Tencent02: VM-0-9-centos:39:97 [6] NCCL INFO Channel 27/32 :    0
Tencent02: VM-0-9-centos:39:97 [6] NCCL INFO Channel 28/32 :    0
Tencent02: VM-0-9-centos:39:97 [6] NCCL INFO Channel 29/32 :    0
Tencent02: VM-0-9-centos:39:97 [6] NCCL INFO Channel 30/32 :    0
Tencent02: VM-0-9-centos:39:97 [6] NCCL INFO Channel 31/32 :    0
Tencent02: VM-0-9-centos:39:97 [6] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0->-1/
Tencent02: VM-0-9-centos:39:97 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000
Tencent02: VM-0-9-centos:39:97 [6] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
Tencent02: VM-0-9-centos:39:97 [6] NCCL INFO comm 0x7fb7c8006890 rank 0 nranks 1 cudaDev 6 busId b1000 - Init COMPLETE
Tencent01: VM-0-14-centos:89:89 [5] NCCL INFO Bootstrap : Using [0]bond0:9.216.102.34<0>
Tencent01: VM-0-14-centos:89:89 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
Tencent01: VM-0-14-centos:89:89 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
Tencent01: VM-0-14-centos:89:89 [5] NCCL INFO NET/IB : Using [0]mlx5_bond_0:1/RoCE ; OOB bond0:9.216.102.34<0>
Tencent01: VM-0-14-centos:89:89 [5] NCCL INFO Using network IB
Tencent01: NCCL version 2.7.8+cuda10.1
Tencent01: VM-0-14-centos:89:145 [5] NCCL INFO Channel 00/32 :    0
Tencent01: VM-0-14-centos:89:145 [5] NCCL INFO Channel 01/32 :    0
Tencent01: VM-0-14-centos:89:145 [5] NCCL INFO Channel 02/32 :    0
Tencent01: VM-0-14-centos:89:145 [5] NCCL INFO Channel 03/32 :    0
Tencent01: VM-0-14-centos:89:145 [5] NCCL INFO Channel 04/32 :    0
Tencent01: VM-0-14-centos:89:145 [5] NCCL INFO Channel 05/32 :    0
Tencent01: VM-0-14-centos:89:145 [5] NCCL INFO Channel 06/32 :    0
Tencent01: VM-0-14-centos:89:145 [5] NCCL INFO Channel 07/32 :    0
Tencent01: VM-0-14-centos:89:145 [5] NCCL INFO Channel 08/32 :    0
Tencent01: VM-0-14-centos:89:145 [5] NCCL INFO Channel 09/32 :    0
Tencent01: VM-0-14-centos:89:145 [5] NCCL INFO Channel 10/32 :    0
Tencent01: VM-0-14-centos:89:145 [5] NCCL INFO Channel 11/32 :    0
Tencent01: VM-0-14-centos:89:145 [5] NCCL INFO Channel 12/32 :    0
Tencent01: VM-0-14-centos:89:145 [5] NCCL INFO Channel 13/32 :    0
Tencent01: VM-0-14-centos:89:145 [5] NCCL INFO Channel 14/32 :    0
Tencent01: VM-0-14-centos:89:145 [5] NCCL INFO Channel 15/32 :    0
Tencent01: VM-0-14-centos:89:145 [5] NCCL INFO Channel 16/32 :    0
Tencent01: VM-0-14-centos:89:145 [5] NCCL INFO Channel 17/32 :    0
Tencent01: VM-0-14-centos:89:145 [5] NCCL INFO Channel 18/32 :    0
Tencent01: VM-0-14-centos:89:145 [5] NCCL INFO Channel 19/32 :    0
Tencent01: VM-0-14-centos:89:145 [5] NCCL INFO Channel 20/32 :    0
Tencent01: VM-0-14-centos:89:145 [5] NCCL INFO Channel 21/32 :    0
Tencent01: VM-0-14-centos:89:145 [5] NCCL INFO Channel 22/32 :    0
Tencent01: VM-0-14-centos:89:145 [5] NCCL INFO Channel 23/32 :    0
Tencent01: VM-0-14-centos:89:145 [5] NCCL INFO Channel 24/32 :    0
Tencent01: VM-0-14-centos:89:145 [5] NCCL INFO Channel 25/32 :    0
Tencent01: VM-0-14-centos:89:145 [5] NCCL INFO Channel 26/32 :    0
Tencent01: VM-0-14-centos:89:145 [5] NCCL INFO Channel 27/32 :    0
Tencent01: VM-0-14-centos:89:145 [5] NCCL INFO Channel 28/32 :    0
Tencent01: VM-0-14-centos:89:145 [5] NCCL INFO Channel 29/32 :    0
Tencent01: VM-0-14-centos:89:145 [5] NCCL INFO Channel 30/32 :    0
Tencent01: VM-0-14-centos:89:145 [5] NCCL INFO Channel 31/32 :    0
Tencent01: VM-0-14-centos:89:145 [5] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0->-
Tencent01: VM-0-14-centos:89:145 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000
Tencent01: VM-0-14-centos:89:145 [5] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
Tencent01: VM-0-14-centos:89:145 [5] NCCL INFO comm 0x7fa7f8006890 rank 0 nranks 1 cudaDev 5 busId 89000 - Init COMPLETE
Tencent01: VM-0-14-centos:91:91 [7] NCCL INFO Bootstrap : Using [0]bond0:9.216.102.34<0>
Tencent01: VM-0-14-centos:91:91 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
Tencent01: VM-0-14-centos:91:91 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
Tencent01: VM-0-14-centos:91:91 [7] NCCL INFO NET/IB : Using [0]mlx5_bond_0:1/RoCE ; OOB bond0:9.216.102.34<0>
Tencent01: VM-0-14-centos:91:91 [7] NCCL INFO Using network IB
Tencent01: NCCL version 2.7.8+cuda10.1
Tencent01: VM-0-14-centos:91:150 [7] NCCL INFO Channel 00/32 :    0
Tencent01: VM-0-14-centos:91:150 [7] NCCL INFO Channel 01/32 :    0
Tencent01: VM-0-14-centos:91:150 [7] NCCL INFO Channel 02/32 :    0
Tencent01: VM-0-14-centos:91:150 [7] NCCL INFO Channel 03/32 :    0
Tencent01: VM-0-14-centos:91:150 [7] NCCL INFO Channel 04/32 :    0
Tencent01: VM-0-14-centos:91:150 [7] NCCL INFO Channel 05/32 :    0
Tencent01: VM-0-14-centos:91:150 [7] NCCL INFO Channel 06/32 :    0
Tencent01: VM-0-14-centos:91:150 [7] NCCL INFO Channel 07/32 :    0
Tencent01: VM-0-14-centos:91:150 [7] NCCL INFO Channel 08/32 :    0
Tencent01: VM-0-14-centos:91:150 [7] NCCL INFO Channel 09/32 :    0
Tencent01: VM-0-14-centos:91:150 [7] NCCL INFO Channel 10/32 :    0
Tencent01: VM-0-14-centos:91:150 [7] NCCL INFO Channel 11/32 :    0
Tencent01: VM-0-14-centos:91:150 [7] NCCL INFO Channel 12/32 :    0
Tencent01: VM-0-14-centos:91:150 [7] NCCL INFO Channel 13/32 :    0
Tencent01: VM-0-14-centos:91:150 [7] NCCL INFO Channel 14/32 :    0
Tencent01: VM-0-14-centos:91:150 [7] NCCL INFO Channel 15/32 :    0
Tencent01: VM-0-14-centos:91:150 [7] NCCL INFO Channel 16/32 :    0
Tencent01: VM-0-14-centos:91:150 [7] NCCL INFO Channel 17/32 :    0
Tencent01: VM-0-14-centos:91:150 [7] NCCL INFO Channel 18/32 :    0
Tencent01: VM-0-14-centos:91:150 [7] NCCL INFO Channel 19/32 :    0
Tencent01: VM-0-14-centos:91:150 [7] NCCL INFO Channel 20/32 :    0
Tencent01: VM-0-14-centos:91:150 [7] NCCL INFO Channel 21/32 :    0
Tencent01: VM-0-14-centos:91:150 [7] NCCL INFO Channel 22/32 :    0
Tencent01: VM-0-14-centos:91:150 [7] NCCL INFO Channel 23/32 :    0
Tencent01: VM-0-14-centos:91:150 [7] NCCL INFO Channel 24/32 :    0
Tencent01: VM-0-14-centos:91:150 [7] NCCL INFO Channel 25/32 :    0
Tencent01: VM-0-14-centos:91:150 [7] NCCL INFO Channel 26/32 :    0
Tencent01: VM-0-14-centos:91:150 [7] NCCL INFO Channel 27/32 :    0
Tencent01: VM-0-14-centos:91:150 [7] NCCL INFO Channel 28/32 :    0
Tencent01: VM-0-14-centos:91:150 [7] NCCL INFO Channel 29/32 :    0
Tencent01: VM-0-14-centos:91:150 [7] NCCL INFO Channel 30/32 :    0
Tencent01: VM-0-14-centos:91:150 [7] NCCL INFO Channel 31/32 :    0
Tencent01: VM-0-14-centos:91:150 [7] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0->-
Tencent01: VM-0-14-centos:91:150 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000
Tencent01: VM-0-14-centos:91:150 [7] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
Tencent01: VM-0-14-centos:91:150 [7] NCCL INFO comm 0x7f20b4006890 rank 0 nranks 1 cudaDev 7 busId b2000 - Init COMPLETE
Tencent02: VM-0-9-centos:38:38 [5] NCCL INFO Bootstrap : Using [0]bond0:9.216.102.2<0>
Tencent02: VM-0-9-centos:38:38 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
Tencent02: VM-0-9-centos:38:38 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
Tencent02: VM-0-9-centos:38:38 [5] NCCL INFO NET/IB : Using [0]mlx5_bond_0:1/RoCE ; OOB bond0:9.216.102.2<0>
Tencent02: VM-0-9-centos:38:38 [5] NCCL INFO Using network IB
Tencent02: NCCL version 2.7.8+cuda10.1
Tencent02: VM-0-9-centos:38:102 [5] NCCL INFO Channel 00/32 :    0
Tencent02: VM-0-9-centos:38:102 [5] NCCL INFO Channel 01/32 :    0
Tencent02: VM-0-9-centos:38:102 [5] NCCL INFO Channel 02/32 :    0
Tencent02: VM-0-9-centos:38:102 [5] NCCL INFO Channel 03/32 :    0
Tencent02: VM-0-9-centos:38:102 [5] NCCL INFO Channel 04/32 :    0
Tencent02: VM-0-9-centos:38:102 [5] NCCL INFO Channel 05/32 :    0
Tencent02: VM-0-9-centos:38:102 [5] NCCL INFO Channel 06/32 :    0
Tencent02: VM-0-9-centos:38:102 [5] NCCL INFO Channel 07/32 :    0
Tencent02: VM-0-9-centos:38:102 [5] NCCL INFO Channel 08/32 :    0
Tencent02: VM-0-9-centos:38:102 [5] NCCL INFO Channel 09/32 :    0
Tencent02: VM-0-9-centos:38:102 [5] NCCL INFO Channel 10/32 :    0
Tencent02: VM-0-9-centos:38:102 [5] NCCL INFO Channel 11/32 :    0
Tencent02: VM-0-9-centos:38:102 [5] NCCL INFO Channel 12/32 :    0
Tencent02: VM-0-9-centos:38:102 [5] NCCL INFO Channel 13/32 :    0
Tencent02: VM-0-9-centos:38:102 [5] NCCL INFO Channel 14/32 :    0
Tencent02: VM-0-9-centos:38:102 [5] NCCL INFO Channel 15/32 :    0
Tencent02: VM-0-9-centos:38:102 [5] NCCL INFO Channel 16/32 :    0
Tencent02: VM-0-9-centos:38:102 [5] NCCL INFO Channel 17/32 :    0
Tencent02: VM-0-9-centos:38:102 [5] NCCL INFO Channel 18/32 :    0
Tencent02: VM-0-9-centos:38:102 [5] NCCL INFO Channel 19/32 :    0
Tencent02: VM-0-9-centos:38:102 [5] NCCL INFO Channel 20/32 :    0
Tencent02: VM-0-9-centos:38:102 [5] NCCL INFO Channel 21/32 :    0
Tencent02: VM-0-9-centos:38:102 [5] NCCL INFO Channel 22/32 :    0
Tencent02: VM-0-9-centos:38:102 [5] NCCL INFO Channel 23/32 :    0
Tencent02: VM-0-9-centos:38:102 [5] NCCL INFO Channel 24/32 :    0
Tencent02: VM-0-9-centos:38:102 [5] NCCL INFO Channel 25/32 :    0
Tencent02: VM-0-9-centos:38:102 [5] NCCL INFO Channel 26/32 :    0
Tencent02: VM-0-9-centos:38:102 [5] NCCL INFO Channel 27/32 :    0
Tencent02: VM-0-9-centos:38:102 [5] NCCL INFO Channel 28/32 :    0
Tencent02: VM-0-9-centos:38:102 [5] NCCL INFO Channel 29/32 :    0
Tencent02: VM-0-9-centos:38:102 [5] NCCL INFO Channel 30/32 :    0
Tencent02: VM-0-9-centos:38:102 [5] NCCL INFO Channel 31/32 :    0
Tencent02: VM-0-9-centos:38:102 [5] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0->-1
Tencent02: VM-0-9-centos:38:102 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000
Tencent02: VM-0-9-centos:38:102 [5] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
Tencent02: VM-0-9-centos:38:102 [5] NCCL INFO comm 0x7f0ef8006890 rank 0 nranks 1 cudaDev 5 busId 89000 - Init COMPLETE
Tencent01: VM-0-14-centos:90:90 [6] NCCL INFO Bootstrap : Using [0]bond0:9.216.102.34<0>
Tencent01: VM-0-14-centos:90:90 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
Tencent01: VM-0-14-centos:90:90 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
Tencent01: VM-0-14-centos:90:90 [6] NCCL INFO NET/IB : Using [0]mlx5_bond_0:1/RoCE ; OOB bond0:9.216.102.34<0>
Tencent01: VM-0-14-centos:90:90 [6] NCCL INFO Using network IB
Tencent01: NCCL version 2.7.8+cuda10.1
Tencent01: VM-0-14-centos:90:154 [6] NCCL INFO Channel 00/32 :    0
Tencent01: VM-0-14-centos:90:154 [6] NCCL INFO Channel 01/32 :    0
Tencent01: VM-0-14-centos:90:154 [6] NCCL INFO Channel 02/32 :    0
Tencent01: VM-0-14-centos:90:154 [6] NCCL INFO Channel 03/32 :    0
Tencent01: VM-0-14-centos:90:154 [6] NCCL INFO Channel 04/32 :    0
Tencent01: VM-0-14-centos:90:154 [6] NCCL INFO Channel 05/32 :    0
Tencent01: VM-0-14-centos:90:154 [6] NCCL INFO Channel 06/32 :    0
Tencent01: VM-0-14-centos:90:154 [6] NCCL INFO Channel 07/32 :    0
Tencent01: VM-0-14-centos:90:154 [6] NCCL INFO Channel 08/32 :    0
Tencent01: VM-0-14-centos:90:154 [6] NCCL INFO Channel 09/32 :    0
Tencent01: VM-0-14-centos:90:154 [6] NCCL INFO Channel 10/32 :    0
Tencent01: VM-0-14-centos:90:154 [6] NCCL INFO Channel 11/32 :    0
Tencent01: VM-0-14-centos:90:154 [6] NCCL INFO Channel 12/32 :    0
Tencent01: VM-0-14-centos:90:154 [6] NCCL INFO Channel 13/32 :    0
Tencent01: VM-0-14-centos:90:154 [6] NCCL INFO Channel 14/32 :    0
Tencent01: VM-0-14-centos:90:154 [6] NCCL INFO Channel 15/32 :    0
Tencent01: VM-0-14-centos:90:154 [6] NCCL INFO Channel 16/32 :    0
Tencent01: VM-0-14-centos:90:154 [6] NCCL INFO Channel 17/32 :    0
Tencent01: VM-0-14-centos:90:154 [6] NCCL INFO Channel 18/32 :    0
Tencent01: VM-0-14-centos:90:154 [6] NCCL INFO Channel 19/32 :    0
Tencent01: VM-0-14-centos:90:154 [6] NCCL INFO Channel 20/32 :    0
Tencent01: VM-0-14-centos:90:154 [6] NCCL INFO Channel 21/32 :    0
Tencent01: VM-0-14-centos:90:154 [6] NCCL INFO Channel 22/32 :    0
Tencent01: VM-0-14-centos:90:154 [6] NCCL INFO Channel 23/32 :    0
Tencent01: VM-0-14-centos:90:154 [6] NCCL INFO Channel 24/32 :    0
Tencent01: VM-0-14-centos:90:154 [6] NCCL INFO Channel 25/32 :    0
Tencent01: VM-0-14-centos:90:154 [6] NCCL INFO Channel 26/32 :    0
Tencent01: VM-0-14-centos:90:154 [6] NCCL INFO Channel 27/32 :    0
Tencent01: VM-0-14-centos:90:154 [6] NCCL INFO Channel 28/32 :    0
Tencent01: VM-0-14-centos:90:154 [6] NCCL INFO Channel 29/32 :    0
Tencent01: VM-0-14-centos:90:154 [6] NCCL INFO Channel 30/32 :    0
Tencent01: VM-0-14-centos:90:154 [6] NCCL INFO Channel 31/32 :    0
Tencent01: VM-0-14-centos:90:154 [6] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0->-
Tencent01: VM-0-14-centos:90:154 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000
Tencent01: VM-0-14-centos:90:154 [6] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
Tencent01: VM-0-14-centos:90:154 [6] NCCL INFO comm 0x7f133c006890 rank 0 nranks 1 cudaDev 6 busId b1000 - Init COMPLETE
Tencent02: VM-0-9-centos:34:34 [1] NCCL INFO Bootstrap : Using [0]bond0:9.216.102.2<0>
Tencent02: VM-0-9-centos:34:34 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
Tencent02: VM-0-9-centos:34:34 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
Tencent02: VM-0-9-centos:34:34 [1] NCCL INFO NET/IB : Using [0]mlx5_bond_0:1/RoCE ; OOB bond0:9.216.102.2<0>
Tencent02: VM-0-9-centos:34:34 [1] NCCL INFO Using network IB
Tencent02: NCCL version 2.7.8+cuda10.1
Tencent02: VM-0-9-centos:34:107 [1] NCCL INFO Channel 00/32 :    0
Tencent02: VM-0-9-centos:34:107 [1] NCCL INFO Channel 01/32 :    0
Tencent02: VM-0-9-centos:34:107 [1] NCCL INFO Channel 02/32 :    0
Tencent02: VM-0-9-centos:34:107 [1] NCCL INFO Channel 03/32 :    0
Tencent02: VM-0-9-centos:34:107 [1] NCCL INFO Channel 04/32 :    0
Tencent02: VM-0-9-centos:34:107 [1] NCCL INFO Channel 05/32 :    0
Tencent02: VM-0-9-centos:34:107 [1] NCCL INFO Channel 06/32 :    0
Tencent02: VM-0-9-centos:34:107 [1] NCCL INFO Channel 07/32 :    0
Tencent02: VM-0-9-centos:34:107 [1] NCCL INFO Channel 08/32 :    0
Tencent02: VM-0-9-centos:34:107 [1] NCCL INFO Channel 09/32 :    0
Tencent02: VM-0-9-centos:34:107 [1] NCCL INFO Channel 10/32 :    0
Tencent02: VM-0-9-centos:34:107 [1] NCCL INFO Channel 11/32 :    0
Tencent02: VM-0-9-centos:34:107 [1] NCCL INFO Channel 12/32 :    0
Tencent02: VM-0-9-centos:34:107 [1] NCCL INFO Channel 13/32 :    0
Tencent02: VM-0-9-centos:34:107 [1] NCCL INFO Channel 14/32 :    0
Tencent02: VM-0-9-centos:34:107 [1] NCCL INFO Channel 15/32 :    0
Tencent02: VM-0-9-centos:34:107 [1] NCCL INFO Channel 16/32 :    0
Tencent02: VM-0-9-centos:34:107 [1] NCCL INFO Channel 17/32 :    0
Tencent02: VM-0-9-centos:34:107 [1] NCCL INFO Channel 18/32 :    0
Tencent02: VM-0-9-centos:34:107 [1] NCCL INFO Channel 19/32 :    0
Tencent02: VM-0-9-centos:34:107 [1] NCCL INFO Channel 20/32 :    0
Tencent02: VM-0-9-centos:34:107 [1] NCCL INFO Channel 21/32 :    0
Tencent02: VM-0-9-centos:34:107 [1] NCCL INFO Channel 22/32 :    0
Tencent02: VM-0-9-centos:34:107 [1] NCCL INFO Channel 23/32 :    0
Tencent02: VM-0-9-centos:34:107 [1] NCCL INFO Channel 24/32 :    0
Tencent02: VM-0-9-centos:34:107 [1] NCCL INFO Channel 25/32 :    0
Tencent02: VM-0-9-centos:34:107 [1] NCCL INFO Channel 26/32 :    0
Tencent02: VM-0-9-centos:34:107 [1] NCCL INFO Channel 27/32 :    0
Tencent02: VM-0-9-centos:34:107 [1] NCCL INFO Channel 28/32 :    0
Tencent02: VM-0-9-centos:34:107 [1] NCCL INFO Channel 29/32 :    0
Tencent02: VM-0-9-centos:34:107 [1] NCCL INFO Channel 30/32 :    0
Tencent02: VM-0-9-centos:34:107 [1] NCCL INFO Channel 31/32 :    0
Tencent02: VM-0-9-centos:34:107 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0->-1
Tencent02: VM-0-9-centos:34:107 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff
Tencent02: VM-0-9-centos:34:107 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
Tencent02: VM-0-9-centos:34:107 [1] NCCL INFO comm 0x7f8ab0006890 rank 0 nranks 1 cudaDev 1 busId 1b000 - Init COMPLETE
Tencent01: VM-0-14-centos:86:86 [2] NCCL INFO Bootstrap : Using [0]bond0:9.216.102.34<0>
Tencent01: VM-0-14-centos:86:86 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
Tencent01: VM-0-14-centos:86:86 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
Tencent01: VM-0-14-centos:86:86 [2] NCCL INFO NET/IB : Using [0]mlx5_bond_0:1/RoCE ; OOB bond0:9.216.102.34<0>
Tencent01: VM-0-14-centos:86:86 [2] NCCL INFO Using network IB
Tencent01: NCCL version 2.7.8+cuda10.1
Tencent01: VM-0-14-centos:86:160 [2] NCCL INFO Channel 00/32 :    0
Tencent01: VM-0-14-centos:86:160 [2] NCCL INFO Channel 01/32 :    0
Tencent01: VM-0-14-centos:86:160 [2] NCCL INFO Channel 02/32 :    0
Tencent01: VM-0-14-centos:86:160 [2] NCCL INFO Channel 03/32 :    0
Tencent01: VM-0-14-centos:86:160 [2] NCCL INFO Channel 04/32 :    0
Tencent01: VM-0-14-centos:86:160 [2] NCCL INFO Channel 05/32 :    0
Tencent01: VM-0-14-centos:86:160 [2] NCCL INFO Channel 06/32 :    0
Tencent01: VM-0-14-centos:86:160 [2] NCCL INFO Channel 07/32 :    0
Tencent01: VM-0-14-centos:86:160 [2] NCCL INFO Channel 08/32 :    0
Tencent01: VM-0-14-centos:86:160 [2] NCCL INFO Channel 09/32 :    0
Tencent01: VM-0-14-centos:86:160 [2] NCCL INFO Channel 10/32 :    0
Tencent01: VM-0-14-centos:86:160 [2] NCCL INFO Channel 11/32 :    0
Tencent01: VM-0-14-centos:86:160 [2] NCCL INFO Channel 12/32 :    0
Tencent01: VM-0-14-centos:86:160 [2] NCCL INFO Channel 13/32 :    0
Tencent01: VM-0-14-centos:86:160 [2] NCCL INFO Channel 14/32 :    0
Tencent01: VM-0-14-centos:86:160 [2] NCCL INFO Channel 15/32 :    0
Tencent01: VM-0-14-centos:86:160 [2] NCCL INFO Channel 16/32 :    0
Tencent01: VM-0-14-centos:86:160 [2] NCCL INFO Channel 17/32 :    0
Tencent01: VM-0-14-centos:86:160 [2] NCCL INFO Channel 18/32 :    0
Tencent01: VM-0-14-centos:86:160 [2] NCCL INFO Channel 19/32 :    0
Tencent01: VM-0-14-centos:86:160 [2] NCCL INFO Channel 20/32 :    0
Tencent01: VM-0-14-centos:86:160 [2] NCCL INFO Channel 21/32 :    0
Tencent01: VM-0-14-centos:86:160 [2] NCCL INFO Channel 22/32 :    0
Tencent01: VM-0-14-centos:86:160 [2] NCCL INFO Channel 23/32 :    0
Tencent01: VM-0-14-centos:86:160 [2] NCCL INFO Channel 24/32 :    0
Tencent01: VM-0-14-centos:86:160 [2] NCCL INFO Channel 25/32 :    0
Tencent01: VM-0-14-centos:86:160 [2] NCCL INFO Channel 26/32 :    0
Tencent01: VM-0-14-centos:86:160 [2] NCCL INFO Channel 27/32 :    0
Tencent01: VM-0-14-centos:86:160 [2] NCCL INFO Channel 28/32 :    0
Tencent01: VM-0-14-centos:86:160 [2] NCCL INFO Channel 29/32 :    0
Tencent01: VM-0-14-centos:86:160 [2] NCCL INFO Channel 30/32 :    0
Tencent01: VM-0-14-centos:86:160 [2] NCCL INFO Channel 31/32 :    0
Tencent01: VM-0-14-centos:86:160 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0->-
Tencent01: VM-0-14-centos:86:160 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff
Tencent01: VM-0-14-centos:86:160 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
Tencent01: VM-0-14-centos:86:160 [2] NCCL INFO comm 0x7eec28006890 rank 0 nranks 1 cudaDev 2 busId 3d000 - Init COMPLETE
Tencent02: VM-0-9-centos:40:40 [7] NCCL INFO Bootstrap : Using [0]bond0:9.216.102.2<0>
Tencent02: VM-0-9-centos:40:40 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
Tencent02: VM-0-9-centos:40:40 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
Tencent02: VM-0-9-centos:40:40 [7] NCCL INFO NET/IB : Using [0]mlx5_bond_0:1/RoCE ; OOB bond0:9.216.102.2<0>
Tencent02: VM-0-9-centos:40:40 [7] NCCL INFO Using network IB
Tencent02: NCCL version 2.7.8+cuda10.1
Tencent02: VM-0-9-centos:40:112 [7] NCCL INFO Channel 00/32 :    0
Tencent02: VM-0-9-centos:40:112 [7] NCCL INFO Channel 01/32 :    0
Tencent02: VM-0-9-centos:40:112 [7] NCCL INFO Channel 02/32 :    0
Tencent02: VM-0-9-centos:40:112 [7] NCCL INFO Channel 03/32 :    0
Tencent02: VM-0-9-centos:40:112 [7] NCCL INFO Channel 04/32 :    0
Tencent02: VM-0-9-centos:40:112 [7] NCCL INFO Channel 05/32 :    0
Tencent02: VM-0-9-centos:40:112 [7] NCCL INFO Channel 06/32 :    0
Tencent02: VM-0-9-centos:40:112 [7] NCCL INFO Channel 07/32 :    0
Tencent02: VM-0-9-centos:40:112 [7] NCCL INFO Channel 08/32 :    0
Tencent02: VM-0-9-centos:40:112 [7] NCCL INFO Channel 09/32 :    0
Tencent02: VM-0-9-centos:40:112 [7] NCCL INFO Channel 10/32 :    0
Tencent02: VM-0-9-centos:40:112 [7] NCCL INFO Channel 11/32 :    0
Tencent02: VM-0-9-centos:40:112 [7] NCCL INFO Channel 12/32 :    0
Tencent02: VM-0-9-centos:40:112 [7] NCCL INFO Channel 13/32 :    0
Tencent02: VM-0-9-centos:40:112 [7] NCCL INFO Channel 14/32 :    0
Tencent02: VM-0-9-centos:40:112 [7] NCCL INFO Channel 15/32 :    0
Tencent02: VM-0-9-centos:40:112 [7] NCCL INFO Channel 16/32 :    0
Tencent02: VM-0-9-centos:40:112 [7] NCCL INFO Channel 17/32 :    0
Tencent02: VM-0-9-centos:40:112 [7] NCCL INFO Channel 18/32 :    0
Tencent02: VM-0-9-centos:40:112 [7] NCCL INFO Channel 19/32 :    0
Tencent02: VM-0-9-centos:40:112 [7] NCCL INFO Channel 20/32 :    0
Tencent02: VM-0-9-centos:40:112 [7] NCCL INFO Channel 21/32 :    0
Tencent02: VM-0-9-centos:40:112 [7] NCCL INFO Channel 22/32 :    0
Tencent02: VM-0-9-centos:40:112 [7] NCCL INFO Channel 23/32 :    0
Tencent02: VM-0-9-centos:40:112 [7] NCCL INFO Channel 24/32 :    0
Tencent02: VM-0-9-centos:40:112 [7] NCCL INFO Channel 25/32 :    0
Tencent02: VM-0-9-centos:40:112 [7] NCCL INFO Channel 26/32 :    0
Tencent02: VM-0-9-centos:40:112 [7] NCCL INFO Channel 27/32 :    0
Tencent02: VM-0-9-centos:40:112 [7] NCCL INFO Channel 28/32 :    0
Tencent02: VM-0-9-centos:40:112 [7] NCCL INFO Channel 29/32 :    0
Tencent02: VM-0-9-centos:40:112 [7] NCCL INFO Channel 30/32 :    0
Tencent02: VM-0-9-centos:40:112 [7] NCCL INFO Channel 31/32 :    0
Tencent02: VM-0-9-centos:40:112 [7] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0->-1
Tencent02: VM-0-9-centos:40:112 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000
Tencent02: VM-0-9-centos:40:112 [7] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
Tencent02: VM-0-9-centos:40:112 [7] NCCL INFO comm 0x7f2f08006890 rank 0 nranks 1 cudaDev 7 busId b2000 - Init COMPLETE
Tencent01: VM-0-14-centos:87:87 [3] NCCL INFO Bootstrap : Using [0]bond0:9.216.102.34<0>
Tencent01: VM-0-14-centos:87:87 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
Tencent01: VM-0-14-centos:87:87 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
Tencent01: VM-0-14-centos:87:87 [3] NCCL INFO NET/IB : Using [0]mlx5_bond_0:1/RoCE ; OOB bond0:9.216.102.34<0>
Tencent01: VM-0-14-centos:87:87 [3] NCCL INFO Using network IB
Tencent01: NCCL version 2.7.8+cuda10.1
Tencent01: VM-0-14-centos:87:164 [3] NCCL INFO Channel 00/32 :    0
Tencent01: VM-0-14-centos:87:164 [3] NCCL INFO Channel 01/32 :    0
Tencent01: VM-0-14-centos:87:164 [3] NCCL INFO Channel 02/32 :    0
Tencent01: VM-0-14-centos:87:164 [3] NCCL INFO Channel 03/32 :    0
Tencent01: VM-0-14-centos:87:164 [3] NCCL INFO Channel 04/32 :    0
Tencent01: VM-0-14-centos:87:164 [3] NCCL INFO Channel 05/32 :    0
Tencent01: VM-0-14-centos:87:164 [3] NCCL INFO Channel 06/32 :    0
Tencent01: VM-0-14-centos:87:164 [3] NCCL INFO Channel 07/32 :    0
Tencent01: VM-0-14-centos:87:164 [3] NCCL INFO Channel 08/32 :    0
Tencent01: VM-0-14-centos:87:164 [3] NCCL INFO Channel 09/32 :    0
Tencent01: VM-0-14-centos:87:164 [3] NCCL INFO Channel 10/32 :    0
Tencent01: VM-0-14-centos:87:164 [3] NCCL INFO Channel 11/32 :    0
Tencent01: VM-0-14-centos:87:164 [3] NCCL INFO Channel 12/32 :    0
Tencent01: VM-0-14-centos:87:164 [3] NCCL INFO Channel 13/32 :    0
Tencent01: VM-0-14-centos:87:164 [3] NCCL INFO Channel 14/32 :    0
Tencent01: VM-0-14-centos:87:164 [3] NCCL INFO Channel 15/32 :    0
Tencent01: VM-0-14-centos:87:164 [3] NCCL INFO Channel 16/32 :    0
Tencent01: VM-0-14-centos:87:164 [3] NCCL INFO Channel 17/32 :    0
Tencent01: VM-0-14-centos:87:164 [3] NCCL INFO Channel 18/32 :    0
Tencent01: VM-0-14-centos:87:164 [3] NCCL INFO Channel 19/32 :    0
Tencent01: VM-0-14-centos:87:164 [3] NCCL INFO Channel 20/32 :    0
Tencent01: VM-0-14-centos:87:164 [3] NCCL INFO Channel 21/32 :    0
Tencent01: VM-0-14-centos:87:164 [3] NCCL INFO Channel 22/32 :    0
Tencent01: VM-0-14-centos:87:164 [3] NCCL INFO Channel 23/32 :    0
Tencent01: VM-0-14-centos:87:164 [3] NCCL INFO Channel 24/32 :    0
Tencent01: VM-0-14-centos:87:164 [3] NCCL INFO Channel 25/32 :    0
Tencent01: VM-0-14-centos:87:164 [3] NCCL INFO Channel 26/32 :    0
Tencent01: VM-0-14-centos:87:164 [3] NCCL INFO Channel 27/32 :    0
Tencent01: VM-0-14-centos:87:164 [3] NCCL INFO Channel 28/32 :    0
Tencent01: VM-0-14-centos:87:164 [3] NCCL INFO Channel 29/32 :    0
Tencent01: VM-0-14-centos:87:164 [3] NCCL INFO Channel 30/32 :    0
Tencent01: VM-0-14-centos:87:164 [3] NCCL INFO Channel 31/32 :    0
Tencent01: VM-0-14-centos:87:164 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0->-
Tencent01: VM-0-14-centos:87:164 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff
Tencent01: VM-0-14-centos:87:164 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
Tencent01: VM-0-14-centos:87:164 [3] NCCL INFO comm 0x7f065c006890 rank 0 nranks 1 cudaDev 3 busId 3e000 - Init COMPLETE
Tencent01: VM-0-14-centos:88:88 [4] NCCL INFO Bootstrap : Using [0]bond0:9.216.102.34<0>
Tencent01: VM-0-14-centos:88:88 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
Tencent01: VM-0-14-centos:88:88 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
Tencent01: VM-0-14-centos:88:88 [4] NCCL INFO NET/IB : Using [0]mlx5_bond_0:1/RoCE ; OOB bond0:9.216.102.34<0>
Tencent01: VM-0-14-centos:88:88 [4] NCCL INFO Using network IB
Tencent01: NCCL version 2.7.8+cuda10.1
Tencent01: VM-0-14-centos:88:169 [4] NCCL INFO Channel 00/32 :    0
Tencent01: VM-0-14-centos:88:169 [4] NCCL INFO Channel 01/32 :    0
Tencent01: VM-0-14-centos:88:169 [4] NCCL INFO Channel 02/32 :    0
Tencent01: VM-0-14-centos:88:169 [4] NCCL INFO Channel 03/32 :    0
Tencent01: VM-0-14-centos:88:169 [4] NCCL INFO Channel 04/32 :    0
Tencent01: VM-0-14-centos:88:169 [4] NCCL INFO Channel 05/32 :    0
Tencent01: VM-0-14-centos:88:169 [4] NCCL INFO Channel 06/32 :    0
Tencent01: VM-0-14-centos:88:169 [4] NCCL INFO Channel 07/32 :    0
Tencent01: VM-0-14-centos:88:169 [4] NCCL INFO Channel 08/32 :    0
Tencent01: VM-0-14-centos:88:169 [4] NCCL INFO Channel 09/32 :    0
Tencent01: VM-0-14-centos:88:169 [4] NCCL INFO Channel 10/32 :    0
Tencent01: VM-0-14-centos:88:169 [4] NCCL INFO Channel 11/32 :    0
Tencent01: VM-0-14-centos:88:169 [4] NCCL INFO Channel 12/32 :    0
Tencent01: VM-0-14-centos:88:169 [4] NCCL INFO Channel 13/32 :    0
Tencent01: VM-0-14-centos:88:169 [4] NCCL INFO Channel 14/32 :    0
Tencent01: VM-0-14-centos:88:169 [4] NCCL INFO Channel 15/32 :    0
Tencent01: VM-0-14-centos:88:169 [4] NCCL INFO Channel 16/32 :    0
Tencent01: VM-0-14-centos:88:169 [4] NCCL INFO Channel 17/32 :    0
Tencent01: VM-0-14-centos:88:169 [4] NCCL INFO Channel 18/32 :    0
Tencent01: VM-0-14-centos:88:169 [4] NCCL INFO Channel 19/32 :    0
Tencent01: VM-0-14-centos:88:169 [4] NCCL INFO Channel 20/32 :    0
Tencent01: VM-0-14-centos:88:169 [4] NCCL INFO Channel 21/32 :    0
Tencent01: VM-0-14-centos:88:169 [4] NCCL INFO Channel 22/32 :    0
Tencent01: VM-0-14-centos:88:169 [4] NCCL INFO Channel 23/32 :    0
Tencent01: VM-0-14-centos:88:169 [4] NCCL INFO Channel 24/32 :    0
Tencent01: VM-0-14-centos:88:169 [4] NCCL INFO Channel 25/32 :    0
Tencent01: VM-0-14-centos:88:169 [4] NCCL INFO Channel 26/32 :    0
Tencent01: VM-0-14-centos:88:169 [4] NCCL INFO Channel 27/32 :    0
Tencent01: VM-0-14-centos:88:169 [4] NCCL INFO Channel 28/32 :    0
Tencent01: VM-0-14-centos:88:169 [4] NCCL INFO Channel 29/32 :    0
Tencent01: VM-0-14-centos:88:169 [4] NCCL INFO Channel 30/32 :    0
Tencent01: VM-0-14-centos:88:169 [4] NCCL INFO Channel 31/32 :    0
Tencent01: VM-0-14-centos:88:169 [4] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0->-
Tencent01: VM-0-14-centos:88:169 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000
Tencent01: VM-0-14-centos:88:169 [4] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
Tencent01: VM-0-14-centos:88:169 [4] NCCL INFO comm 0x7f8a5c006890 rank 0 nranks 1 cudaDev 4 busId 88000 - Init COMPLETE
Tencent02: VM-0-9-centos:37:37 [4] NCCL INFO Bootstrap : Using [0]bond0:9.216.102.2<0>
Tencent02: VM-0-9-centos:37:37 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
Tencent02: VM-0-9-centos:37:37 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
Tencent02: VM-0-9-centos:37:37 [4] NCCL INFO NET/IB : Using [0]mlx5_bond_0:1/RoCE ; OOB bond0:9.216.102.2<0>
Tencent02: VM-0-9-centos:37:37 [4] NCCL INFO Using network IB
Tencent02: NCCL version 2.7.8+cuda10.1
Tencent02: VM-0-9-centos:37:117 [4] NCCL INFO Channel 00/32 :    0
Tencent02: VM-0-9-centos:37:117 [4] NCCL INFO Channel 01/32 :    0
Tencent02: VM-0-9-centos:37:117 [4] NCCL INFO Channel 02/32 :    0
Tencent02: VM-0-9-centos:37:117 [4] NCCL INFO Channel 03/32 :    0
Tencent02: VM-0-9-centos:37:117 [4] NCCL INFO Channel 04/32 :    0
Tencent02: VM-0-9-centos:37:117 [4] NCCL INFO Channel 05/32 :    0
Tencent02: VM-0-9-centos:37:117 [4] NCCL INFO Channel 06/32 :    0
Tencent02: VM-0-9-centos:37:117 [4] NCCL INFO Channel 07/32 :    0
Tencent02: VM-0-9-centos:37:117 [4] NCCL INFO Channel 08/32 :    0
Tencent02: VM-0-9-centos:37:117 [4] NCCL INFO Channel 09/32 :    0
Tencent02: VM-0-9-centos:37:117 [4] NCCL INFO Channel 10/32 :    0
Tencent02: VM-0-9-centos:37:117 [4] NCCL INFO Channel 11/32 :    0
Tencent02: VM-0-9-centos:37:117 [4] NCCL INFO Channel 12/32 :    0
Tencent02: VM-0-9-centos:37:117 [4] NCCL INFO Channel 13/32 :    0
Tencent02: VM-0-9-centos:37:117 [4] NCCL INFO Channel 14/32 :    0
Tencent02: VM-0-9-centos:37:117 [4] NCCL INFO Channel 15/32 :    0
Tencent02: VM-0-9-centos:37:117 [4] NCCL INFO Channel 16/32 :    0
Tencent02: VM-0-9-centos:37:117 [4] NCCL INFO Channel 17/32 :    0
Tencent02: VM-0-9-centos:37:117 [4] NCCL INFO Channel 18/32 :    0
Tencent02: VM-0-9-centos:37:117 [4] NCCL INFO Channel 19/32 :    0
Tencent02: VM-0-9-centos:37:117 [4] NCCL INFO Channel 20/32 :    0
Tencent02: VM-0-9-centos:37:117 [4] NCCL INFO Channel 21/32 :    0
Tencent02: VM-0-9-centos:37:117 [4] NCCL INFO Channel 22/32 :    0
Tencent02: VM-0-9-centos:37:117 [4] NCCL INFO Channel 23/32 :    0
Tencent02: VM-0-9-centos:37:117 [4] NCCL INFO Channel 24/32 :    0
Tencent02: VM-0-9-centos:37:117 [4] NCCL INFO Channel 25/32 :    0
Tencent02: VM-0-9-centos:37:117 [4] NCCL INFO Channel 26/32 :    0
Tencent02: VM-0-9-centos:37:117 [4] NCCL INFO Channel 27/32 :    0
Tencent02: VM-0-9-centos:37:117 [4] NCCL INFO Channel 28/32 :    0
Tencent02: VM-0-9-centos:37:117 [4] NCCL INFO Channel 29/32 :    0
Tencent02: VM-0-9-centos:37:117 [4] NCCL INFO Channel 30/32 :    0
Tencent02: VM-0-9-centos:37:117 [4] NCCL INFO Channel 31/32 :    0
Tencent02: VM-0-9-centos:37:117 [4] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0->-1
Tencent02: VM-0-9-centos:37:117 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000
Tencent02: VM-0-9-centos:37:117 [4] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
Tencent02: VM-0-9-centos:37:117 [4] NCCL INFO comm 0x7f4058006890 rank 0 nranks 1 cudaDev 4 busId 88000 - Init COMPLETE
Tencent02: VM-0-9-centos:35:35 [2] NCCL INFO Bootstrap : Using [0]bond0:9.216.102.2<0>
Tencent02: VM-0-9-centos:35:35 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
Tencent02: VM-0-9-centos:35:35 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
Tencent02: VM-0-9-centos:35:35 [2] NCCL INFO NET/IB : Using [0]mlx5_bond_0:1/RoCE ; OOB bond0:9.216.102.2<0>
Tencent02: VM-0-9-centos:35:35 [2] NCCL INFO Using network IB
Tencent02: NCCL version 2.7.8+cuda10.1
Tencent02: VM-0-9-centos:35:122 [2] NCCL INFO Channel 00/32 :    0
Tencent02: VM-0-9-centos:35:122 [2] NCCL INFO Channel 01/32 :    0
Tencent02: VM-0-9-centos:35:122 [2] NCCL INFO Channel 02/32 :    0
Tencent02: VM-0-9-centos:35:122 [2] NCCL INFO Channel 03/32 :    0
Tencent02: VM-0-9-centos:35:122 [2] NCCL INFO Channel 04/32 :    0
Tencent02: VM-0-9-centos:35:122 [2] NCCL INFO Channel 05/32 :    0
Tencent02: VM-0-9-centos:35:122 [2] NCCL INFO Channel 06/32 :    0
Tencent02: VM-0-9-centos:35:122 [2] NCCL INFO Channel 07/32 :    0
Tencent02: VM-0-9-centos:35:122 [2] NCCL INFO Channel 08/32 :    0
Tencent02: VM-0-9-centos:35:122 [2] NCCL INFO Channel 09/32 :    0
Tencent02: VM-0-9-centos:35:122 [2] NCCL INFO Channel 10/32 :    0
Tencent02: VM-0-9-centos:35:122 [2] NCCL INFO Channel 11/32 :    0
Tencent02: VM-0-9-centos:35:122 [2] NCCL INFO Channel 12/32 :    0
Tencent02: VM-0-9-centos:35:122 [2] NCCL INFO Channel 13/32 :    0
Tencent02: VM-0-9-centos:35:122 [2] NCCL INFO Channel 14/32 :    0
Tencent02: VM-0-9-centos:35:122 [2] NCCL INFO Channel 15/32 :    0
Tencent02: VM-0-9-centos:35:122 [2] NCCL INFO Channel 16/32 :    0
Tencent02: VM-0-9-centos:35:122 [2] NCCL INFO Channel 17/32 :    0
Tencent02: VM-0-9-centos:35:122 [2] NCCL INFO Channel 18/32 :    0
Tencent02: VM-0-9-centos:35:122 [2] NCCL INFO Channel 19/32 :    0
Tencent02: VM-0-9-centos:35:122 [2] NCCL INFO Channel 20/32 :    0
Tencent02: VM-0-9-centos:35:122 [2] NCCL INFO Channel 21/32 :    0
Tencent02: VM-0-9-centos:35:122 [2] NCCL INFO Channel 22/32 :    0
Tencent02: VM-0-9-centos:35:122 [2] NCCL INFO Channel 23/32 :    0
Tencent02: VM-0-9-centos:35:122 [2] NCCL INFO Channel 24/32 :    0
Tencent02: VM-0-9-centos:35:122 [2] NCCL INFO Channel 25/32 :    0
Tencent02: VM-0-9-centos:35:122 [2] NCCL INFO Channel 26/32 :    0
Tencent02: VM-0-9-centos:35:122 [2] NCCL INFO Channel 27/32 :    0
Tencent02: VM-0-9-centos:35:122 [2] NCCL INFO Channel 28/32 :    0
Tencent02: VM-0-9-centos:35:122 [2] NCCL INFO Channel 29/32 :    0
Tencent02: VM-0-9-centos:35:122 [2] NCCL INFO Channel 30/32 :    0
Tencent02: VM-0-9-centos:35:122 [2] NCCL INFO Channel 31/32 :    0
Tencent02: VM-0-9-centos:35:122 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0->-1
Tencent02: VM-0-9-centos:35:122 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff
Tencent02: VM-0-9-centos:35:122 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
Tencent02: VM-0-9-centos:35:122 [2] NCCL INFO comm 0x7f70f4006890 rank 0 nranks 1 cudaDev 2 busId 3d000 - Init COMPLETE
Tencent01: VM-0-14-centos:85:85 [1] NCCL INFO Bootstrap : Using [0]bond0:9.216.102.34<0>
Tencent01: VM-0-14-centos:85:85 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
Tencent01: VM-0-14-centos:85:85 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
Tencent01: VM-0-14-centos:85:85 [1] NCCL INFO NET/IB : Using [0]mlx5_bond_0:1/RoCE ; OOB bond0:9.216.102.34<0>
Tencent01: VM-0-14-centos:85:85 [1] NCCL INFO Using network IB
Tencent01: NCCL version 2.7.8+cuda10.1
Tencent01: VM-0-14-centos:85:174 [1] NCCL INFO Channel 00/32 :    0
Tencent01: VM-0-14-centos:85:174 [1] NCCL INFO Channel 01/32 :    0
Tencent01: VM-0-14-centos:85:174 [1] NCCL INFO Channel 02/32 :    0
Tencent01: VM-0-14-centos:85:174 [1] NCCL INFO Channel 03/32 :    0
Tencent01: VM-0-14-centos:85:174 [1] NCCL INFO Channel 04/32 :    0
Tencent01: VM-0-14-centos:85:174 [1] NCCL INFO Channel 05/32 :    0
Tencent01: VM-0-14-centos:85:174 [1] NCCL INFO Channel 06/32 :    0
Tencent01: VM-0-14-centos:85:174 [1] NCCL INFO Channel 07/32 :    0
Tencent01: VM-0-14-centos:85:174 [1] NCCL INFO Channel 08/32 :    0
Tencent01: VM-0-14-centos:85:174 [1] NCCL INFO Channel 09/32 :    0
Tencent01: VM-0-14-centos:85:174 [1] NCCL INFO Channel 10/32 :    0
Tencent01: VM-0-14-centos:85:174 [1] NCCL INFO Channel 11/32 :    0
Tencent01: VM-0-14-centos:85:174 [1] NCCL INFO Channel 12/32 :    0
Tencent01: VM-0-14-centos:85:174 [1] NCCL INFO Channel 13/32 :    0
Tencent01: VM-0-14-centos:85:174 [1] NCCL INFO Channel 14/32 :    0
Tencent01: VM-0-14-centos:85:174 [1] NCCL INFO Channel 15/32 :    0
Tencent01: VM-0-14-centos:85:174 [1] NCCL INFO Channel 16/32 :    0
Tencent01: VM-0-14-centos:85:174 [1] NCCL INFO Channel 17/32 :    0
Tencent01: VM-0-14-centos:85:174 [1] NCCL INFO Channel 18/32 :    0
Tencent01: VM-0-14-centos:85:174 [1] NCCL INFO Channel 19/32 :    0
Tencent01: VM-0-14-centos:85:174 [1] NCCL INFO Channel 20/32 :    0
Tencent01: VM-0-14-centos:85:174 [1] NCCL INFO Channel 21/32 :    0
Tencent01: VM-0-14-centos:85:174 [1] NCCL INFO Channel 22/32 :    0
Tencent01: VM-0-14-centos:85:174 [1] NCCL INFO Channel 23/32 :    0
Tencent01: VM-0-14-centos:85:174 [1] NCCL INFO Channel 24/32 :    0
Tencent01: VM-0-14-centos:85:174 [1] NCCL INFO Channel 25/32 :    0
Tencent01: VM-0-14-centos:85:174 [1] NCCL INFO Channel 26/32 :    0
Tencent01: VM-0-14-centos:85:174 [1] NCCL INFO Channel 27/32 :    0
Tencent01: VM-0-14-centos:85:174 [1] NCCL INFO Channel 28/32 :    0
Tencent01: VM-0-14-centos:85:174 [1] NCCL INFO Channel 29/32 :    0
Tencent01: VM-0-14-centos:85:174 [1] NCCL INFO Channel 30/32 :    0
Tencent01: VM-0-14-centos:85:174 [1] NCCL INFO Channel 31/32 :    0
Tencent01: VM-0-14-centos:85:174 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0->-
Tencent01: VM-0-14-centos:85:174 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff
Tencent01: VM-0-14-centos:85:174 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
Tencent01: VM-0-14-centos:85:174 [1] NCCL INFO comm 0x7f66ac006890 rank 0 nranks 1 cudaDev 1 busId 1b000 - Init COMPLETE
Tencent02: VM-0-9-centos:33:33 [0] NCCL INFO Bootstrap : Using [0]bond0:9.216.102.2<0>
Tencent02: VM-0-9-centos:33:33 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
Tencent02: VM-0-9-centos:33:33 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
Tencent02: VM-0-9-centos:33:33 [0] NCCL INFO NET/IB : Using [0]mlx5_bond_0:1/RoCE ; OOB bond0:9.216.102.2<0>
Tencent02: VM-0-9-centos:33:33 [0] NCCL INFO Using network IB
Tencent02: NCCL version 2.7.8+cuda10.1
Tencent02: VM-0-9-centos:33:127 [0] NCCL INFO Channel 00/32 :    0
Tencent02: VM-0-9-centos:33:127 [0] NCCL INFO Channel 01/32 :    0
Tencent02: VM-0-9-centos:33:127 [0] NCCL INFO Channel 02/32 :    0
Tencent02: VM-0-9-centos:33:127 [0] NCCL INFO Channel 03/32 :    0
Tencent02: VM-0-9-centos:33:127 [0] NCCL INFO Channel 04/32 :    0
Tencent02: VM-0-9-centos:33:127 [0] NCCL INFO Channel 05/32 :    0
Tencent02: VM-0-9-centos:33:127 [0] NCCL INFO Channel 06/32 :    0
Tencent02: VM-0-9-centos:33:127 [0] NCCL INFO Channel 07/32 :    0
Tencent02: VM-0-9-centos:33:127 [0] NCCL INFO Channel 08/32 :    0
Tencent02: VM-0-9-centos:33:127 [0] NCCL INFO Channel 09/32 :    0
Tencent02: VM-0-9-centos:33:127 [0] NCCL INFO Channel 10/32 :    0
Tencent02: VM-0-9-centos:33:127 [0] NCCL INFO Channel 11/32 :    0
Tencent02: VM-0-9-centos:33:127 [0] NCCL INFO Channel 12/32 :    0
Tencent02: VM-0-9-centos:33:127 [0] NCCL INFO Channel 13/32 :    0
Tencent02: VM-0-9-centos:33:127 [0] NCCL INFO Channel 14/32 :    0
Tencent02: VM-0-9-centos:33:127 [0] NCCL INFO Channel 15/32 :    0
Tencent02: VM-0-9-centos:33:127 [0] NCCL INFO Channel 16/32 :    0
Tencent02: VM-0-9-centos:33:127 [0] NCCL INFO Channel 17/32 :    0
Tencent02: VM-0-9-centos:33:127 [0] NCCL INFO Channel 18/32 :    0
Tencent02: VM-0-9-centos:33:127 [0] NCCL INFO Channel 19/32 :    0
Tencent02: VM-0-9-centos:33:127 [0] NCCL INFO Channel 20/32 :    0
Tencent02: VM-0-9-centos:33:127 [0] NCCL INFO Channel 21/32 :    0
Tencent02: VM-0-9-centos:33:127 [0] NCCL INFO Channel 22/32 :    0
Tencent02: VM-0-9-centos:33:127 [0] NCCL INFO Channel 23/32 :    0
Tencent02: VM-0-9-centos:33:127 [0] NCCL INFO Channel 24/32 :    0
Tencent02: VM-0-9-centos:33:127 [0] NCCL INFO Channel 25/32 :    0
Tencent02: VM-0-9-centos:33:127 [0] NCCL INFO Channel 26/32 :    0
Tencent02: VM-0-9-centos:33:127 [0] NCCL INFO Channel 27/32 :    0
Tencent02: VM-0-9-centos:33:127 [0] NCCL INFO Channel 28/32 :    0
Tencent02: VM-0-9-centos:33:127 [0] NCCL INFO Channel 29/32 :    0
Tencent02: VM-0-9-centos:33:127 [0] NCCL INFO Channel 30/32 :    0
Tencent02: VM-0-9-centos:33:127 [0] NCCL INFO Channel 31/32 :    0
Tencent02: VM-0-9-centos:33:127 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0->-1
Tencent02: VM-0-9-centos:33:127 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff
Tencent02: VM-0-9-centos:33:127 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
Tencent02: VM-0-9-centos:33:127 [0] NCCL INFO comm 0x7ecc08006890 rank 0 nranks 1 cudaDev 0 busId 1a000 - Init COMPLETE
Tencent01: > padded vocab (size: 50001) with 47 dummy tokens (new size: 50048)
Tencent01: > found end-of-document token: 50000
Tencent01: VM-0-14-centos:84:84 [0] NCCL INFO Bootstrap : Using [0]bond0:9.216.102.34<0>
Tencent01: VM-0-14-centos:84:84 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
Tencent01: VM-0-14-centos:84:84 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
Tencent01: VM-0-14-centos:84:84 [0] NCCL INFO NET/IB : Using [0]mlx5_bond_0:1/RoCE ; OOB bond0:9.216.102.34<0>
Tencent01: VM-0-14-centos:84:84 [0] NCCL INFO Using network IB
Tencent01: NCCL version 2.7.8+cuda10.1
Tencent01: VM-0-14-centos:84:179 [0] NCCL INFO Channel 00/32 :    0
Tencent01: VM-0-14-centos:84:179 [0] NCCL INFO Channel 01/32 :    0
Tencent01: VM-0-14-centos:84:179 [0] NCCL INFO Channel 02/32 :    0
Tencent01: VM-0-14-centos:84:179 [0] NCCL INFO Channel 03/32 :    0
Tencent01: VM-0-14-centos:84:179 [0] NCCL INFO Channel 04/32 :    0
Tencent01: VM-0-14-centos:84:179 [0] NCCL INFO Channel 05/32 :    0
Tencent01: VM-0-14-centos:84:179 [0] NCCL INFO Channel 06/32 :    0
Tencent01: VM-0-14-centos:84:179 [0] NCCL INFO Channel 07/32 :    0
Tencent01: VM-0-14-centos:84:179 [0] NCCL INFO Channel 08/32 :    0
Tencent01: VM-0-14-centos:84:179 [0] NCCL INFO Channel 09/32 :    0
Tencent01: VM-0-14-centos:84:179 [0] NCCL INFO Channel 10/32 :    0
Tencent01: VM-0-14-centos:84:179 [0] NCCL INFO Channel 11/32 :    0
Tencent01: VM-0-14-centos:84:179 [0] NCCL INFO Channel 12/32 :    0
Tencent01: VM-0-14-centos:84:179 [0] NCCL INFO Channel 13/32 :    0
Tencent01: VM-0-14-centos:84:179 [0] NCCL INFO Channel 14/32 :    0
Tencent01: VM-0-14-centos:84:179 [0] NCCL INFO Channel 15/32 :    0
Tencent01: VM-0-14-centos:84:179 [0] NCCL INFO Channel 16/32 :    0
Tencent01: VM-0-14-centos:84:179 [0] NCCL INFO Channel 17/32 :    0
Tencent01: VM-0-14-centos:84:179 [0] NCCL INFO Channel 18/32 :    0
Tencent01: VM-0-14-centos:84:179 [0] NCCL INFO Channel 19/32 :    0
Tencent01: VM-0-14-centos:84:179 [0] NCCL INFO Channel 20/32 :    0
Tencent01: VM-0-14-centos:84:179 [0] NCCL INFO Channel 21/32 :    0
Tencent01: VM-0-14-centos:84:179 [0] NCCL INFO Channel 22/32 :    0
Tencent01: VM-0-14-centos:84:179 [0] NCCL INFO Channel 23/32 :    0
Tencent01: VM-0-14-centos:84:179 [0] NCCL INFO Channel 24/32 :    0
Tencent01: VM-0-14-centos:84:179 [0] NCCL INFO Channel 25/32 :    0
Tencent01: VM-0-14-centos:84:179 [0] NCCL INFO Channel 26/32 :    0
Tencent01: VM-0-14-centos:84:179 [0] NCCL INFO Channel 27/32 :    0
Tencent01: VM-0-14-centos:84:179 [0] NCCL INFO Channel 28/32 :    0
Tencent01: VM-0-14-centos:84:179 [0] NCCL INFO Channel 29/32 :    0
Tencent01: VM-0-14-centos:84:179 [0] NCCL INFO Channel 30/32 :    0
Tencent01: VM-0-14-centos:84:179 [0] NCCL INFO Channel 31/32 :    0
Tencent01: VM-0-14-centos:84:179 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0->-
Tencent01: VM-0-14-centos:84:179 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff
Tencent01: VM-0-14-centos:84:179 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
Tencent01: VM-0-14-centos:84:179 [0] NCCL INFO comm 0x7fbe4c006890 rank 0 nranks 1 cudaDev 0 busId 1a000 - Init COMPLETE
Tencent01: building GPT2 model ...
Tencent01:  > number of parameters on model parallel rank 0: 1024227072
Tencent01: DeepSpeed is enabled.
Tencent01: [2020-11-23 11:52:21,717] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed info: version=0.3.0+1afca8f, git-hash=1afca8f, git-branch=HEAD
Tencent01: VM-0-14-centos:91:558 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent01: VM-0-14-centos:91:558 [7] NCCL INFO Trees [0] 0/-1/-1->7->5|5->7->0/-1/-1 [1] 0/-1/-1->7->5|5->7->0/-1/-1
Tencent01: VM-0-14-centos:91:558 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000
Tencent02: VM-0-9-centos:33:509 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent02: VM-0-9-centos:34:508 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent02: VM-0-9-centos:35:511 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent02: VM-0-9-centos:34:508 [1] NCCL INFO Trees [0] 14/-1/-1->9->11|11->9->14/-1/-1 [1] 14/-1/-1->9->11|11->9->14/-1/-1
Tencent02: VM-0-9-centos:33:509 [0] NCCL INFO Trees [0] 10/-1/-1->8->15|15->8->10/-1/-1 [1] 10/-1/-1->8->15|15->8->10/-1/-1
Tencent02: VM-0-9-centos:35:511 [2] NCCL INFO Trees [0] 11/-1/-1->10->8|8->10->11/-1/-1 [1] 11/-1/-1->10->8|8->10->11/-1/-1
Tencent02: VM-0-9-centos:34:508 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff
Tencent02: VM-0-9-centos:33:509 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff
Tencent02: VM-0-9-centos:36:506 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent02: VM-0-9-centos:35:511 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff
Tencent02: VM-0-9-centos:36:506 [3] NCCL INFO Trees [0] 9/-1/-1->11->10|10->11->9/-1/-1 [1] 9/-1/-1->11->10|10->11->9/-1/-1
Tencent02: VM-0-9-centos:36:506 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff
Tencent02: VM-0-9-centos:37:512 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent02: VM-0-9-centos:37:512 [4] NCCL INFO Trees [0] 13/-1/-1->12->5|5->12->13/-1/-1 [1] 13/-1/-1->12->-1|-1->12->13/-1/-1
Tencent02: VM-0-9-centos:37:512 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000
Tencent02: VM-0-9-centos:38:505 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent02: VM-0-9-centos:39:507 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent02: VM-0-9-centos:40:510 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent02: VM-0-9-centos:38:505 [5] NCCL INFO Trees [0] 15/-1/-1->13->12|12->13->15/-1/-1 [1] 15/4/-1->13->12|12->13->15/4/-1
Tencent02: VM-0-9-centos:39:507 [6] NCCL INFO Trees [0] -1/-1/-1->14->9|9->14->-1/-1/-1 [1] -1/-1/-1->14->9|9->14->-1/-1/-1
Tencent02: VM-0-9-centos:40:510 [7] NCCL INFO Trees [0] 8/-1/-1->15->13|13->15->8/-1/-1 [1] 8/-1/-1->15->13|13->15->8/-1/-1
Tencent02: VM-0-9-centos:38:505 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000
Tencent02: VM-0-9-centos:39:507 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000
Tencent02: VM-0-9-centos:40:510 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000
Tencent01: VM-0-14-centos:84:565 [0] NCCL INFO Channel 00/02 :    0   1   3   2   5  12  14  15   8   9  11  10  13   4   6   7
Tencent01: VM-0-14-centos:84:565 [0] NCCL INFO Channel 01/02 :    0   1   3   2   5  12  14  15   8   9  11  10  13   4   6   7
Tencent01: VM-0-14-centos:85:560 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent01: VM-0-14-centos:86:562 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent01: VM-0-14-centos:85:560 [1] NCCL INFO Trees [0] 6/-1/-1->1->3|3->1->6/-1/-1 [1] 6/-1/-1->1->3|3->1->6/-1/-1
Tencent01: VM-0-14-centos:86:562 [2] NCCL INFO Trees [0] 3/-1/-1->2->0|0->2->3/-1/-1 [1] 3/-1/-1->2->0|0->2->3/-1/-1
Tencent01: VM-0-14-centos:85:560 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff
Tencent01: VM-0-14-centos:86:562 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff
Tencent01: VM-0-14-centos:87:564 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent01: VM-0-14-centos:87:564 [3] NCCL INFO Trees [0] 1/-1/-1->3->2|2->3->1/-1/-1 [1] 1/-1/-1->3->2|2->3->1/-1/-1
Tencent01: VM-0-14-centos:89:559 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent01: VM-0-14-centos:88:561 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent01: VM-0-14-centos:87:564 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff
Tencent01: VM-0-14-centos:89:559 [5] NCCL INFO Trees [0] 7/12/-1->5->4|4->5->7/12/-1 [1] 7/-1/-1->5->4|4->5->7/-1/-1
Tencent01: VM-0-14-centos:88:561 [4] NCCL INFO Trees [0] 5/-1/-1->4->-1|-1->4->5/-1/-1 [1] 5/-1/-1->4->13|13->4->5/-1/-1
Tencent01: VM-0-14-centos:84:565 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent01: VM-0-14-centos:89:559 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000
Tencent01: VM-0-14-centos:84:565 [0] NCCL INFO Trees [0] 2/-1/-1->0->7|7->0->2/-1/-1 [1] 2/-1/-1->0->7|7->0->2/-1/-1
Tencent01: VM-0-14-centos:88:561 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000
Tencent01: VM-0-14-centos:90:563 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent01: VM-0-14-centos:84:565 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff
Tencent01: VM-0-14-centos:90:563 [6] NCCL INFO Trees [0] -1/-1/-1->6->1|1->6->-1/-1/-1 [1] -1/-1/-1->6->1|1->6->-1/-1/-1
Tencent01: VM-0-14-centos:90:563 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000
Tencent02: VM-0-9-centos:34:508 [1] NCCL INFO Channel 00 : 9[1b000] -> 11[3e000] via P2P/IPC
Tencent02: VM-0-9-centos:36:506 [3] NCCL INFO Channel 00 : 11[3e000] -> 10[3d000] via P2P/IPC
Tencent02: VM-0-9-centos:33:509 [0] NCCL INFO Channel 00 : 8[1a000] -> 9[1b000] via P2P/IPC
Tencent02: VM-0-9-centos:35:511 [2] NCCL INFO Channel 00 : 10[3d000] -> 13[89000] via P2P/IPC
Tencent01: VM-0-14-centos:91:558 [7] NCCL INFO Channel 00 : 7[b2000] -> 0[1a000] via P2P/IPC
Tencent01: VM-0-14-centos:90:563 [6] NCCL INFO Channel 00 : 6[b1000] -> 7[b2000] via P2P/IPC
Tencent01: VM-0-14-centos:85:560 [1] NCCL INFO Channel 00 : 1[1b000] -> 3[3e000] via P2P/IPC
Tencent01: VM-0-14-centos:86:562 [2] NCCL INFO Channel 00 : 2[3d000] -> 5[89000] via P2P/IPC
Tencent01: VM-0-14-centos:87:564 [3] NCCL INFO Channel 00 : 3[3e000] -> 2[3d000] via P2P/IPC
Tencent01: VM-0-14-centos:84:565 [0] NCCL INFO Channel 00 : 0[1a000] -> 1[1b000] via P2P/IPC
Tencent02: VM-0-9-centos:37:512 [4] NCCL INFO Channel 00 : 5[89000] -> 12[88000] [receive] via NET/IB/0
Tencent02: VM-0-9-centos:38:505 [5] NCCL INFO Channel 00 : 13[89000] -> 4[88000] [send] via NET/IB/0
Tencent02: VM-0-9-centos:39:507 [6] NCCL INFO Channel 00 : 14[b1000] -> 15[b2000] via P2P/IPC
Tencent02: VM-0-9-centos:40:510 [7] NCCL INFO Channel 00 : 15[b2000] -> 8[1a000] via P2P/IPC
Tencent01: VM-0-14-centos:88:561 [4] NCCL INFO Channel 00 : 13[89000] -> 4[88000] [receive] via NET/IB/0
Tencent02: VM-0-9-centos:37:512 [4] NCCL INFO Channel 00 : 12[88000] -> 14[b1000] via P2P/IPC
Tencent02: VM-0-9-centos:35:511 [2] NCCL INFO Channel 00 : 10[3d000] -> 8[1a000] via P2P/IPC
Tencent02: VM-0-9-centos:38:505 [5] NCCL INFO NCCL_IB_GID_INDEX set by environment to 3.
Tencent01: VM-0-14-centos:89:559 [5] NCCL INFO Channel 00 : 5[89000] -> 12[88000] [send] via NET/IB/0
Tencent02: VM-0-9-centos:36:506 [3] NCCL INFO Channel 00 : 11[3e000] -> 9[1b000] via P2P/IPC
Tencent01: VM-0-14-centos:88:561 [4] NCCL INFO Channel 00 : 4[88000] -> 6[b1000] via P2P/IPC
Tencent01: VM-0-14-centos:89:559 [5] NCCL INFO NCCL_IB_GID_INDEX set by environment to 3.
Tencent02: VM-0-9-centos:37:512 [4] NCCL INFO NCCL_IB_GID_INDEX set by environment to 3.
Tencent02: VM-0-9-centos:39:507 [6] NCCL INFO Channel 00 : 14[b1000] -> 9[1b000] via P2P/IPC
Tencent02: VM-0-9-centos:33:509 [0] NCCL INFO Channel 00 : 8[1a000] -> 15[b2000] via P2P/IPC
Tencent01: VM-0-14-centos:86:562 [2] NCCL INFO Channel 00 : 2[3d000] -> 0[1a000] via P2P/IPC
Tencent01: VM-0-14-centos:88:561 [4] NCCL INFO NCCL_IB_GID_INDEX set by environment to 3.
Tencent01: VM-0-14-centos:91:558 [7] NCCL INFO Channel 00 : 7[b2000] -> 5[89000] via P2P/IPC
Tencent02: VM-0-9-centos:40:510 [7] NCCL INFO Channel 00 : 15[b2000] -> 13[89000] via P2P/IPC
Tencent02: VM-0-9-centos:38:505 [5] NCCL INFO Channel 00 : 13[89000] -> 12[88000] via P2P/IPC
Tencent01: VM-0-14-centos:84:565 [0] NCCL INFO Channel 00 : 0[1a000] -> 7[b2000] via P2P/IPC
Tencent01: VM-0-14-centos:87:564 [3] NCCL INFO Channel 00 : 3[3e000] -> 1[1b000] via P2P/IPC
Tencent01: VM-0-14-centos:90:563 [6] NCCL INFO Channel 00 : 6[b1000] -> 1[1b000] via P2P/IPC
Tencent02: VM-0-9-centos:35:511 [2] NCCL INFO Channel 00 : 10[3d000] -> 11[3e000] via P2P/IPC
Tencent02: VM-0-9-centos:33:509 [0] NCCL INFO Channel 00 : 8[1a000] -> 10[3d000] via P2P/IPC
Tencent02: VM-0-9-centos:34:508 [1] NCCL INFO Channel 00 : 9[1b000] -> 14[b1000] via P2P/IPC
Tencent01: VM-0-14-centos:86:562 [2] NCCL INFO Channel 00 : 2[3d000] -> 3[3e000] via P2P/IPC
Tencent02: VM-0-9-centos:37:512 [4] NCCL INFO Channel 00 : 12[88000] -> 5[89000] [send] via NET/IB/0
Tencent01: VM-0-14-centos:84:565 [0] NCCL INFO Channel 00 : 0[1a000] -> 2[3d000] via P2P/IPC
Tencent02: VM-0-9-centos:38:505 [5] NCCL INFO Channel 00 : 13[89000] -> 15[b2000] via P2P/IPC
Tencent01: VM-0-14-centos:85:560 [1] NCCL INFO Channel 00 : 1[1b000] -> 6[b1000] via P2P/IPC
Tencent01: VM-0-14-centos:89:559 [5] NCCL INFO Channel 00 : 12[88000] -> 5[89000] [receive] via NET/IB/0
Tencent02: VM-0-9-centos:36:506 [3] NCCL INFO Channel 01 : 11[3e000] -> 10[3d000] via P2P/IPC
Tencent02: VM-0-9-centos:35:511 [2] NCCL INFO Channel 01 : 10[3d000] -> 13[89000] via P2P/IPC
Tencent02: VM-0-9-centos:33:509 [0] NCCL INFO Channel 01 : 8[1a000] -> 9[1b000] via P2P/IPC
Tencent02: VM-0-9-centos:39:507 [6] NCCL INFO Channel 01 : 14[b1000] -> 15[b2000] via P2P/IPC
Tencent02: VM-0-9-centos:34:508 [1] NCCL INFO Channel 01 : 9[1b000] -> 11[3e000] via P2P/IPC
Tencent01: VM-0-14-centos:89:559 [5] NCCL INFO Channel 00 : 5[89000] -> 4[88000] via P2P/IPC
Tencent02: VM-0-9-centos:37:512 [4] NCCL INFO Channel 00 : 12[88000] -> 13[89000] via P2P/IPC
Tencent02: VM-0-9-centos:40:510 [7] NCCL INFO Channel 01 : 15[b2000] -> 8[1a000] via P2P/IPC
Tencent01: VM-0-14-centos:84:565 [0] NCCL INFO Channel 01 : 0[1a000] -> 1[1b000] via P2P/IPC
Tencent01: VM-0-14-centos:86:562 [2] NCCL INFO Channel 01 : 2[3d000] -> 5[89000] via P2P/IPC
Tencent01: VM-0-14-centos:87:564 [3] NCCL INFO Channel 01 : 3[3e000] -> 2[3d000] via P2P/IPC
Tencent01: VM-0-14-centos:90:563 [6] NCCL INFO Channel 01 : 6[b1000] -> 7[b2000] via P2P/IPC
Tencent02: VM-0-9-centos:36:506 [3] NCCL INFO Channel 01 : 11[3e000] -> 9[1b000] via P2P/IPC
Tencent01: VM-0-14-centos:88:561 [4] NCCL INFO Channel 00 : 4[88000] -> 5[89000] via P2P/IPC
Tencent01: VM-0-14-centos:85:560 [1] NCCL INFO Channel 01 : 1[1b000] -> 3[3e000] via P2P/IPC
Tencent02: VM-0-9-centos:33:509 [0] NCCL INFO Channel 01 : 8[1a000] -> 15[b2000] via P2P/IPC
Tencent02: VM-0-9-centos:40:510 [7] NCCL INFO Channel 01 : 15[b2000] -> 13[89000] via P2P/IPC
Tencent02: VM-0-9-centos:35:511 [2] NCCL INFO Channel 01 : 10[3d000] -> 8[1a000] via P2P/IPC
Tencent02: VM-0-9-centos:33:509 [0] NCCL INFO Channel 01 : 8[1a000] -> 10[3d000] via P2P/IPC
Tencent02: VM-0-9-centos:37:512 [4] NCCL INFO Channel 01 : 5[89000] -> 12[88000] [receive] via NET/IB/0
Tencent02: VM-0-9-centos:38:505 [5] NCCL INFO Channel 01 : 13[89000] -> 4[88000] [send] via NET/IB/0
Tencent02: VM-0-9-centos:33:509 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
Tencent02: VM-0-9-centos:33:509 [0] NCCL INFO comm 0x7ec7dc006890 rank 8 nranks 16 cudaDev 0 busId 1a000 - Init COMPLETE
Tencent02: VM-0-9-centos:35:511 [2] NCCL INFO Channel 01 : 10[3d000] -> 11[3e000] via P2P/IPC
Tencent02: VM-0-9-centos:37:512 [4] NCCL INFO Channel 01 : 12[88000] -> 14[b1000] via P2P/IPC
Tencent01: VM-0-14-centos:87:564 [3] NCCL INFO Channel 01 : 3[3e000] -> 1[1b000] via P2P/IPC
Tencent01: VM-0-14-centos:89:559 [5] NCCL INFO Channel 00 : 5[89000] -> 7[b2000] via P2P/IPC
Tencent02: VM-0-9-centos:35:511 [2] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
Tencent02: VM-0-9-centos:35:511 [2] NCCL INFO comm 0x7f6cc8006890 rank 10 nranks 16 cudaDev 2 busId 3d000 - Init COMPLETE
Tencent02: VM-0-9-centos:39:507 [6] NCCL INFO Channel 01 : 14[b1000] -> 9[1b000] via P2P/IPC
Tencent01: VM-0-14-centos:91:558 [7] NCCL INFO Channel 01 : 7[b2000] -> 0[1a000] via P2P/IPC
Tencent02: VM-0-9-centos:34:508 [1] NCCL INFO Channel 01 : 9[1b000] -> 14[b1000] via P2P/IPC
Tencent02: VM-0-9-centos:36:506 [3] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
Tencent02: VM-0-9-centos:36:506 [3] NCCL INFO comm 0x7fa040006890 rank 11 nranks 16 cudaDev 3 busId 3e000 - Init COMPLETE
Tencent02: VM-0-9-centos:39:507 [6] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
Tencent02: VM-0-9-centos:39:507 [6] NCCL INFO comm 0x7fb3a4006890 rank 14 nranks 16 cudaDev 6 busId b1000 - Init COMPLETE
Tencent01: VM-0-14-centos:86:562 [2] NCCL INFO Channel 01 : 2[3d000] -> 0[1a000] via P2P/IPC
Tencent01: VM-0-14-centos:88:561 [4] NCCL INFO Channel 01 : 13[89000] -> 4[88000] [receive] via NET/IB/0
Tencent02: VM-0-9-centos:34:508 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
Tencent02: VM-0-9-centos:34:508 [1] NCCL INFO comm 0x7f88fc006890 rank 9 nranks 16 cudaDev 1 busId 1b000 - Init COMPLETE
Tencent01: VM-0-14-centos:84:565 [0] NCCL INFO Channel 01 : 0[1a000] -> 7[b2000] via P2P/IPC
Tencent01: VM-0-14-centos:88:561 [4] NCCL INFO Channel 01 : 4[88000] -> 6[b1000] via P2P/IPC
Tencent01: VM-0-14-centos:91:558 [7] NCCL INFO Channel 01 : 7[b2000] -> 5[89000] via P2P/IPC
Tencent01: VM-0-14-centos:89:559 [5] NCCL INFO Channel 01 : 5[89000] -> 12[88000] [send] via NET/IB/0
Tencent01: VM-0-14-centos:86:562 [2] NCCL INFO Channel 01 : 2[3d000] -> 3[3e000] via P2P/IPC
Tencent01: VM-0-14-centos:84:565 [0] NCCL INFO Channel 01 : 0[1a000] -> 2[3d000] via P2P/IPC
Tencent01: VM-0-14-centos:90:563 [6] NCCL INFO Channel 01 : 6[b1000] -> 1[1b000] via P2P/IPC
Tencent02: VM-0-9-centos:38:505 [5] NCCL INFO Channel 01 : 4[88000] -> 13[89000] [receive] via NET/IB/0
Tencent02: VM-0-9-centos:38:505 [5] NCCL INFO Channel 01 : 13[89000] -> 12[88000] via P2P/IPC
Tencent01: VM-0-14-centos:84:565 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
Tencent01: VM-0-14-centos:84:565 [0] NCCL INFO comm 0x7fba1c006890 rank 0 nranks 16 cudaDev 0 busId 1a000 - Init COMPLETE
Tencent01: VM-0-14-centos:84:84 [0] NCCL INFO Launch mode Parallel
Tencent02: VM-0-9-centos:37:512 [4] NCCL INFO Channel 01 : 12[88000] -> 13[89000] via P2P/IPC
Tencent01: VM-0-14-centos:86:562 [2] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
Tencent01: VM-0-14-centos:86:562 [2] NCCL INFO comm 0x7ee8d8006890 rank 2 nranks 16 cudaDev 2 busId 3d000 - Init COMPLETE
Tencent01: VM-0-14-centos:89:559 [5] NCCL INFO Channel 01 : 5[89000] -> 4[88000] via P2P/IPC
Tencent01: VM-0-14-centos:85:560 [1] NCCL INFO Channel 01 : 1[1b000] -> 6[b1000] via P2P/IPC
Tencent01: VM-0-14-centos:87:564 [3] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
Tencent01: VM-0-14-centos:87:564 [3] NCCL INFO comm 0x7f0234006890 rank 3 nranks 16 cudaDev 3 busId 3e000 - Init COMPLETE
Tencent01: VM-0-14-centos:90:563 [6] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
Tencent01: VM-0-14-centos:90:563 [6] NCCL INFO comm 0x7f0fec006890 rank 6 nranks 16 cudaDev 6 busId b1000 - Init COMPLETE
Tencent01: VM-0-14-centos:88:561 [4] NCCL INFO Channel 01 : 4[88000] -> 13[89000] [send] via NET/IB/0
Tencent01: VM-0-14-centos:85:560 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
Tencent01: VM-0-14-centos:85:560 [1] NCCL INFO comm 0x7f635c006890 rank 1 nranks 16 cudaDev 1 busId 1b000 - Init COMPLETE
Tencent02: VM-0-9-centos:37:512 [4] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
Tencent02: VM-0-9-centos:38:505 [5] NCCL INFO Channel 01 : 13[89000] -> 15[b2000] via P2P/IPC
Tencent02: VM-0-9-centos:37:512 [4] NCCL INFO comm 0x7f3d08006890 rank 12 nranks 16 cudaDev 4 busId 88000 - Init COMPLETE
Tencent02: VM-0-9-centos:40:510 [7] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
Tencent02: VM-0-9-centos:40:510 [7] NCCL INFO comm 0x7f2bb8006890 rank 15 nranks 16 cudaDev 7 busId b2000 - Init COMPLETE
Tencent01: VM-0-14-centos:89:559 [5] NCCL INFO Channel 01 : 5[89000] -> 7[b2000] via P2P/IPC
Tencent01: VM-0-14-centos:88:561 [4] NCCL INFO Channel 01 : 4[88000] -> 5[89000] via P2P/IPC
Tencent02: VM-0-9-centos:38:505 [5] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
Tencent01: VM-0-14-centos:88:561 [4] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
Tencent02: VM-0-9-centos:38:505 [5] NCCL INFO comm 0x7f0acc006890 rank 13 nranks 16 cudaDev 5 busId 89000 - Init COMPLETE
Tencent01: VM-0-14-centos:88:561 [4] NCCL INFO comm 0x7f870c006890 rank 4 nranks 16 cudaDev 4 busId 88000 - Init COMPLETE
Tencent01: VM-0-14-centos:91:558 [7] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
Tencent01: VM-0-14-centos:91:558 [7] NCCL INFO comm 0x7f1d64006890 rank 7 nranks 16 cudaDev 7 busId b2000 - Init COMPLETE
Tencent01: VM-0-14-centos:89:559 [5] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
Tencent01: VM-0-14-centos:89:559 [5] NCCL INFO comm 0x7fa4a8006890 rank 5 nranks 16 cudaDev 5 busId 89000 - Init COMPLETE
Tencent01: [2020-11-23 11:52:22,521] [INFO] [engine.py:515:_configure_optimizer] Using DeepSpeed Optimizer param name adam as basic optimizer
Tencent01: [2020-11-23 11:52:22,521] [INFO] [engine.py:518:_configure_optimizer] DeepSpeed Basic Optimizer = FusedAdam (
Tencent01: Parameter Group 0
Tencent01:     betas: (0.9, 0.999)
Tencent01:     bias_correction: True
Tencent01:     eps: 1e-08
Tencent01:     lr: 0.00015
Tencent01:     weight_decay: 0.01
Tencent01: 
Tencent01: Parameter Group 1
Tencent01:     betas: (0.9, 0.999)
Tencent01:     bias_correction: True
Tencent01:     eps: 1e-08
Tencent01:     lr: 0.00015
Tencent01:     weight_decay: 0.0
Tencent01: )
Tencent01: [2020-11-23 11:52:22,521] [INFO] [engine.py:580:_configure_fp16_optimizer] Creating fp16 optimizer with dynamic loss scale
Tencent01: [2020-11-23 11:52:22,521] [INFO] [engine.py:580:_configure_fp16_optimizer] Creating fp16 optimizer with dynamic loss scale
Tencent01: [2020-11-23 11:52:22,522] [INFO] [engine.py:580:_configure_fp16_optimizer] Creating fp16 optimizer with dynamic loss scale
Tencent01: [2020-11-23 11:52:22,523] [INFO] [engine.py:580:_configure_fp16_optimizer] Creating fp16 optimizer with dynamic loss scale
Tencent01: [2020-11-23 11:52:22,523] [INFO] [engine.py:580:_configure_fp16_optimizer] Creating fp16 optimizer with dynamic loss scale
Tencent02: [2020-11-23 11:52:22,525] [INFO] [engine.py:580:_configure_fp16_optimizer] Creating fp16 optimizer with dynamic loss scale
Tencent02: [2020-11-23 11:52:22,525] [INFO] [engine.py:580:_configure_fp16_optimizer] Creating fp16 optimizer with dynamic loss scale
Tencent02: [2020-11-23 11:52:22,525] [INFO] [engine.py:580:_configure_fp16_optimizer] Creating fp16 optimizer with dynamic loss scale
Tencent02: [2020-11-23 11:52:22,525] [INFO] [engine.py:580:_configure_fp16_optimizer] Creating fp16 optimizer with dynamic loss scale
Tencent02: [2020-11-23 11:52:22,525] [INFO] [engine.py:580:_configure_fp16_optimizer] Creating fp16 optimizer with dynamic loss scale
Tencent02: [2020-11-23 11:52:22,526] [INFO] [engine.py:580:_configure_fp16_optimizer] Creating fp16 optimizer with dynamic loss scale
Tencent02: [2020-11-23 11:52:22,526] [INFO] [engine.py:580:_configure_fp16_optimizer] Creating fp16 optimizer with dynamic loss scale
Tencent02: [2020-11-23 11:52:22,527] [INFO] [engine.py:580:_configure_fp16_optimizer] Creating fp16 optimizer with dynamic loss scale
Tencent01: [2020-11-23 11:52:22,527] [INFO] [engine.py:580:_configure_fp16_optimizer] Creating fp16 optimizer with dynamic loss scale
Tencent01: [2020-11-23 11:52:22,527] [INFO] [engine.py:580:_configure_fp16_optimizer] Creating fp16 optimizer with dynamic loss scale
Tencent01: [2020-11-23 11:52:22,527] [INFO] [engine.py:580:_configure_fp16_optimizer] Creating fp16 optimizer with dynamic loss scale
Tencent02: [2020-11-23 11:52:32,313] [INFO] [engine.py:542:_configure_optimizer] DeepSpeed Final Optimizer = FusedAdam (
Tencent02: Parameter Group 0
Tencent02:     betas: (0.9, 0.999)
Tencent02:     bias_correction: True
Tencent02:     eps: 1e-08
Tencent02:     lr: 0.00015
Tencent02:     step: 1
Tencent02:     weight_decay: 0.01
Tencent02: 
Tencent02: Parameter Group 1
Tencent02:     betas: (0.9, 0.999)
Tencent02:     bias_correction: True
Tencent02:     eps: 1e-08
Tencent02:     lr: 0.00015
Tencent02:     step: 1
Tencent02:     weight_decay: 0.0
Tencent02: )
Tencent02: [2020-11-23 11:52:32,377] [INFO] [engine.py:543:_configure_optimizer] DeepSpeed Final Optimizer = {'dynamic_loss_scale': True, 'cur_scale': 4294967296, 'cur_iter': 0, 'last_overflow_iter': -1, 'scale_factor': 2, 'scale_window': 1000, 'optimizer_state_dict': {'state': {0: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:6'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:6')}, 1: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:6'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:6')}}, 'param_groups': [{'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'step': 1, 'params': [0]}, {'weight_decay': 0.0, 'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'step': 1, 'params': [1]}]}, 'fp32_groups_flat': [tensor([-0.0022, -0.0099,  0.0033,  ..., -0.0024, -0.0020, -0.0019],
Tencent02:        device='cuda:6', requires_grad=True), tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:6', requires_grad=True)], 'clip_grad': 1.0}
Tencent02: [2020-11-23 11:52:32,378] [INFO] [engine.py:1224:_load_checkpoint] rank: 14 loading checkpoint: /root/data/checkpoints/gpt-345M11-14-13-21/70000/mp_rank_00_model_states.pt
Tencent01: [2020-11-23 11:52:32,783] [INFO] [engine.py:542:_configure_optimizer] DeepSpeed Final Optimizer = FusedAdam (
Tencent01: Parameter Group 0
Tencent01:     betas: (0.9, 0.999)
Tencent01:     bias_correction: True
Tencent01:     eps: 1e-08
Tencent01:     lr: 0.00015
Tencent01:     step: 1
Tencent01:     weight_decay: 0.01
Tencent01: 
Tencent01: Parameter Group 1
Tencent01:     betas: (0.9, 0.999)
Tencent01:     bias_correction: True
Tencent01:     eps: 1e-08
Tencent01:     lr: 0.00015
Tencent01:     step: 1
Tencent01:     weight_decay: 0.0
Tencent01: )
Tencent01: [2020-11-23 11:52:32,840] [INFO] [engine.py:543:_configure_optimizer] DeepSpeed Final Optimizer = {'dynamic_loss_scale': True, 'cur_scale': 4294967296, 'cur_iter': 0, 'last_overflow_iter': -1, 'scale_factor': 2, 'scale_window': 1000, 'optimizer_state_dict': {'state': {0: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:4'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:4')}, 1: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:4'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:4')}}, 'param_groups': [{'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'step': 1, 'params': [0]}, {'weight_decay': 0.0, 'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'step': 1, 'params': [1]}]}, 'fp32_groups_flat': [tensor([-0.0022, -0.0099,  0.0033,  ..., -0.0024, -0.0020, -0.0019],
Tencent01:        device='cuda:4', requires_grad=True), tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:4', requires_grad=True)], 'clip_grad': 1.0}
Tencent01: [2020-11-23 11:52:32,841] [INFO] [engine.py:1224:_load_checkpoint] rank: 4 loading checkpoint: /root/data/checkpoints/gpt-345M11-14-13-21/70000/mp_rank_00_model_states.pt
Tencent01: [2020-11-23 11:52:32,942] [INFO] [engine.py:542:_configure_optimizer] DeepSpeed Final Optimizer = FusedAdam (
Tencent01: Parameter Group 0
Tencent01:     betas: (0.9, 0.999)
Tencent01:     bias_correction: True
Tencent01:     eps: 1e-08
Tencent01:     lr: 0.00015
Tencent01:     step: 1
Tencent01:     weight_decay: 0.01
Tencent01: 
Tencent01: Parameter Group 1
Tencent01:     betas: (0.9, 0.999)
Tencent01:     bias_correction: True
Tencent01:     eps: 1e-08
Tencent01:     lr: 0.00015
Tencent01:     step: 1
Tencent01:     weight_decay: 0.0
Tencent01: )
Tencent01: [2020-11-23 11:52:32,997] [INFO] [engine.py:543:_configure_optimizer] DeepSpeed Final Optimizer = {'dynamic_loss_scale': True, 'cur_scale': 4294967296, 'cur_iter': 0, 'last_overflow_iter': -1, 'scale_factor': 2, 'scale_window': 1000, 'optimizer_state_dict': {'state': {0: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 1: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}}, 'param_groups': [{'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'step': 1, 'params': [0]}, {'weight_decay': 0.0, 'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'step': 1, 'params': [1]}]}, 'fp32_groups_flat': [tensor([-0.0022, -0.0099,  0.0033,  ..., -0.0024, -0.0020, -0.0019],
Tencent01:        device='cuda:0', requires_grad=True), tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:0', requires_grad=True)], 'clip_grad': 1.0}
Tencent01: [2020-11-23 11:52:32,997] [INFO] [engine.py:382:_configure_lr_scheduler] DeepSpeed using client LR scheduler
Tencent01: [2020-11-23 11:52:32,997] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
Tencent01: [2020-11-23 11:52:32,997] [INFO] [logging.py:60:log_dist] [Rank 0] step=0, skipped=0, lr=[0.00015, 0.00015], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 11:52:32,997] [INFO] [config.py:620:print] DeepSpeedLight configuration:
Tencent01: [2020-11-23 11:52:32,998] [INFO] [config.py:624:print]   activation_checkpointing_config  <deepspeed.runtime.activation_checkpointing.config.DeepSpeedActivationCheckpointingConfig object at 0x7ff79cc777d0>
Tencent01: [2020-11-23 11:52:32,998] [INFO] [config.py:624:print]   allreduce_always_fp32 ........ False
Tencent01: [2020-11-23 11:52:32,998] [INFO] [config.py:624:print]   amp_enabled .................. False
Tencent01: [2020-11-23 11:52:32,998] [INFO] [config.py:624:print]   amp_params ................... False
Tencent01: [2020-11-23 11:52:32,998] [INFO] [config.py:624:print]   disable_allgather ............ False
Tencent01: [2020-11-23 11:52:32,998] [INFO] [config.py:624:print]   dump_state ................... False
Tencent01: [2020-11-23 11:52:32,998] [INFO] [config.py:624:print]   dynamic_loss_scale_args ...... {'init_scale': 4294967296, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}
Tencent01: [2020-11-23 11:52:32,998] [INFO] [config.py:624:print]   fp16_enabled ................. True
Tencent01: [2020-11-23 11:52:32,998] [INFO] [config.py:624:print]   global_rank .................. 0
Tencent01: [2020-11-23 11:52:32,998] [INFO] [config.py:624:print]   gradient_accumulation_steps .. 4
Tencent01: [2020-11-23 11:52:32,998] [INFO] [config.py:624:print]   gradient_clipping ............ 1.0
Tencent01: [2020-11-23 11:52:32,998] [INFO] [config.py:624:print]   gradient_predivide_factor .... 1.0
Tencent01: [2020-11-23 11:52:32,998] [INFO] [config.py:624:print]   initial_dynamic_scale ........ 4294967296
Tencent01: [2020-11-23 11:52:32,998] [INFO] [config.py:624:print]   loss_scale ................... 0
Tencent01: [2020-11-23 11:52:32,998] [INFO] [config.py:624:print]   memory_breakdown ............. False
Tencent01: [2020-11-23 11:52:32,998] [INFO] [config.py:624:print]   optimizer_legacy_fusion ...... False
Tencent01: [2020-11-23 11:52:32,998] [INFO] [config.py:624:print]   optimizer_name ............... adam
Tencent01: [2020-11-23 11:52:32,998] [INFO] [config.py:624:print]   optimizer_params ............. {'lr': 0.00015, 'weight_decay': 0.01}
Tencent01: [2020-11-23 11:52:32,998] [INFO] [config.py:624:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
Tencent01: [2020-11-23 11:52:32,998] [INFO] [config.py:624:print]   prescale_gradients ........... False
Tencent01: [2020-11-23 11:52:32,998] [INFO] [config.py:624:print]   scheduler_name ............... None
Tencent01: [2020-11-23 11:52:32,998] [INFO] [config.py:624:print]   scheduler_params ............. None
Tencent01: [2020-11-23 11:52:32,998] [INFO] [config.py:624:print]   sparse_attention ............. None
Tencent01: [2020-11-23 11:52:32,998] [INFO] [config.py:624:print]   sparse_gradients_enabled ..... False
Tencent01: [2020-11-23 11:52:32,998] [INFO] [config.py:624:print]   steps_per_print .............. 100
Tencent01: [2020-11-23 11:52:32,998] [INFO] [config.py:624:print]   tensorboard_enabled .......... False
Tencent01: [2020-11-23 11:52:32,998] [INFO] [config.py:624:print]   tensorboard_job_name ......... DeepSpeedJobName
Tencent01: [2020-11-23 11:52:32,998] [INFO] [config.py:624:print]   tensorboard_output_path ...... 
Tencent01: [2020-11-23 11:52:32,998] [INFO] [config.py:624:print]   train_batch_size ............. 512
Tencent01: [2020-11-23 11:52:32,998] [INFO] [config.py:624:print]   train_micro_batch_size_per_gpu  8
Tencent01: [2020-11-23 11:52:32,998] [INFO] [config.py:624:print]   wall_clock_breakdown ......... False
Tencent01: [2020-11-23 11:52:32,998] [INFO] [config.py:624:print]   world_size ................... 16
Tencent01: [2020-11-23 11:52:32,998] [INFO] [config.py:624:print]   zero_allow_untested_optimizer  False
Tencent01: [2020-11-23 11:52:32,998] [INFO] [config.py:624:print]   zero_config .................. <deepspeed.runtime.zero.config.DeepSpeedZeroConfig object at 0x7ff79cc77790>
Tencent01: [2020-11-23 11:52:32,998] [INFO] [config.py:624:print]   zero_enabled ................. False
Tencent01: [2020-11-23 11:52:32,998] [INFO] [config.py:624:print]   zero_optimization_stage ...... 0
Tencent01: [2020-11-23 11:52:32,999] [INFO] [config.py:630:print]   json = {
Tencent01:     "activation_checkpointing":{
Tencent01:         "contiguous_memory_optimization":false,
Tencent01:         "partition_activations":false
Tencent01:     },
Tencent01:     "fp16":{
Tencent01:         "enabled":true,
Tencent01:         "hysteresis":2,
Tencent01:         "loss_scale":0,
Tencent01:         "loss_scale_window":1000,
Tencent01:         "min_loss_scale":1
Tencent01:     },
Tencent01:     "gradient_accumulation_steps":4,
Tencent01:     "gradient_clipping":1.0,
Tencent01:     "optimizer":{
Tencent01:         "params":{
Tencent01:             "lr":0.00015,
Tencent01:             "weight_decay":0.01
Tencent01:         },
Tencent01:         "type":"Adam"
Tencent01:     },
Tencent01:     "steps_per_print":100,
Tencent01:     "train_micro_batch_size_per_gpu":8,
Tencent01:     "wall_clock_breakdown":false
Tencent01: }
Tencent01: learning rate decaying style cosine, ratio 10.0
Tencent02: [2020-11-23 11:52:33,014] [INFO] [engine.py:542:_configure_optimizer] DeepSpeed Final Optimizer = FusedAdam (
Tencent02: Parameter Group 0
Tencent02:     betas: (0.9, 0.999)
Tencent02:     bias_correction: True
Tencent02:     eps: 1e-08
Tencent02:     lr: 0.00015
Tencent02:     step: 1
Tencent02:     weight_decay: 0.01
Tencent02: 
Tencent02: Parameter Group 1
Tencent02:     betas: (0.9, 0.999)
Tencent02:     bias_correction: True
Tencent02:     eps: 1e-08
Tencent02:     lr: 0.00015
Tencent02:     step: 1
Tencent02:     weight_decay: 0.0
Tencent02: )
Tencent02: [2020-11-23 11:52:33,067] [INFO] [engine.py:543:_configure_optimizer] DeepSpeed Final Optimizer = {'dynamic_loss_scale': True, 'cur_scale': 4294967296, 'cur_iter': 0, 'last_overflow_iter': -1, 'scale_factor': 2, 'scale_window': 1000, 'optimizer_state_dict': {'state': {0: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:7'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:7')}, 1: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:7'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:7')}}, 'param_groups': [{'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'step': 1, 'params': [0]}, {'weight_decay': 0.0, 'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'step': 1, 'params': [1]}]}, 'fp32_groups_flat': [tensor([-0.0022, -0.0099,  0.0033,  ..., -0.0024, -0.0020, -0.0019],
Tencent02:        device='cuda:7', requires_grad=True), tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:7', requires_grad=True)], 'clip_grad': 1.0}
Tencent02: [2020-11-23 11:52:33,092] [INFO] [engine.py:542:_configure_optimizer] DeepSpeed Final Optimizer = FusedAdam (
Tencent02: Parameter Group 0
Tencent02:     betas: (0.9, 0.999)
Tencent02:     bias_correction: True
Tencent02:     eps: 1e-08
Tencent02:     lr: 0.00015
Tencent02:     step: 1
Tencent02:     weight_decay: 0.01
Tencent02: 
Tencent02: Parameter Group 1
Tencent02:     betas: (0.9, 0.999)
Tencent02:     bias_correction: True
Tencent02:     eps: 1e-08
Tencent02:     lr: 0.00015
Tencent02:     step: 1
Tencent02:     weight_decay: 0.0
Tencent02: )
Tencent02: [2020-11-23 11:52:33,147] [INFO] [engine.py:543:_configure_optimizer] DeepSpeed Final Optimizer = {'dynamic_loss_scale': True, 'cur_scale': 4294967296, 'cur_iter': 0, 'last_overflow_iter': -1, 'scale_factor': 2, 'scale_window': 1000, 'optimizer_state_dict': {'state': {0: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:5'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:5')}, 1: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:5'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:5')}}, 'param_groups': [{'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'step': 1, 'params': [0]}, {'weight_decay': 0.0, 'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'step': 1, 'params': [1]}]}, 'fp32_groups_flat': [tensor([-0.0022, -0.0099,  0.0033,  ..., -0.0024, -0.0020, -0.0019],
Tencent02:        device='cuda:5', requires_grad=True), tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:5', requires_grad=True)], 'clip_grad': 1.0}
Tencent01: [2020-11-23 11:52:33,264] [INFO] [engine.py:542:_configure_optimizer] DeepSpeed Final Optimizer = FusedAdam (
Tencent01: Parameter Group 0
Tencent01:     betas: (0.9, 0.999)
Tencent01:     bias_correction: True
Tencent01:     eps: 1e-08
Tencent01:     lr: 0.00015
Tencent01:     step: 1
Tencent01:     weight_decay: 0.01
Tencent01: 
Tencent01: Parameter Group 1
Tencent01:     betas: (0.9, 0.999)
Tencent01:     bias_correction: True
Tencent01:     eps: 1e-08
Tencent01:     lr: 0.00015
Tencent01:     step: 1
Tencent01:     weight_decay: 0.0
Tencent01: )
Tencent01: [2020-11-23 11:52:33,320] [INFO] [engine.py:543:_configure_optimizer] DeepSpeed Final Optimizer = {'dynamic_loss_scale': True, 'cur_scale': 4294967296, 'cur_iter': 0, 'last_overflow_iter': -1, 'scale_factor': 2, 'scale_window': 1000, 'optimizer_state_dict': {'state': {0: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:5'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:5')}, 1: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:5'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:5')}}, 'param_groups': [{'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'step': 1, 'params': [0]}, {'weight_decay': 0.0, 'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'step': 1, 'params': [1]}]}, 'fp32_groups_flat': [tensor([-0.0022, -0.0099,  0.0033,  ..., -0.0024, -0.0020, -0.0019],
Tencent01:        device='cuda:5', requires_grad=True), tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:5', requires_grad=True)], 'clip_grad': 1.0}
Tencent02: [2020-11-23 11:52:33,729] [INFO] [engine.py:542:_configure_optimizer] DeepSpeed Final Optimizer = FusedAdam (
Tencent02: Parameter Group 0
Tencent02:     betas: (0.9, 0.999)
Tencent02:     bias_correction: True
Tencent02:     eps: 1e-08
Tencent02:     lr: 0.00015
Tencent02:     step: 1
Tencent02:     weight_decay: 0.01
Tencent02: 
Tencent02: Parameter Group 1
Tencent02:     betas: (0.9, 0.999)
Tencent02:     bias_correction: True
Tencent02:     eps: 1e-08
Tencent02:     lr: 0.00015
Tencent02:     step: 1
Tencent02:     weight_decay: 0.0
Tencent02: )
Tencent02: [2020-11-23 11:52:33,781] [INFO] [engine.py:543:_configure_optimizer] DeepSpeed Final Optimizer = {'dynamic_loss_scale': True, 'cur_scale': 4294967296, 'cur_iter': 0, 'last_overflow_iter': -1, 'scale_factor': 2, 'scale_window': 1000, 'optimizer_state_dict': {'state': {0: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:2'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:2')}, 1: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:2'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:2')}}, 'param_groups': [{'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'step': 1, 'params': [0]}, {'weight_decay': 0.0, 'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'step': 1, 'params': [1]}]}, 'fp32_groups_flat': [tensor([-0.0022, -0.0099,  0.0033,  ..., -0.0024, -0.0020, -0.0019],
Tencent02:        device='cuda:2', requires_grad=True), tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:2', requires_grad=True)], 'clip_grad': 1.0}
Tencent02: [2020-11-23 11:52:34,392] [INFO] [engine.py:542:_configure_optimizer] DeepSpeed Final Optimizer = FusedAdam (
Tencent02: Parameter Group 0
Tencent02:     betas: (0.9, 0.999)
Tencent02:     bias_correction: True
Tencent02:     eps: 1e-08
Tencent02:     lr: 0.00015
Tencent02:     step: 1
Tencent02:     weight_decay: 0.01
Tencent02: 
Tencent02: Parameter Group 1
Tencent02:     betas: (0.9, 0.999)
Tencent02:     bias_correction: True
Tencent02:     eps: 1e-08
Tencent02:     lr: 0.00015
Tencent02:     step: 1
Tencent02:     weight_decay: 0.0
Tencent02: )
Tencent02: [2020-11-23 11:52:34,446] [INFO] [engine.py:543:_configure_optimizer] DeepSpeed Final Optimizer = {'dynamic_loss_scale': True, 'cur_scale': 4294967296, 'cur_iter': 0, 'last_overflow_iter': -1, 'scale_factor': 2, 'scale_window': 1000, 'optimizer_state_dict': {'state': {0: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:4'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:4')}, 1: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:4'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:4')}}, 'param_groups': [{'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'step': 1, 'params': [0]}, {'weight_decay': 0.0, 'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'step': 1, 'params': [1]}]}, 'fp32_groups_flat': [tensor([-0.0022, -0.0099,  0.0033,  ..., -0.0024, -0.0020, -0.0019],
Tencent02:        device='cuda:4', requires_grad=True), tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:4', requires_grad=True)], 'clip_grad': 1.0}
Tencent02: [2020-11-23 11:52:34,609] [INFO] [engine.py:542:_configure_optimizer] DeepSpeed Final Optimizer = FusedAdam (
Tencent02: Parameter Group 0
Tencent02:     betas: (0.9, 0.999)
Tencent02:     bias_correction: True
Tencent02:     eps: 1e-08
Tencent02:     lr: 0.00015
Tencent02:     step: 1
Tencent02:     weight_decay: 0.01
Tencent02: 
Tencent02: Parameter Group 1
Tencent02:     betas: (0.9, 0.999)
Tencent02:     bias_correction: True
Tencent02:     eps: 1e-08
Tencent02:     lr: 0.00015
Tencent02:     step: 1
Tencent02:     weight_decay: 0.0
Tencent02: )
Tencent02: [2020-11-23 11:52:34,663] [INFO] [engine.py:543:_configure_optimizer] DeepSpeed Final Optimizer = {'dynamic_loss_scale': True, 'cur_scale': 4294967296, 'cur_iter': 0, 'last_overflow_iter': -1, 'scale_factor': 2, 'scale_window': 1000, 'optimizer_state_dict': {'state': {0: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}, 1: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')}}, 'param_groups': [{'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'step': 1, 'params': [0]}, {'weight_decay': 0.0, 'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'step': 1, 'params': [1]}]}, 'fp32_groups_flat': [tensor([-0.0022, -0.0099,  0.0033,  ..., -0.0024, -0.0020, -0.0019],
Tencent02:        device='cuda:0', requires_grad=True), tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:0', requires_grad=True)], 'clip_grad': 1.0}
Tencent01: [2020-11-23 11:52:35,081] [INFO] [engine.py:542:_configure_optimizer] DeepSpeed Final Optimizer = FusedAdam (
Tencent01: Parameter Group 0
Tencent01:     betas: (0.9, 0.999)
Tencent01:     bias_correction: True
Tencent01:     eps: 1e-08
Tencent01:     lr: 0.00015
Tencent01:     step: 1
Tencent01:     weight_decay: 0.01
Tencent01: 
Tencent01: Parameter Group 1
Tencent01:     betas: (0.9, 0.999)
Tencent01:     bias_correction: True
Tencent01:     eps: 1e-08
Tencent01:     lr: 0.00015
Tencent01:     step: 1
Tencent01:     weight_decay: 0.0
Tencent01: )
Tencent01: [2020-11-23 11:52:35,136] [INFO] [engine.py:543:_configure_optimizer] DeepSpeed Final Optimizer = {'dynamic_loss_scale': True, 'cur_scale': 4294967296, 'cur_iter': 0, 'last_overflow_iter': -1, 'scale_factor': 2, 'scale_window': 1000, 'optimizer_state_dict': {'state': {0: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:2'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:2')}, 1: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:2'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:2')}}, 'param_groups': [{'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'step': 1, 'params': [0]}, {'weight_decay': 0.0, 'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'step': 1, 'params': [1]}]}, 'fp32_groups_flat': [tensor([-0.0022, -0.0099,  0.0033,  ..., -0.0024, -0.0020, -0.0019],
Tencent01:        device='cuda:2', requires_grad=True), tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:2', requires_grad=True)], 'clip_grad': 1.0}
Tencent02: [2020-11-23 11:52:35,260] [INFO] [engine.py:542:_configure_optimizer] DeepSpeed Final Optimizer = FusedAdam (
Tencent02: Parameter Group 0
Tencent02:     betas: (0.9, 0.999)
Tencent02:     bias_correction: True
Tencent02:     eps: 1e-08
Tencent02:     lr: 0.00015
Tencent02:     step: 1
Tencent02:     weight_decay: 0.01
Tencent02: 
Tencent02: Parameter Group 1
Tencent02:     betas: (0.9, 0.999)
Tencent02:     bias_correction: True
Tencent02:     eps: 1e-08
Tencent02:     lr: 0.00015
Tencent02:     step: 1
Tencent02:     weight_decay: 0.0
Tencent02: )
Tencent02: [2020-11-23 11:52:35,315] [INFO] [engine.py:543:_configure_optimizer] DeepSpeed Final Optimizer = {'dynamic_loss_scale': True, 'cur_scale': 4294967296, 'cur_iter': 0, 'last_overflow_iter': -1, 'scale_factor': 2, 'scale_window': 1000, 'optimizer_state_dict': {'state': {0: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:1'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:1')}, 1: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:1'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:1')}}, 'param_groups': [{'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'step': 1, 'params': [0]}, {'weight_decay': 0.0, 'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'step': 1, 'params': [1]}]}, 'fp32_groups_flat': [tensor([-0.0022, -0.0099,  0.0033,  ..., -0.0024, -0.0020, -0.0019],
Tencent02:        device='cuda:1', requires_grad=True), tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:1', requires_grad=True)], 'clip_grad': 1.0}
Tencent02: [2020-11-23 11:52:35,354] [INFO] [engine.py:542:_configure_optimizer] DeepSpeed Final Optimizer = FusedAdam (
Tencent02: Parameter Group 0
Tencent02:     betas: (0.9, 0.999)
Tencent02:     bias_correction: True
Tencent02:     eps: 1e-08
Tencent02:     lr: 0.00015
Tencent02:     step: 1
Tencent02:     weight_decay: 0.01
Tencent02: 
Tencent02: Parameter Group 1
Tencent02:     betas: (0.9, 0.999)
Tencent02:     bias_correction: True
Tencent02:     eps: 1e-08
Tencent02:     lr: 0.00015
Tencent02:     step: 1
Tencent02:     weight_decay: 0.0
Tencent02: )
Tencent02: [2020-11-23 11:52:35,409] [INFO] [engine.py:543:_configure_optimizer] DeepSpeed Final Optimizer = {'dynamic_loss_scale': True, 'cur_scale': 4294967296, 'cur_iter': 0, 'last_overflow_iter': -1, 'scale_factor': 2, 'scale_window': 1000, 'optimizer_state_dict': {'state': {0: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:3'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:3')}, 1: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:3'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:3')}}, 'param_groups': [{'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'step': 1, 'params': [0]}, {'weight_decay': 0.0, 'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'step': 1, 'params': [1]}]}, 'fp32_groups_flat': [tensor([-0.0022, -0.0099,  0.0033,  ..., -0.0024, -0.0020, -0.0019],
Tencent02:        device='cuda:3', requires_grad=True), tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:3', requires_grad=True)], 'clip_grad': 1.0}
Tencent01: [2020-11-23 11:52:35,822] [INFO] [engine.py:542:_configure_optimizer] DeepSpeed Final Optimizer = FusedAdam (
Tencent01: Parameter Group 0
Tencent01:     betas: (0.9, 0.999)
Tencent01:     bias_correction: True
Tencent01:     eps: 1e-08
Tencent01:     lr: 0.00015
Tencent01:     step: 1
Tencent01:     weight_decay: 0.01
Tencent01: 
Tencent01: Parameter Group 1
Tencent01:     betas: (0.9, 0.999)
Tencent01:     bias_correction: True
Tencent01:     eps: 1e-08
Tencent01:     lr: 0.00015
Tencent01:     step: 1
Tencent01:     weight_decay: 0.0
Tencent01: )
Tencent01: [2020-11-23 11:52:35,826] [INFO] [engine.py:542:_configure_optimizer] DeepSpeed Final Optimizer = FusedAdam (
Tencent01: Parameter Group 0
Tencent01:     betas: (0.9, 0.999)
Tencent01:     bias_correction: True
Tencent01:     eps: 1e-08
Tencent01:     lr: 0.00015
Tencent01:     step: 1
Tencent01:     weight_decay: 0.01
Tencent01: 
Tencent01: Parameter Group 1
Tencent01:     betas: (0.9, 0.999)
Tencent01:     bias_correction: True
Tencent01:     eps: 1e-08
Tencent01:     lr: 0.00015
Tencent01:     step: 1
Tencent01:     weight_decay: 0.0
Tencent01: )
Tencent01: [2020-11-23 11:52:35,871] [INFO] [engine.py:543:_configure_optimizer] DeepSpeed Final Optimizer = {'dynamic_loss_scale': True, 'cur_scale': 4294967296, 'cur_iter': 0, 'last_overflow_iter': -1, 'scale_factor': 2, 'scale_window': 1000, 'optimizer_state_dict': {'state': {0: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:1'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:1')}, 1: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:1'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:1')}}, 'param_groups': [{'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'step': 1, 'params': [0]}, {'weight_decay': 0.0, 'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'step': 1, 'params': [1]}]}, 'fp32_groups_flat': [tensor([-0.0022, -0.0099,  0.0033,  ..., -0.0024, -0.0020, -0.0019],
Tencent01:        device='cuda:1', requires_grad=True), tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:1', requires_grad=True)], 'clip_grad': 1.0}
Tencent01: [2020-11-23 11:52:35,876] [INFO] [engine.py:543:_configure_optimizer] DeepSpeed Final Optimizer = {'dynamic_loss_scale': True, 'cur_scale': 4294967296, 'cur_iter': 0, 'last_overflow_iter': -1, 'scale_factor': 2, 'scale_window': 1000, 'optimizer_state_dict': {'state': {0: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:6'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:6')}, 1: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:6'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:6')}}, 'param_groups': [{'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'step': 1, 'params': [0]}, {'weight_decay': 0.0, 'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'step': 1, 'params': [1]}]}, 'fp32_groups_flat': [tensor([-0.0022, -0.0099,  0.0033,  ..., -0.0024, -0.0020, -0.0019],
Tencent01:        device='cuda:6', requires_grad=True), tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:6', requires_grad=True)], 'clip_grad': 1.0}
Tencent01: [2020-11-23 11:52:35,878] [INFO] [engine.py:542:_configure_optimizer] DeepSpeed Final Optimizer = FusedAdam (
Tencent01: Parameter Group 0
Tencent01:     betas: (0.9, 0.999)
Tencent01:     bias_correction: True
Tencent01:     eps: 1e-08
Tencent01:     lr: 0.00015
Tencent01:     step: 1
Tencent01:     weight_decay: 0.01
Tencent01: 
Tencent01: Parameter Group 1
Tencent01:     betas: (0.9, 0.999)
Tencent01:     bias_correction: True
Tencent01:     eps: 1e-08
Tencent01:     lr: 0.00015
Tencent01:     step: 1
Tencent01:     weight_decay: 0.0
Tencent01: )
Tencent01: [2020-11-23 11:52:35,901] [INFO] [engine.py:542:_configure_optimizer] DeepSpeed Final Optimizer = FusedAdam (
Tencent01: Parameter Group 0
Tencent01:     betas: (0.9, 0.999)
Tencent01:     bias_correction: True
Tencent01:     eps: 1e-08
Tencent01:     lr: 0.00015
Tencent01:     step: 1
Tencent01:     weight_decay: 0.01
Tencent01: 
Tencent01: Parameter Group 1
Tencent01:     betas: (0.9, 0.999)
Tencent01:     bias_correction: True
Tencent01:     eps: 1e-08
Tencent01:     lr: 0.00015
Tencent01:     step: 1
Tencent01:     weight_decay: 0.0
Tencent01: )
Tencent01: [2020-11-23 11:52:35,929] [INFO] [engine.py:543:_configure_optimizer] DeepSpeed Final Optimizer = {'dynamic_loss_scale': True, 'cur_scale': 4294967296, 'cur_iter': 0, 'last_overflow_iter': -1, 'scale_factor': 2, 'scale_window': 1000, 'optimizer_state_dict': {'state': {0: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:7'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:7')}, 1: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:7'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:7')}}, 'param_groups': [{'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'step': 1, 'params': [0]}, {'weight_decay': 0.0, 'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'step': 1, 'params': [1]}]}, 'fp32_groups_flat': [tensor([-0.0022, -0.0099,  0.0033,  ..., -0.0024, -0.0020, -0.0019],
Tencent01:        device='cuda:7', requires_grad=True), tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:7', requires_grad=True)], 'clip_grad': 1.0}
Tencent01: [2020-11-23 11:52:35,954] [INFO] [engine.py:543:_configure_optimizer] DeepSpeed Final Optimizer = {'dynamic_loss_scale': True, 'cur_scale': 4294967296, 'cur_iter': 0, 'last_overflow_iter': -1, 'scale_factor': 2, 'scale_window': 1000, 'optimizer_state_dict': {'state': {0: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:3'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:3')}, 1: {'exp_avg': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:3'), 'exp_avg_sq': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:3')}}, 'param_groups': [{'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'step': 1, 'params': [0]}, {'weight_decay': 0.0, 'lr': 0.00015, 'bias_correction': True, 'betas': (0.9, 0.999), 'eps': 1e-08, 'step': 1, 'params': [1]}]}, 'fp32_groups_flat': [tensor([-0.0022, -0.0099,  0.0033,  ..., -0.0024, -0.0020, -0.0019],
Tencent01:        device='cuda:3', requires_grad=True), tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:3', requires_grad=True)], 'clip_grad': 1.0}
Tencent02: [2020-11-23 11:52:46,487] [INFO] [engine.py:1224:_load_checkpoint] rank: 15 loading checkpoint: /root/data/checkpoints/gpt-345M11-14-13-21/70000/mp_rank_00_model_states.pt
Tencent01: [2020-11-23 11:52:56,745] [INFO] [engine.py:1224:_load_checkpoint] rank: 3 loading checkpoint: /root/data/checkpoints/gpt-345M11-14-13-21/70000/mp_rank_00_model_states.pt
Tencent02: [2020-11-23 11:53:07,309] [INFO] [engine.py:1224:_load_checkpoint] rank: 11 loading checkpoint: /root/data/checkpoints/gpt-345M11-14-13-21/70000/mp_rank_00_model_states.pt
Tencent01: [2020-11-23 11:53:08,679] [INFO] [engine.py:1224:_load_checkpoint] rank: 7 loading checkpoint: /root/data/checkpoints/gpt-345M11-14-13-21/70000/mp_rank_00_model_states.pt
Tencent02: [2020-11-23 11:53:18,122] [INFO] [engine.py:1224:_load_checkpoint] rank: 13 loading checkpoint: /root/data/checkpoints/gpt-345M11-14-13-21/70000/mp_rank_00_model_states.pt
Tencent01: [2020-11-23 11:53:19,988] [INFO] [engine.py:1224:_load_checkpoint] rank: 1 loading checkpoint: /root/data/checkpoints/gpt-345M11-14-13-21/70000/mp_rank_00_model_states.pt
Tencent02: [2020-11-23 11:53:29,125] [INFO] [engine.py:1224:_load_checkpoint] rank: 10 loading checkpoint: /root/data/checkpoints/gpt-345M11-14-13-21/70000/mp_rank_00_model_states.pt
Tencent01: [2020-11-23 11:53:31,474] [INFO] [engine.py:1224:_load_checkpoint] rank: 5 loading checkpoint: /root/data/checkpoints/gpt-345M11-14-13-21/70000/mp_rank_00_model_states.pt
Tencent02: [2020-11-23 11:53:38,900] [INFO] [engine.py:1224:_load_checkpoint] rank: 12 loading checkpoint: /root/data/checkpoints/gpt-345M11-14-13-21/70000/mp_rank_00_model_states.pt
Tencent01: [2020-11-23 11:53:42,749] [INFO] [engine.py:1224:_load_checkpoint] rank: 6 loading checkpoint: /root/data/checkpoints/gpt-345M11-14-13-21/70000/mp_rank_00_model_states.pt
Tencent01: [2020-11-23 11:53:53,972] [INFO] [engine.py:1224:_load_checkpoint] rank: 0 loading checkpoint: /root/data/checkpoints/gpt-345M11-14-13-21/70000/mp_rank_00_model_states.pt
Tencent02: [2020-11-23 11:53:59,902] [INFO] [engine.py:1224:_load_checkpoint] rank: 8 loading checkpoint: /root/data/checkpoints/gpt-345M11-14-13-21/70000/mp_rank_00_model_states.pt
Tencent01: Load lr scheduler state
Tencent01:   successfully loaded /root/data/checkpoints/gpt-345M11-14-13-21/70000/mp_rank_00_model_states.pt
Tencent01: [2020-11-23 11:54:05,260] [INFO] [engine.py:1224:_load_checkpoint] rank: 2 loading checkpoint: /root/data/checkpoints/gpt-345M11-14-13-21/70000/mp_rank_00_model_states.pt
Tencent02: [2020-11-23 11:54:21,486] [INFO] [engine.py:1224:_load_checkpoint] rank: 9 loading checkpoint: /root/data/checkpoints/gpt-345M11-14-13-21/70000/mp_rank_00_model_states.pt
Tencent01: VM-0-14-centos:91:580 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent02: VM-0-9-centos:36:522 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent01: VM-0-14-centos:91:580 [7] NCCL INFO Trees [0] 0/-1/-1->7->5|5->7->0/-1/-1 [1] 0/-1/-1->7->5|5->7->0/-1/-1
Tencent01: VM-0-14-centos:91:580 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000
Tencent02: VM-0-9-centos:36:522 [3] NCCL INFO Trees [0] 9/-1/-1->11->10|10->11->9/-1/-1 [1] 9/-1/-1->11->10|10->11->9/-1/-1
Tencent02: VM-0-9-centos:36:522 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff
Tencent02: VM-0-9-centos:38:521 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent02: VM-0-9-centos:37:526 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent02: VM-0-9-centos:38:521 [5] NCCL INFO Trees [0] 15/-1/-1->13->12|12->13->15/-1/-1 [1] 15/4/-1->13->12|12->13->15/4/-1
Tencent02: VM-0-9-centos:37:526 [4] NCCL INFO Trees [0] 13/-1/-1->12->5|5->12->13/-1/-1 [1] 13/-1/-1->12->-1|-1->12->13/-1/-1
Tencent02: VM-0-9-centos:38:521 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000
Tencent02: VM-0-9-centos:37:526 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000
Tencent02: VM-0-9-centos:34:528 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent02: VM-0-9-centos:35:524 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent02: VM-0-9-centos:34:528 [1] NCCL INFO Trees [0] 14/-1/-1->9->11|11->9->14/-1/-1 [1] 14/-1/-1->9->11|11->9->14/-1/-1
Tencent02: VM-0-9-centos:35:524 [2] NCCL INFO Trees [0] 11/-1/-1->10->8|8->10->11/-1/-1 [1] 11/-1/-1->10->8|8->10->11/-1/-1
Tencent02: VM-0-9-centos:33:527 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent02: VM-0-9-centos:34:528 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff
Tencent02: VM-0-9-centos:35:524 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff
Tencent01: VM-0-14-centos:84:575 [0] NCCL INFO Channel 00/02 :    0   1   3   2   5  12  14  15   8   9  11  10  13   4   6   7
Tencent02: VM-0-9-centos:33:527 [0] NCCL INFO Trees [0] 10/-1/-1->8->15|15->8->10/-1/-1 [1] 10/-1/-1->8->15|15->8->10/-1/-1
Tencent01: VM-0-14-centos:84:575 [0] NCCL INFO Channel 01/02 :    0   1   3   2   5  12  14  15   8   9  11  10  13   4   6   7
Tencent02: VM-0-9-centos:40:523 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent02: VM-0-9-centos:33:527 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff
Tencent01: VM-0-14-centos:90:578 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent02: VM-0-9-centos:39:525 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent01: VM-0-14-centos:90:578 [6] NCCL INFO Trees [0] -1/-1/-1->6->1|1->6->-1/-1/-1 [1] -1/-1/-1->6->1|1->6->-1/-1/-1
Tencent02: VM-0-9-centos:40:523 [7] NCCL INFO Trees [0] 8/-1/-1->15->13|13->15->8/-1/-1 [1] 8/-1/-1->15->13|13->15->8/-1/-1
Tencent01: VM-0-14-centos:84:575 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent02: VM-0-9-centos:39:525 [6] NCCL INFO Trees [0] -1/-1/-1->14->9|9->14->-1/-1/-1 [1] -1/-1/-1->14->9|9->14->-1/-1/-1
Tencent01: VM-0-14-centos:90:578 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000
Tencent02: VM-0-9-centos:40:523 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000
Tencent01: VM-0-14-centos:84:575 [0] NCCL INFO Trees [0] 2/-1/-1->0->7|7->0->2/-1/-1 [1] 2/-1/-1->0->7|7->0->2/-1/-1
Tencent02: VM-0-9-centos:39:525 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000
Tencent01: VM-0-14-centos:89:577 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent01: VM-0-14-centos:84:575 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff
Tencent01: VM-0-14-centos:89:577 [5] NCCL INFO Trees [0] 7/12/-1->5->4|4->5->7/12/-1 [1] 7/-1/-1->5->4|4->5->7/-1/-1
Tencent01: VM-0-14-centos:89:577 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000
Tencent01: VM-0-14-centos:86:582 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent01: VM-0-14-centos:88:576 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent01: VM-0-14-centos:87:579 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent01: VM-0-14-centos:88:576 [4] NCCL INFO Trees [0] 5/-1/-1->4->-1|-1->4->5/-1/-1 [1] 5/-1/-1->4->13|13->4->5/-1/-1
Tencent01: VM-0-14-centos:86:582 [2] NCCL INFO Trees [0] 3/-1/-1->2->0|0->2->3/-1/-1 [1] 3/-1/-1->2->0|0->2->3/-1/-1
Tencent01: VM-0-14-centos:87:579 [3] NCCL INFO Trees [0] 1/-1/-1->3->2|2->3->1/-1/-1 [1] 1/-1/-1->3->2|2->3->1/-1/-1
Tencent01: VM-0-14-centos:88:576 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000
Tencent01: VM-0-14-centos:86:582 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff
Tencent01: VM-0-14-centos:87:579 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff
Tencent01: VM-0-14-centos:85:581 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64
Tencent01: VM-0-14-centos:85:581 [1] NCCL INFO Trees [0] 6/-1/-1->1->3|3->1->6/-1/-1 [1] 6/-1/-1->1->3|3->1->6/-1/-1
Tencent01: VM-0-14-centos:85:581 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff
Tencent02: VM-0-9-centos:37:526 [4] NCCL INFO Channel 00 : 5[89000] -> 12[88000] [receive] via NET/IB/0
Tencent02: VM-0-9-centos:36:522 [3] NCCL INFO Channel 00 : 11[3e000] -> 10[3d000] via P2P/IPC
Tencent02: VM-0-9-centos:33:527 [0] NCCL INFO Channel 00 : 8[1a000] -> 9[1b000] via P2P/IPC
Tencent02: VM-0-9-centos:40:523 [7] NCCL INFO Channel 00 : 15[b2000] -> 8[1a000] via P2P/IPC
Tencent02: VM-0-9-centos:35:524 [2] NCCL INFO Channel 00 : 10[3d000] -> 13[89000] via P2P/IPC
Tencent02: VM-0-9-centos:34:528 [1] NCCL INFO Channel 00 : 9[1b000] -> 11[3e000] via P2P/IPC
Tencent02: VM-0-9-centos:37:526 [4] NCCL INFO Channel 00 : 12[88000] -> 14[b1000] via P2P/IPC
Tencent02: VM-0-9-centos:39:525 [6] NCCL INFO Channel 00 : 14[b1000] -> 15[b2000] via P2P/IPC
Tencent02: VM-0-9-centos:38:521 [5] NCCL INFO Channel 00 : 13[89000] -> 4[88000] [send] via NET/IB/0
Tencent01: VM-0-14-centos:90:578 [6] NCCL INFO Channel 00 : 6[b1000] -> 7[b2000] via P2P/IPC
Tencent01: VM-0-14-centos:84:575 [0] NCCL INFO Channel 00 : 0[1a000] -> 1[1b000] via P2P/IPC
Tencent01: VM-0-14-centos:86:582 [2] NCCL INFO Channel 00 : 2[3d000] -> 5[89000] via P2P/IPC
Tencent01: VM-0-14-centos:87:579 [3] NCCL INFO Channel 00 : 3[3e000] -> 2[3d000] via P2P/IPC
Tencent01: VM-0-14-centos:85:581 [1] NCCL INFO Channel 00 : 1[1b000] -> 3[3e000] via P2P/IPC
Tencent01: VM-0-14-centos:91:580 [7] NCCL INFO Channel 00 : 7[b2000] -> 0[1a000] via P2P/IPC
Tencent01: VM-0-14-centos:88:576 [4] NCCL INFO Channel 00 : 13[89000] -> 4[88000] [receive] via NET/IB/0
Tencent02: VM-0-9-centos:35:524 [2] NCCL INFO Channel 00 : 10[3d000] -> 8[1a000] via P2P/IPC
Tencent02: VM-0-9-centos:39:525 [6] NCCL INFO Channel 00 : 14[b1000] -> 9[1b000] via P2P/IPC
Tencent01: VM-0-14-centos:89:577 [5] NCCL INFO Channel 00 : 5[89000] -> 12[88000] [send] via NET/IB/0
Tencent01: VM-0-14-centos:88:576 [4] NCCL INFO Channel 00 : 4[88000] -> 6[b1000] via P2P/IPC
Tencent02: VM-0-9-centos:36:522 [3] NCCL INFO Channel 00 : 11[3e000] -> 9[1b000] via P2P/IPC
Tencent02: VM-0-9-centos:33:527 [0] NCCL INFO Channel 00 : 8[1a000] -> 15[b2000] via P2P/IPC
Tencent02: VM-0-9-centos:40:523 [7] NCCL INFO Channel 00 : 15[b2000] -> 13[89000] via P2P/IPC
Tencent01: VM-0-14-centos:86:582 [2] NCCL INFO Channel 00 : 2[3d000] -> 0[1a000] via P2P/IPC
Tencent02: VM-0-9-centos:38:521 [5] NCCL INFO Channel 00 : 13[89000] -> 12[88000] via P2P/IPC
Tencent02: VM-0-9-centos:35:524 [2] NCCL INFO Channel 00 : 10[3d000] -> 11[3e000] via P2P/IPC
Tencent02: VM-0-9-centos:34:528 [1] NCCL INFO Channel 00 : 9[1b000] -> 14[b1000] via P2P/IPC
Tencent02: VM-0-9-centos:33:527 [0] NCCL INFO Channel 00 : 8[1a000] -> 10[3d000] via P2P/IPC
Tencent01: VM-0-14-centos:84:575 [0] NCCL INFO Channel 00 : 0[1a000] -> 7[b2000] via P2P/IPC
Tencent01: VM-0-14-centos:90:578 [6] NCCL INFO Channel 00 : 6[b1000] -> 1[1b000] via P2P/IPC
Tencent01: VM-0-14-centos:87:579 [3] NCCL INFO Channel 00 : 3[3e000] -> 1[1b000] via P2P/IPC
Tencent01: VM-0-14-centos:91:580 [7] NCCL INFO Channel 00 : 7[b2000] -> 5[89000] via P2P/IPC
Tencent02: VM-0-9-centos:37:526 [4] NCCL INFO Channel 00 : 12[88000] -> 5[89000] [send] via NET/IB/0
Tencent01: VM-0-14-centos:86:582 [2] NCCL INFO Channel 00 : 2[3d000] -> 3[3e000] via P2P/IPC
Tencent02: VM-0-9-centos:36:522 [3] NCCL INFO Channel 01 : 11[3e000] -> 10[3d000] via P2P/IPC
Tencent02: VM-0-9-centos:39:525 [6] NCCL INFO Channel 01 : 14[b1000] -> 15[b2000] via P2P/IPC
Tencent02: VM-0-9-centos:38:521 [5] NCCL INFO Channel 00 : 13[89000] -> 15[b2000] via P2P/IPC
Tencent02: VM-0-9-centos:33:527 [0] NCCL INFO Channel 01 : 8[1a000] -> 9[1b000] via P2P/IPC
Tencent02: VM-0-9-centos:35:524 [2] NCCL INFO Channel 01 : 10[3d000] -> 13[89000] via P2P/IPC
Tencent01: VM-0-14-centos:84:575 [0] NCCL INFO Channel 00 : 0[1a000] -> 2[3d000] via P2P/IPC
Tencent02: VM-0-9-centos:34:528 [1] NCCL INFO Channel 01 : 9[1b000] -> 11[3e000] via P2P/IPC
Tencent01: VM-0-14-centos:85:581 [1] NCCL INFO Channel 00 : 1[1b000] -> 6[b1000] via P2P/IPC
Tencent02: VM-0-9-centos:40:523 [7] NCCL INFO Channel 01 : 15[b2000] -> 8[1a000] via P2P/IPC
Tencent02: VM-0-9-centos:36:522 [3] NCCL INFO Channel 01 : 11[3e000] -> 9[1b000] via P2P/IPC
Tencent02: VM-0-9-centos:33:527 [0] NCCL INFO Channel 01 : 8[1a000] -> 15[b2000] via P2P/IPC
Tencent02: VM-0-9-centos:40:523 [7] NCCL INFO Channel 01 : 15[b2000] -> 13[89000] via P2P/IPC
Tencent01: VM-0-14-centos:89:577 [5] NCCL INFO Channel 00 : 12[88000] -> 5[89000] [receive] via NET/IB/0
Tencent02: VM-0-9-centos:37:526 [4] NCCL INFO Channel 00 : 12[88000] -> 13[89000] via P2P/IPC
Tencent01: VM-0-14-centos:87:579 [3] NCCL INFO Channel 01 : 3[3e000] -> 2[3d000] via P2P/IPC
Tencent01: VM-0-14-centos:84:575 [0] NCCL INFO Channel 01 : 0[1a000] -> 1[1b000] via P2P/IPC
Tencent01: VM-0-14-centos:90:578 [6] NCCL INFO Channel 01 : 6[b1000] -> 7[b2000] via P2P/IPC
Tencent01: VM-0-14-centos:86:582 [2] NCCL INFO Channel 01 : 2[3d000] -> 5[89000] via P2P/IPC
Tencent01: VM-0-14-centos:89:577 [5] NCCL INFO Channel 00 : 5[89000] -> 4[88000] via P2P/IPC
Tencent02: VM-0-9-centos:35:524 [2] NCCL INFO Channel 01 : 10[3d000] -> 8[1a000] via P2P/IPC
Tencent01: VM-0-14-centos:85:581 [1] NCCL INFO Channel 01 : 1[1b000] -> 3[3e000] via P2P/IPC
Tencent02: VM-0-9-centos:33:527 [0] NCCL INFO Channel 01 : 8[1a000] -> 10[3d000] via P2P/IPC
Tencent02: VM-0-9-centos:33:527 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
Tencent02: VM-0-9-centos:33:527 [0] NCCL INFO comm 0x7ec268006890 rank 8 nranks 16 cudaDev 0 busId 1a000 - Init COMPLETE
Tencent02: VM-0-9-centos:35:524 [2] NCCL INFO Channel 01 : 10[3d000] -> 11[3e000] via P2P/IPC
Tencent01: VM-0-14-centos:88:576 [4] NCCL INFO Channel 00 : 4[88000] -> 5[89000] via P2P/IPC
Tencent02: VM-0-9-centos:37:526 [4] NCCL INFO Channel 01 : 5[89000] -> 12[88000] [receive] via NET/IB/0
Tencent02: VM-0-9-centos:38:521 [5] NCCL INFO Channel 01 : 13[89000] -> 4[88000] [send] via NET/IB/0
Tencent01: VM-0-14-centos:87:579 [3] NCCL INFO Channel 01 : 3[3e000] -> 1[1b000] via P2P/IPC
Tencent02: VM-0-9-centos:37:526 [4] NCCL INFO Channel 01 : 12[88000] -> 14[b1000] via P2P/IPC
Tencent02: VM-0-9-centos:35:524 [2] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
Tencent02: VM-0-9-centos:35:524 [2] NCCL INFO comm 0x7f6758006890 rank 10 nranks 16 cudaDev 2 busId 3d000 - Init COMPLETE
Tencent01: VM-0-14-centos:89:577 [5] NCCL INFO Channel 00 : 5[89000] -> 7[b2000] via P2P/IPC
Tencent02: VM-0-9-centos:39:525 [6] NCCL INFO Channel 01 : 14[b1000] -> 9[1b000] via P2P/IPC
Tencent02: VM-0-9-centos:34:528 [1] NCCL INFO Channel 01 : 9[1b000] -> 14[b1000] via P2P/IPC
Tencent02: VM-0-9-centos:36:522 [3] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
Tencent02: VM-0-9-centos:36:522 [3] NCCL INFO comm 0x7f99f0006890 rank 11 nranks 16 cudaDev 3 busId 3e000 - Init COMPLETE
Tencent02: VM-0-9-centos:39:525 [6] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
Tencent02: VM-0-9-centos:39:525 [6] NCCL INFO comm 0x7fae34006890 rank 14 nranks 16 cudaDev 6 busId b1000 - Init COMPLETE
Tencent01: VM-0-14-centos:91:580 [7] NCCL INFO Channel 01 : 7[b2000] -> 0[1a000] via P2P/IPC
Tencent02: VM-0-9-centos:34:528 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
Tencent02: VM-0-9-centos:34:528 [1] NCCL INFO comm 0x7f8388006890 rank 9 nranks 16 cudaDev 1 busId 1b000 - Init COMPLETE
Tencent01: VM-0-14-centos:86:582 [2] NCCL INFO Channel 01 : 2[3d000] -> 0[1a000] via P2P/IPC
Tencent01: VM-0-14-centos:88:576 [4] NCCL INFO Channel 01 : 13[89000] -> 4[88000] [receive] via NET/IB/0
Tencent01: VM-0-14-centos:88:576 [4] NCCL INFO Channel 01 : 4[88000] -> 6[b1000] via P2P/IPC
Tencent01: VM-0-14-centos:84:575 [0] NCCL INFO Channel 01 : 0[1a000] -> 7[b2000] via P2P/IPC
Tencent01: VM-0-14-centos:91:580 [7] NCCL INFO Channel 01 : 7[b2000] -> 5[89000] via P2P/IPC
Tencent01: VM-0-14-centos:89:577 [5] NCCL INFO Channel 01 : 5[89000] -> 12[88000] [send] via NET/IB/0
Tencent02: VM-0-9-centos:38:521 [5] NCCL INFO Channel 01 : 4[88000] -> 13[89000] [receive] via NET/IB/0
Tencent02: VM-0-9-centos:38:521 [5] NCCL INFO Channel 01 : 13[89000] -> 12[88000] via P2P/IPC
Tencent01: VM-0-14-centos:86:582 [2] NCCL INFO Channel 01 : 2[3d000] -> 3[3e000] via P2P/IPC
Tencent01: VM-0-14-centos:90:578 [6] NCCL INFO Channel 01 : 6[b1000] -> 1[1b000] via P2P/IPC
Tencent02: VM-0-9-centos:37:526 [4] NCCL INFO Channel 01 : 12[88000] -> 13[89000] via P2P/IPC
Tencent01: VM-0-14-centos:84:575 [0] NCCL INFO Channel 01 : 0[1a000] -> 2[3d000] via P2P/IPC
Tencent01: VM-0-14-centos:86:582 [2] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
Tencent01: VM-0-14-centos:84:575 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
Tencent01: VM-0-14-centos:86:582 [2] NCCL INFO comm 0x7ee288006890 rank 2 nranks 16 cudaDev 2 busId 3d000 - Init COMPLETE
Tencent01: VM-0-14-centos:84:575 [0] NCCL INFO comm 0x7fb4ac006890 rank 0 nranks 16 cudaDev 0 busId 1a000 - Init COMPLETE
Tencent01: VM-0-14-centos:84:84 [0] NCCL INFO Launch mode Parallel
Tencent01: VM-0-14-centos:89:577 [5] NCCL INFO Channel 01 : 5[89000] -> 4[88000] via P2P/IPC
Tencent01: VM-0-14-centos:85:581 [1] NCCL INFO Channel 01 : 1[1b000] -> 6[b1000] via P2P/IPC
Tencent01: VM-0-14-centos:87:579 [3] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
Tencent01: VM-0-14-centos:87:579 [3] NCCL INFO comm 0x7efcc0006890 rank 3 nranks 16 cudaDev 3 busId 3e000 - Init COMPLETE
Tencent01: VM-0-14-centos:90:578 [6] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
Tencent01: VM-0-14-centos:88:576 [4] NCCL INFO Channel 01 : 4[88000] -> 13[89000] [send] via NET/IB/0
Tencent01: VM-0-14-centos:90:578 [6] NCCL INFO comm 0x7f09a0006890 rank 6 nranks 16 cudaDev 6 busId b1000 - Init COMPLETE
Tencent01: VM-0-14-centos:85:581 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
Tencent01: VM-0-14-centos:85:581 [1] NCCL INFO comm 0x7f5d10006890 rank 1 nranks 16 cudaDev 1 busId 1b000 - Init COMPLETE
Tencent02: VM-0-9-centos:37:526 [4] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
Tencent02: VM-0-9-centos:37:526 [4] NCCL INFO comm 0x7f36bc006890 rank 12 nranks 16 cudaDev 4 busId 88000 - Init COMPLETE
Tencent02: VM-0-9-centos:38:521 [5] NCCL INFO Channel 01 : 13[89000] -> 15[b2000] via P2P/IPC
Tencent02: VM-0-9-centos:40:523 [7] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
Tencent02: VM-0-9-centos:40:523 [7] NCCL INFO comm 0x7f2568006890 rank 15 nranks 16 cudaDev 7 busId b2000 - Init COMPLETE
Tencent01: VM-0-14-centos:88:576 [4] NCCL INFO Channel 01 : 4[88000] -> 5[89000] via P2P/IPC
Tencent01: VM-0-14-centos:89:577 [5] NCCL INFO Channel 01 : 5[89000] -> 7[b2000] via P2P/IPC
Tencent01: VM-0-14-centos:88:576 [4] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
Tencent02: VM-0-9-centos:38:521 [5] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
Tencent01: VM-0-14-centos:88:576 [4] NCCL INFO comm 0x7f80c0006890 rank 4 nranks 16 cudaDev 4 busId 88000 - Init COMPLETE
Tencent02: VM-0-9-centos:38:521 [5] NCCL INFO comm 0x7f0558006890 rank 13 nranks 16 cudaDev 5 busId 89000 - Init COMPLETE
Tencent01: VM-0-14-centos:91:580 [7] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
Tencent01: VM-0-14-centos:91:580 [7] NCCL INFO comm 0x7f1718006890 rank 7 nranks 16 cudaDev 7 busId b2000 - Init COMPLETE
Tencent01: VM-0-14-centos:89:577 [5] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
Tencent01: VM-0-14-centos:89:577 [5] NCCL INFO comm 0x7f9e58006890 rank 5 nranks 16 cudaDev 5 busId 89000 - Init COMPLETE
Tencent01: Pretrain GPT2 model
Tencent01: arguments:
Tencent01:   transformer_xl ............... False
Tencent01:   pretrained_bert .............. False
Tencent01:   attention_dropout ............ 0.1
Tencent01:   num_attention_heads .......... 16
Tencent01:   hidden_size .................. 1408
Tencent01:   intermediate_size ............ None
Tencent01:   num_layers ................... 40
Tencent01:   layernorm_epsilon ............ 1e-05
Tencent01:   hidden_dropout ............... 0.1
Tencent01:   max_position_embeddings ...... 1024
Tencent01:   vocab_size ................... 50048
Tencent01:   deep_init .................... False
Tencent01:   make_vocab_size_divisible_by . 128
Tencent01:   cpu_optimizer ................ False
Tencent01:   cpu_torch_adam ............... False
Tencent01:   fp16 ......................... True
Tencent01:   fp32_embedding ............... False
Tencent01:   fp32_layernorm ............... False
Tencent01:   fp32_tokentypes .............. False
Tencent01:   fp32_allreduce ............... False
Tencent01:   hysteresis ................... 2
Tencent01:   loss_scale ................... None
Tencent01:   loss_scale_window ............ 1000
Tencent01:   min_scale .................... 1
Tencent01:   experiment_name .............. gpt-345M11-14-13-21
Tencent01:   batch_size ................... 8
Tencent01:   weight_decay ................. 0.01
Tencent01:   checkpoint_activations ....... True
Tencent01:   checkpoint_num_layers ........ 1
Tencent01:   deepspeed_activation_checkpointing  True
Tencent01:   clip_grad .................... 1.0
Tencent01:   train_iters .................. 320000
Tencent01:   log_interval ................. 100
Tencent01:   exit_interval ................ None
Tencent01:   summary_dir .................. 
Tencent01:   seed ......................... 1234
Tencent01:   reset_position_ids ........... False
Tencent01:   reset_attention_mask ......... False
Tencent01:   lr_decay_iters ............... None
Tencent01:   lr_decay_style ............... cosine
Tencent01:   lr_decay_ratio ............... 0.1
Tencent01:   lr ........................... 0.00015
Tencent01:   warmup ....................... 0.01
Tencent01:   save ......................... /root/data/checkpoints/gpt-345M11-14-13-21
Tencent01:   save_interval ................ 2000
Tencent01:   no_save_optim ................ False
Tencent01:   no_save_rng .................. False
Tencent01:   load ......................... /root/data/checkpoints/gpt-345M11-14-13-21
Tencent01:   no_load_optim ................ False
Tencent01:   no_load_rng .................. False
Tencent01:   finetune ..................... False
Tencent01:   resume_dataloader ............ True
Tencent01:   distributed_backend .......... nccl
Tencent01:   local_rank ................... 0
Tencent01:   eval_batch_size .............. None
Tencent01:   eval_iters ................... 100
Tencent01:   eval_interval ................ 1000
Tencent01:   eval_seq_length .............. None
Tencent01:   eval_max_preds_per_seq ....... None
Tencent01:   overlapping_eval ............. 32
Tencent01:   cloze_eval ................... False
Tencent01:   eval_hf ...................... False
Tencent01:   load_openai .................. False
Tencent01:   temperature .................. 1.0
Tencent01:   top_p ........................ 0.0
Tencent01:   top_k ........................ 0
Tencent01:   out_seq_length ............... 256
Tencent01:   model_parallel_size .......... 1
Tencent01:   shuffle ...................... False
Tencent01:   train_data ................... ['zhihu', 'baike', 'zhidao']
Tencent01:   use_npy_data_loader .......... False
Tencent01:   train_data_path .............. 
Tencent01:   val_data_path ................ 
Tencent01:   test_data_path ............... 
Tencent01:   input_data_sizes_file ........ sizes.txt
Tencent01:   delim ........................ ,
Tencent01:   text_key ..................... sentence
Tencent01:   eval_text_key ................ None
Tencent01:   valid_data ................... None
Tencent01:   split ........................ 949,50,1
Tencent01:   test_data .................... None
Tencent01:   lazy_loader .................. True
Tencent01:   loose_json ................... False
Tencent01:   presplit_sentences ........... False
Tencent01:   num_workers .................. 2
Tencent01:   tokenizer_model_type ......... bert-large-uncased
Tencent01:   tokenizer_path ............... tokenizer.model
Tencent01:   tokenizer_type ............... ChineseSPTokenizer
Tencent01:   pre_tokenize ................. True
Tencent01:   cache_dir .................... None
Tencent01:   use_tfrecords ................ False
Tencent01:   seq_length ................... 1024
Tencent01:   mem_length ................... 512
Tencent01:   max_preds_per_seq ............ None
Tencent01:   sample_one_document .......... False
Tencent01:   deepspeed .................... True
Tencent01:   deepspeed_config ............. /root/code/Megatron-LM/scripts/ds_config.json
Tencent01:   deepscale .................... False
Tencent01:   deepscale_config ............. None
Tencent01:   deepspeed_mpi ................ False
Tencent01:   cuda ......................... True
Tencent01:   rank ......................... 0
Tencent01:   world_size ................... 16
Tencent01:   dynamic_loss_scale ........... True
Tencent01:   gradient_accumulation_steps .. 4
Tencent01:   persist_state ................ 0
Tencent01:   lazy ......................... False
Tencent01:   transpose .................... False
Tencent01:   data_set_type ................ GPT2
Tencent01:   samples_per_shard ............ 100
Tencent01:   do_train ..................... 1
Tencent01:   do_valid ..................... 1
Tencent01:   do_test ...................... 1
Tencent01:   eod_token .................... 50000
Tencent01:   iteration .................... 70000
Tencent01: [2020-11-23 11:55:20,108] [INFO] [checkpointing.py:63:see_memory_usage] First Forward Begining
Tencent01: [2020-11-23 11:55:20,109] [INFO] [checkpointing.py:66:see_memory_usage] Memory Allocated 13.433034420013428 GigaBytes
Tencent01: [2020-11-23 11:55:20,109] [INFO] [checkpointing.py:70:see_memory_usage] Max Memory Allocated 20.986533641815186 GigaBytes
Tencent01: /opt/conda/lib/python3.7/site-packages/torch/cuda/memory.py:346: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved
Tencent01:   FutureWarning)
Tencent01: [2020-11-23 11:55:20,110] [INFO] [checkpointing.py:74:see_memory_usage] Cache Allocated 26.916015625 GigaBytes
Tencent01: /opt/conda/lib/python3.7/site-packages/torch/cuda/memory.py:354: FutureWarning: torch.cuda.max_memory_cached has been renamed to torch.cuda.max_memory_reserved
Tencent01:   FutureWarning)
Tencent01: [2020-11-23 11:55:20,111] [INFO] [checkpointing.py:78:see_memory_usage] Max cache Allocated 26.916015625 GigaBytes
Tencent01: [2020-11-23 11:55:20,111] [INFO] [checkpointing.py:357:forward] Activation Checkpointing Information
Tencent01: [2020-11-23 11:55:20,111] [INFO] [checkpointing.py:359:forward] ----Partition Activations False, CPU CHECKPOINTING False
Tencent01: [2020-11-23 11:55:20,111] [INFO] [checkpointing.py:362:forward] ----contiguous Memory Checkpointing False with 40 total layers
Tencent01: [2020-11-23 11:55:20,111] [INFO] [checkpointing.py:364:forward] ----Synchronization False
Tencent01: [2020-11-23 11:55:20,111] [INFO] [checkpointing.py:365:forward] ----Profiling False
Tencent01: [2020-11-23 11:58:57,431] [INFO] [timer.py:157:stop] 0/100, SamplesPerSec=59.248970136359254
Tencent01: [2020-11-23 12:02:33,623] [INFO] [timer.py:157:stop] 0/200, SamplesPerSec=59.24864154770481
Tencent01: [2020-11-23 12:06:09,821] [INFO] [timer.py:157:stop] 0/300, SamplesPerSec=59.24850937529321
Tencent01: [2020-11-23 12:09:58,078] [INFO] [logging.py:60:log_dist] [Rank 0] step=70100, skipped=74, lr=[0.00013596800293404798, 0.00013596800293404798], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 12:09:58,129] [INFO] [timer.py:157:stop] 0/400, SamplesPerSec=58.42560097777036
Tencent01:  iteration    70100/  320000 | elapsed time per iteration (ms): 8780.4 | learning rate 1.360E-04 | lm loss 2.967103E+00 | loss scale 65536.0 |
Tencent01: /opt/conda/lib/python3.7/site-packages/torch/cuda/memory.py:346: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved
Tencent01:   FutureWarning)
Tencent01: after 70100 iterations memory (MB) | allocated: 13675.193359375 | max allocated: 23543.7509765625 | cached: 27564.0 | max cached: 27564.0
Tencent01: time (ms) | forward: 2472.89 | backward: 6185.50 | optimizer: 121.59 | batch generator: 5.66 | data loader: 1.04
Tencent02: /opt/conda/lib/python3.7/site-packages/torch/cuda/memory.py:346: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved
Tencent02:   FutureWarning)
Tencent01: /opt/conda/lib/python3.7/site-packages/torch/cuda/memory.py:346: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved
Tencent01:   FutureWarning)
Tencent02: /opt/conda/lib/python3.7/site-packages/torch/cuda/memory.py:346: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved
Tencent02:   FutureWarning)
Tencent02: /opt/conda/lib/python3.7/site-packages/torch/cuda/memory.py:346: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved
Tencent02:   FutureWarning)
Tencent02: /opt/conda/lib/python3.7/site-packages/torch/cuda/memory.py:346: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved
Tencent02:   FutureWarning)
Tencent02: /opt/conda/lib/python3.7/site-packages/torch/cuda/memory.py:346: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved
Tencent02:   FutureWarning)
Tencent01: /opt/conda/lib/python3.7/site-packages/torch/cuda/memory.py:346: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved
Tencent01:   FutureWarning)
Tencent01: /opt/conda/lib/python3.7/site-packages/torch/cuda/memory.py:346: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved
Tencent01:   FutureWarning)
Tencent01: /opt/conda/lib/python3.7/site-packages/torch/cuda/memory.py:346: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved
Tencent01:   FutureWarning)
Tencent02: /opt/conda/lib/python3.7/site-packages/torch/cuda/memory.py:346: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved
Tencent02:   FutureWarning)
Tencent02: /opt/conda/lib/python3.7/site-packages/torch/cuda/memory.py:346: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved
Tencent02:   FutureWarning)
Tencent01: /opt/conda/lib/python3.7/site-packages/torch/cuda/memory.py:346: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved
Tencent01:   FutureWarning)
Tencent01: /opt/conda/lib/python3.7/site-packages/torch/cuda/memory.py:346: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved
Tencent01:   FutureWarning)
Tencent02: /opt/conda/lib/python3.7/site-packages/torch/cuda/memory.py:346: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved
Tencent02:   FutureWarning)
Tencent01: [2020-11-23 12:13:34,411] [INFO] [timer.py:157:stop] 0/500, SamplesPerSec=58.58420316180379
Tencent01: [2020-11-23 12:17:10,610] [INFO] [timer.py:157:stop] 0/600, SamplesPerSec=58.69396862130223
Tencent01: [2020-11-23 12:20:46,722] [INFO] [timer.py:157:stop] 0/700, SamplesPerSec=58.775306970541514
Tencent01: [2020-11-23 12:24:22,877] [INFO] [logging.py:60:log_dist] [Rank 0] step=70200, skipped=74, lr=[0.00013592752937470826, 0.00013592752937470826], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 12:24:22,929] [INFO] [timer.py:157:stop] 0/800, SamplesPerSec=58.833503405974064
Tencent01:  iteration    70200/  320000 | elapsed time per iteration (ms): 8648.0 | learning rate 1.359E-04 | lm loss 2.981495E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 2357.42 | backward: 6168.34 | optimizer: 121.82 | batch generator: 5.36 | data loader: 1.05
Tencent01: [2020-11-23 12:27:59,076] [INFO] [timer.py:157:stop] 0/900, SamplesPerSec=58.880907418976086
Tencent01: [2020-11-23 12:31:35,189] [INFO] [timer.py:157:stop] 0/1000, SamplesPerSec=58.91957428452621
Tencent01: [2020-11-23 12:35:11,292] [INFO] [timer.py:157:stop] 0/1100, SamplesPerSec=58.9517734428768
Tencent01: [2020-11-23 12:38:59,091] [INFO] [logging.py:60:log_dist] [Rank 0] step=70300, skipped=74, lr=[0.00013588700432039426, 0.00013588700432039426], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 12:38:59,143] [INFO] [timer.py:157:stop] 0/1200, SamplesPerSec=58.713137652333565
Tencent01:  iteration    70300/  320000 | elapsed time per iteration (ms): 8762.1 | learning rate 1.359E-04 | lm loss 3.034592E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 2453.36 | backward: 6187.13 | optimizer: 121.29 | batch generator: 5.66 | data loader: 1.01
Tencent01: [2020-11-23 12:42:35,329] [INFO] [timer.py:157:stop] 0/1300, SamplesPerSec=58.75403807673286
Tencent01: [2020-11-23 12:46:11,471] [INFO] [timer.py:157:stop] 0/1400, SamplesPerSec=58.789972824411315
Tencent01: [2020-11-23 12:49:47,613] [INFO] [timer.py:157:stop] 0/1500, SamplesPerSec=58.82123853802852
Tencent01: [2020-11-23 12:53:23,731] [INFO] [logging.py:60:log_dist] [Rank 0] step=70400, skipped=74, lr=[0.00013584642781016516, 0.00013584642781016516], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 12:53:23,783] [INFO] [timer.py:157:stop] 0/1600, SamplesPerSec=58.8481972498385
Tencent01:  iteration    70400/  320000 | elapsed time per iteration (ms): 8646.4 | learning rate 1.358E-04 | lm loss 3.003351E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 2356.93 | backward: 6167.41 | optimizer: 121.68 | batch generator: 5.45 | data loader: 1.03
Tencent01: [2020-11-23 12:56:59,968] [INFO] [timer.py:157:stop] 0/1700, SamplesPerSec=58.87182756261336
Tencent01: [2020-11-23 13:00:36,131] [INFO] [timer.py:157:stop] 0/1800, SamplesPerSec=58.89316647392826
Tencent01: [2020-11-23 13:04:35,020] [INFO] [timer.py:157:stop] 0/1900, SamplesPerSec=58.58944148757126
Tencent01: [2020-11-23 13:08:11,210] [INFO] [logging.py:60:log_dist] [Rank 0] step=70500, skipped=74, lr=[0.00013580579988312978, 0.00013580579988312978], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 13:08:11,263] [INFO] [timer.py:157:stop] 0/2000, SamplesPerSec=58.621329650606945
Tencent01:  iteration    70500/  320000 | elapsed time per iteration (ms): 8874.8 | learning rate 1.358E-04 | lm loss 2.960498E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 2358.20 | backward: 6395.00 | optimizer: 121.23 | batch generator: 5.73 | data loader: 1.04
Tencent01: [2020-11-23 13:11:47,422] [INFO] [timer.py:157:stop] 0/2100, SamplesPerSec=58.65109820917698
Tencent01: [2020-11-23 13:15:23,543] [INFO] [timer.py:157:stop] 0/2200, SamplesPerSec=58.6788823306408
Tencent01: [2020-11-23 13:17:07,210] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-23 13:17:07,210] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-23 13:17:07,210] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-23 13:17:07,211] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-23 13:17:07,211] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-23 13:17:07,211] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-23 13:17:07,210] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-23 13:17:07,211] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-23 13:17:07,210] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-23 13:17:07,210] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-23 13:17:07,210] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-23 13:17:07,210] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-23 13:17:07,211] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-23 13:17:07,211] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-23 13:17:07,211] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-23 13:17:07,211] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-23 13:17:07,210] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-23 13:17:07,211] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-23 13:17:07,211] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-23 13:17:07,211] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-23 13:17:07,211] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-23 13:17:07,211] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-23 13:17:07,211] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-23 13:17:07,211] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-23 13:17:07,211] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-23 13:17:07,211] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-23 13:17:07,211] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-23 13:17:07,211] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-23 13:17:07,211] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-23 13:17:07,211] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-23 13:17:07,211] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-23 13:17:07,212] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-23 13:18:59,706] [INFO] [timer.py:157:stop] 0/2300, SamplesPerSec=58.70373491845641
Tencent01: [2020-11-23 13:22:35,810] [INFO] [logging.py:60:log_dist] [Rank 0] step=70600, skipped=74, lr=[0.00013576512057844644, 0.00013576512057844644], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 13:22:35,862] [INFO] [timer.py:157:stop] 0/2400, SamplesPerSec=58.72678382610538
Tencent01:  iteration    70600/  320000 | elapsed time per iteration (ms): 8646.0 | learning rate 1.358E-04 | lm loss 2.962568E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2357.07 | backward: 6167.41 | optimizer: 121.16 | batch generator: 5.49 | data loader: 0.95
Tencent01: [2020-11-23 13:26:12,002] [INFO] [timer.py:157:stop] 0/2500, SamplesPerSec=58.74814301648753
Tencent01: [2020-11-23 13:30:08,580] [INFO] [timer.py:157:stop] 0/2600, SamplesPerSec=58.556311696396456
Tencent01: [2020-11-23 13:33:44,925] [INFO] [timer.py:157:stop] 0/2700, SamplesPerSec=58.580107584022734
Tencent01: [2020-11-23 13:37:21,074] [INFO] [logging.py:60:log_dist] [Rank 0] step=70700, skipped=74, lr=[0.0001357243899353231, 0.0001357243899353231], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 13:37:21,125] [INFO] [timer.py:157:stop] 0/2800, SamplesPerSec=58.60358426301425
Tencent01:  iteration    70700/  320000 | elapsed time per iteration (ms): 8852.6 | learning rate 1.357E-04 | lm loss 3.021089E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2451.43 | backward: 6279.55 | optimizer: 121.29 | batch generator: 5.57 | data loader: 1.00
Tencent01: [2020-11-23 13:40:57,371] [INFO] [timer.py:157:stop] 0/2900, SamplesPerSec=58.62513974203521
Tencent01: [2020-11-23 13:44:33,604] [INFO] [timer.py:157:stop] 0/3000, SamplesPerSec=58.64536692153942
Tencent01: [2020-11-23 13:48:09,786] [INFO] [timer.py:157:stop] 0/3100, SamplesPerSec=58.664700691475005
Tencent01: [2020-11-23 13:51:45,960] [INFO] [logging.py:60:log_dist] [Rank 0] step=70800, skipped=74, lr=[0.00013568360799301698, 0.00013568360799301698], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 13:51:46,012] [INFO] [timer.py:157:stop] 0/3200, SamplesPerSec=58.68256158118721
Tencent01:  iteration    70800/  320000 | elapsed time per iteration (ms): 8648.9 | learning rate 1.357E-04 | lm loss 2.982319E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2358.19 | backward: 6168.15 | optimizer: 122.11 | batch generator: 5.62 | data loader: 1.11
Tencent01: [2020-11-23 13:55:40,035] [INFO] [timer.py:157:stop] 0/3300, SamplesPerSec=58.55443055314507
Tencent01: [2020-11-23 13:59:27,902] [INFO] [timer.py:157:stop] 0/3400, SamplesPerSec=58.482745121298095
Tencent01: [2020-11-23 14:03:04,194] [INFO] [timer.py:157:stop] 0/3500, SamplesPerSec=58.50361504222732
Tencent01: [2020-11-23 14:06:40,306] [INFO] [logging.py:60:log_dist] [Rank 0] step=70900, skipped=74, lr=[0.00013564277479083498, 0.00013564277479083498], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 14:06:40,359] [INFO] [timer.py:157:stop] 0/3600, SamplesPerSec=58.52429498774722
Tencent01:  iteration    70900/  320000 | elapsed time per iteration (ms): 8943.5 | learning rate 1.356E-04 | lm loss 2.984891E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2436.24 | backward: 6385.52 | optimizer: 121.34 | batch generator: 5.79 | data loader: 1.05
Tencent01: [2020-11-23 14:10:16,514] [INFO] [timer.py:157:stop] 0/3700, SamplesPerSec=58.54394770629181
Tencent01: [2020-11-23 14:13:52,668] [INFO] [timer.py:157:stop] 0/3800, SamplesPerSec=58.56256500293491
Tencent01: [2020-11-23 14:17:28,854] [INFO] [timer.py:157:stop] 0/3900, SamplesPerSec=58.57997968727661
Tencent01: [2020-11-23 14:21:12,419] [INFO] [logging.py:60:log_dist] [Rank 0] step=71000, skipped=74, lr=[0.00013560189036813326, 0.00013560189036813326], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 14:21:12,471] [INFO] [timer.py:157:stop] 0/4000, SamplesPerSec=58.54670835253983
Tencent01:  iteration    71000/  320000 | elapsed time per iteration (ms): 8721.1 | learning rate 1.356E-04 | lm loss 2.984577E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2434.02 | backward: 6165.68 | optimizer: 121.08 | batch generator: 5.59 | data loader: 0.98
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: -----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 71000 | LM loss: 2.966644E+00 | LM PPL: 1.942661E+01
Tencent01: -----------------------------------------------------------------------------------
Tencent01: [2020-11-23 14:25:56,998] [INFO] [timer.py:157:stop] 0/4100, SamplesPerSec=58.47642846460454
Tencent01: [2020-11-23 14:29:33,283] [INFO] [timer.py:157:stop] 0/4200, SamplesPerSec=58.493959507150805
Tencent01: [2020-11-23 14:33:09,461] [INFO] [timer.py:157:stop] 0/4300, SamplesPerSec=58.511470956256275
Tencent01: [2020-11-23 14:36:45,640] [INFO] [logging.py:60:log_dist] [Rank 0] step=71100, skipped=74, lr=[0.0001355609547643174, 0.0001355609547643174], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 14:36:45,692] [INFO] [timer.py:157:stop] 0/4400, SamplesPerSec=58.52775968015373
Tencent01:  iteration    71100/  320000 | elapsed time per iteration (ms): 9332.2 | learning rate 1.356E-04 | lm loss 2.955512E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2357.85 | backward: 6303.04 | optimizer: 121.12 | batch generator: 6.60 | data loader: 1.11
Tencent01: [2020-11-23 14:40:21,960] [INFO] [timer.py:157:stop] 0/4500, SamplesPerSec=58.54313121748765
Tencent01: [2020-11-23 14:43:58,160] [INFO] [timer.py:157:stop] 0/4600, SamplesPerSec=58.55826213870604
Tencent01: [2020-11-23 14:47:34,318] [INFO] [timer.py:157:stop] 0/4700, SamplesPerSec=58.572910413813
Tencent01: [2020-11-23 14:51:39,433] [INFO] [logging.py:60:log_dist] [Rank 0] step=71200, skipped=74, lr=[0.0001355199680188423, 0.0001355199680188423], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 14:51:39,485] [INFO] [timer.py:157:stop] 0/4800, SamplesPerSec=58.42534889732615
Tencent01:  iteration    71200/  320000 | elapsed time per iteration (ms): 8937.9 | learning rate 1.355E-04 | lm loss 2.976204E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2433.68 | backward: 6382.79 | optimizer: 121.11 | batch generator: 5.50 | data loader: 0.98
Tencent01: [2020-11-23 14:55:15,849] [INFO] [timer.py:157:stop] 0/4900, SamplesPerSec=58.441006356757754
Tencent01: [2020-11-23 14:58:52,109] [INFO] [timer.py:157:stop] 0/5000, SamplesPerSec=58.45664612910131
Tencent01: [2020-11-23 15:02:28,265] [INFO] [timer.py:157:stop] 0/5100, SamplesPerSec=58.47210957848539
Tencent01: [2020-11-23 15:06:04,331] [INFO] [logging.py:60:log_dist] [Rank 0] step=71300, skipped=74, lr=[0.00013547893017121213, 0.00013547893017121213], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 15:06:04,383] [INFO] [timer.py:157:stop] 0/5200, SamplesPerSec=58.48729318394443
Tencent01:  iteration    71300/  320000 | elapsed time per iteration (ms): 8649.0 | learning rate 1.355E-04 | lm loss 2.998855E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2358.71 | backward: 6168.66 | optimizer: 121.24 | batch generator: 5.74 | data loader: 1.02
Tencent01: [2020-11-23 15:09:40,548] [INFO] [timer.py:157:stop] 0/5300, SamplesPerSec=58.50156319512542
Tencent01: [2020-11-23 15:11:58,739] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 71340
Tencent01: [2020-11-23 15:11:58,739] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-23 15:11:58,739] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 71340
Tencent01: [2020-11-23 15:11:58,739] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-23 15:11:58,739] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 71340
Tencent01: [2020-11-23 15:11:58,739] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 71340
Tencent01: [2020-11-23 15:11:58,739] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-23 15:11:58,739] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 71340
Tencent01: [2020-11-23 15:11:58,739] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 71340
Tencent01: [2020-11-23 15:11:58,739] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-23 15:11:58,739] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-23 15:11:58,739] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-23 15:11:58,740] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Tencent02: [2020-11-23 15:11:58,738] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 71340
Tencent02: [2020-11-23 15:11:58,738] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 71340
Tencent02: [2020-11-23 15:11:58,738] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 71340
Tencent02: [2020-11-23 15:11:58,738] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-23 15:11:58,738] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-23 15:11:58,738] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-23 15:11:58,738] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 71340
Tencent02: [2020-11-23 15:11:58,738] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 71340
Tencent02: [2020-11-23 15:11:58,739] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-23 15:11:58,739] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 71340
Tencent02: [2020-11-23 15:11:58,739] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-23 15:11:58,739] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 71340
Tencent02: [2020-11-23 15:11:58,739] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-23 15:11:58,739] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: [2020-11-23 15:11:58,739] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: Grad overflow on iteration 71340
Tencent01: [2020-11-23 15:11:58,740] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 71340
Tencent01: [2020-11-23 15:11:58,740] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-23 15:11:58,740] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent02: [2020-11-23 15:11:58,739] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 71340
Tencent02: [2020-11-23 15:11:58,739] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Tencent01: [2020-11-23 15:13:16,540] [INFO] [timer.py:157:stop] 0/5400, SamplesPerSec=58.51619868178214
Tencent01: [2020-11-23 15:17:20,723] [INFO] [timer.py:157:stop] 0/5500, SamplesPerSec=58.39346550779423
Tencent01: [2020-11-23 15:21:07,325] [INFO] [logging.py:60:log_dist] [Rank 0] step=71400, skipped=75, lr=[0.0001354382524027126, 0.0001354382524027126], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 15:21:07,377] [INFO] [timer.py:157:stop] 0/5600, SamplesPerSec=58.35876696987808
Tencent01:  iteration    71400/  320000 | elapsed time per iteration (ms): 9029.9 | learning rate 1.354E-04 | lm loss 3.004952E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 2436.72 | backward: 6472.91 | optimizer: 119.94 | batch generator: 5.56 | data loader: 1.02
Tencent01: [2020-11-23 15:24:43,640] [INFO] [timer.py:157:stop] 0/5700, SamplesPerSec=58.373836124404136
Tencent01: [2020-11-23 15:28:19,855] [INFO] [timer.py:157:stop] 0/5800, SamplesPerSec=58.38856928735774
Tencent01: [2020-11-23 15:31:56,058] [INFO] [timer.py:157:stop] 0/5900, SamplesPerSec=58.40293789789838
Tencent01: [2020-11-23 15:35:32,183] [INFO] [logging.py:60:log_dist] [Rank 0] step=71500, skipped=75, lr=[0.0001353971129795157, 0.0001353971129795157], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 15:35:32,235] [INFO] [timer.py:157:stop] 0/6000, SamplesPerSec=58.4168782452846
Tencent01:  iteration    71500/  320000 | elapsed time per iteration (ms): 8648.6 | learning rate 1.354E-04 | lm loss 2.984360E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 2357.32 | backward: 6169.87 | optimizer: 121.04 | batch generator: 5.62 | data loader: 0.99
Tencent01: [2020-11-23 15:39:08,409] [INFO] [timer.py:157:stop] 0/6100, SamplesPerSec=58.430415769110624
Tencent01: [2020-11-23 15:43:02,095] [INFO] [timer.py:157:stop] 0/6200, SamplesPerSec=58.36822596921737
Tencent01: [2020-11-23 15:47:00,139] [INFO] [timer.py:157:stop] 0/6300, SamplesPerSec=58.28974349775341
Tencent01: [2020-11-23 15:50:36,369] [INFO] [logging.py:60:log_dist] [Rank 0] step=71600, skipped=75, lr=[0.00013535592257257498, 0.00013535592257257498], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 15:50:36,420] [INFO] [timer.py:157:stop] 0/6400, SamplesPerSec=58.3041071233083
Tencent01:  iteration    71600/  320000 | elapsed time per iteration (ms): 9041.8 | learning rate 1.354E-04 | lm loss 2.973571E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 2358.19 | backward: 6562.20 | optimizer: 121.10 | batch generator: 5.58 | data loader: 0.98
Tencent01: [2020-11-23 15:54:12,593] [INFO] [timer.py:157:stop] 0/6500, SamplesPerSec=58.31849177308304
Tencent01: [2020-11-23 15:57:48,702] [INFO] [timer.py:157:stop] 0/6600, SamplesPerSec=58.33271484585986
Tencent01: [2020-11-23 16:01:24,853] [INFO] [timer.py:157:stop] 0/6700, SamplesPerSec=58.34633988402308
Tencent01: [2020-11-23 16:05:00,942] [INFO] [logging.py:60:log_dist] [Rank 0] step=71700, skipped=75, lr=[0.00013531468122159084, 0.00013531468122159084], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 16:05:00,994] [INFO] [timer.py:157:stop] 0/6800, SamplesPerSec=58.359623031008304
Tencent01:  iteration    71700/  320000 | elapsed time per iteration (ms): 8645.7 | learning rate 1.353E-04 | lm loss 2.978256E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 2357.85 | backward: 6166.36 | optimizer: 121.17 | batch generator: 5.65 | data loader: 0.99
Tencent01: [2020-11-23 16:08:44,588] [INFO] [timer.py:157:stop] 0/6900, SamplesPerSec=58.34376318918465
Tencent01: [2020-11-23 16:12:52,486] [INFO] [timer.py:157:stop] 0/7000, SamplesPerSec=58.23622555757634
Tencent01: [2020-11-23 16:16:28,827] [INFO] [timer.py:157:stop] 0/7100, SamplesPerSec=58.24968710548249
Tencent01: [2020-11-23 16:20:04,970] [INFO] [logging.py:60:log_dist] [Rank 0] step=71800, skipped=75, lr=[0.00013527338896631293, 0.00013527338896631293], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 16:20:05,022] [INFO] [timer.py:157:stop] 0/7200, SamplesPerSec=58.26331938580821
Tencent01:  iteration    71800/  320000 | elapsed time per iteration (ms): 9040.3 | learning rate 1.353E-04 | lm loss 2.951531E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 2435.50 | backward: 6483.27 | optimizer: 121.15 | batch generator: 5.63 | data loader: 1.01
Tencent01: [2020-11-23 16:23:41,159] [INFO] [timer.py:157:stop] 0/7300, SamplesPerSec=58.27679631820694
Tencent01: [2020-11-23 16:27:17,239] [INFO] [timer.py:157:stop] 0/7400, SamplesPerSec=58.29011196884314
Tencent01: [2020-11-23 16:30:53,297] [INFO] [timer.py:157:stop] 0/7500, SamplesPerSec=58.30322076806433
Tencent01: [2020-11-23 16:34:36,799] [INFO] [logging.py:60:log_dist] [Rank 0] step=71900, skipped=75, lr=[0.0001352320458465399, 0.0001352320458465399], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 16:34:36,852] [INFO] [timer.py:157:stop] 0/7600, SamplesPerSec=58.28975428690237
Tencent01:  iteration    71900/  320000 | elapsed time per iteration (ms): 8718.3 | learning rate 1.352E-04 | lm loss 2.992231E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 2433.82 | backward: 6162.63 | optimizer: 121.46 | batch generator: 5.76 | data loader: 1.07
Tencent01: [2020-11-23 16:38:34,607] [INFO] [timer.py:157:stop] 0/7700, SamplesPerSec=58.22769942209557
Tencent01: [2020-11-23 16:42:10,849] [INFO] [timer.py:157:stop] 0/7800, SamplesPerSec=58.240394178490114
Tencent01: [2020-11-23 16:45:47,028] [INFO] [timer.py:157:stop] 0/7900, SamplesPerSec=58.252950551781524
Tencent01: [2020-11-23 16:49:23,010] [INFO] [logging.py:60:log_dist] [Rank 0] step=72000, skipped=75, lr=[0.0001351906519021194, 0.0001351906519021194], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 16:49:23,062] [INFO] [timer.py:157:stop] 0/8000, SamplesPerSec=58.26573307218982
Tencent01:  iteration    72000/  320000 | elapsed time per iteration (ms): 8862.1 | learning rate 1.352E-04 | lm loss 3.013762E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 2358.52 | backward: 6382.10 | optimizer: 121.13 | batch generator: 5.48 | data loader: 0.98
Tencent01: [2020-11-23 16:49:23,066] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: /root/data/checkpoints/gpt-345M11-14-13-21/72000/mp_rank_00_model_states.pt
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: -----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 72000 | LM loss: 2.954876E+00 | LM PPL: 1.919935E+01
Tencent01: -----------------------------------------------------------------------------------
Tencent01: [2020-11-23 16:54:56,340] [INFO] [timer.py:157:stop] 0/8100, SamplesPerSec=58.2723067952805
Tencent01: [2020-11-23 16:58:32,424] [INFO] [timer.py:157:stop] 0/8200, SamplesPerSec=58.28436900883249
Tencent01: [2020-11-23 17:02:18,946] [INFO] [timer.py:157:stop] 0/8300, SamplesPerSec=58.26279050852177
Tencent01: [2020-11-23 17:06:27,356] [INFO] [logging.py:60:log_dist] [Rank 0] step=72100, skipped=75, lr=[0.0001351492071729481, 0.0001351492071729481], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 17:06:27,408] [INFO] [timer.py:157:stop] 0/8400, SamplesPerSec=58.17258053437779
Tencent01:  iteration    72100/  320000 | elapsed time per iteration (ms): 10243.5 | learning rate 1.351E-04 | lm loss 3.001716E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 2464.62 | backward: 6502.66 | optimizer: 121.36 | batch generator: 6.74 | data loader: 1.16
Tencent01: [2020-11-23 17:10:03,734] [INFO] [timer.py:157:stop] 0/8500, SamplesPerSec=58.18456734715209
Tencent01: [2020-11-23 17:13:39,902] [INFO] [timer.py:157:stop] 0/8600, SamplesPerSec=58.19677622460911
Tencent01: [2020-11-23 17:17:15,953] [INFO] [timer.py:157:stop] 0/8700, SamplesPerSec=58.2090783934032
Tencent01: [2020-11-23 17:20:51,988] [INFO] [logging.py:60:log_dist] [Rank 0] step=72200, skipped=75, lr=[0.00013510771169897165, 0.00013510771169897165], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 17:20:52,041] [INFO] [timer.py:157:stop] 0/8800, SamplesPerSec=58.220997992029226
Tencent01:  iteration    72200/  320000 | elapsed time per iteration (ms): 8646.3 | learning rate 1.351E-04 | lm loss 3.000670E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 2357.71 | backward: 6167.18 | optimizer: 121.09 | batch generator: 5.42 | data loader: 0.94
Tencent01: [2020-11-23 17:24:28,094] [INFO] [timer.py:157:stop] 0/8900, SamplesPerSec=58.23272902299316
Tencent01: [2020-11-23 17:28:04,222] [INFO] [timer.py:157:stop] 0/9000, SamplesPerSec=58.24402837619398
Tencent01: [2020-11-23 17:31:59,592] [INFO] [timer.py:157:stop] 0/9100, SamplesPerSec=58.19906744432513
Tencent01: [2020-11-23 17:35:47,551] [INFO] [logging.py:60:log_dist] [Rank 0] step=72300, skipped=75, lr=[0.00013506616552018448, 0.00013506616552018448], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 17:35:47,603] [INFO] [timer.py:157:stop] 0/9200, SamplesPerSec=58.176283918926686
Tencent01:  iteration    72300/  320000 | elapsed time per iteration (ms): 8955.6 | learning rate 1.351E-04 | lm loss 2.918847E+00 | loss scale 65536.0 |
Tencent01: time (ms) | forward: 2357.97 | backward: 6476.15 | optimizer: 121.15 | batch generator: 5.60 | data loader: 1.00
Tencent01: [2020-11-23 17:39:23,837] [INFO] [timer.py:157:stop] 0/9300, SamplesPerSec=58.18746780258932
Tencent01: [2020-11-23 17:41:50,731] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-23 17:41:50,731] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-23 17:41:50,731] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-23 17:41:50,732] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-23 17:41:50,732] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-23 17:41:50,732] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-23 17:41:50,732] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-23 17:41:50,732] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-23 17:41:50,732] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-23 17:41:50,732] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-23 17:41:50,732] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-23 17:41:50,732] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-23 17:41:50,732] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-23 17:41:50,732] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-23 17:41:50,732] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-23 17:41:50,733] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-23 17:41:50,733] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-23 17:41:50,733] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-23 17:41:50,733] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-23 17:41:50,733] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-23 17:41:50,733] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-23 17:41:50,733] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-23 17:41:50,733] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-23 17:41:50,733] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-23 17:41:50,733] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-23 17:41:50,733] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-23 17:41:50,733] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-23 17:41:50,733] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-23 17:41:50,733] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-23 17:41:50,733] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent02: [2020-11-23 17:41:50,733] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-23 17:41:50,733] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Tencent01: [2020-11-23 17:42:59,986] [INFO] [timer.py:157:stop] 0/9400, SamplesPerSec=58.198678727826696
Tencent01: [2020-11-23 17:46:36,098] [INFO] [timer.py:157:stop] 0/9500, SamplesPerSec=58.2097565144658
Tencent01: [2020-11-23 17:50:12,244] [INFO] [logging.py:60:log_dist] [Rank 0] step=72400, skipped=75, lr=[0.00013502456867663006, 0.00013502456867663006], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 17:50:12,296] [INFO] [timer.py:157:stop] 0/9600, SamplesPerSec=58.220379621980975
Tencent01:  iteration    72400/  320000 | elapsed time per iteration (ms): 8646.9 | learning rate 1.350E-04 | lm loss 2.994084E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2357.65 | backward: 6167.57 | optimizer: 121.36 | batch generator: 5.53 | data loader: 0.97
Tencent01: [2020-11-23 17:53:48,428] [INFO] [timer.py:157:stop] 0/9700, SamplesPerSec=58.231006242833686
Tencent01: [2020-11-23 17:57:44,621] [INFO] [timer.py:157:stop] 0/9800, SamplesPerSec=58.187194613655116
Tencent01: [2020-11-23 18:01:41,827] [INFO] [timer.py:157:stop] 0/9900, SamplesPerSec=58.141603925959444
Tencent01: [2020-11-23 18:05:18,035] [INFO] [logging.py:60:log_dist] [Rank 0] step=72500, skipped=75, lr=[0.0001349829212084006, 0.0001349829212084006], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 18:05:18,087] [INFO] [timer.py:157:stop] 0/10000, SamplesPerSec=58.152316059862244
Tencent01:  iteration    72500/  320000 | elapsed time per iteration (ms): 9057.9 | learning rate 1.350E-04 | lm loss 2.999131E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2359.01 | backward: 6577.11 | optimizer: 121.41 | batch generator: 5.91 | data loader: 1.08
Tencent01: [2020-11-23 18:08:54,230] [INFO] [timer.py:157:stop] 0/10100, SamplesPerSec=58.16316581087992
Tencent01: [2020-11-23 18:12:30,335] [INFO] [timer.py:157:stop] 0/10200, SamplesPerSec=58.173819271677004
Tencent01: [2020-11-23 18:16:06,431] [INFO] [timer.py:157:stop] 0/10300, SamplesPerSec=58.18426387508529
Tencent01: [2020-11-23 18:19:42,495] [INFO] [logging.py:60:log_dist] [Rank 0] step=72600, skipped=75, lr=[0.00013494122315563708, 0.00013494122315563708], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 18:19:42,547] [INFO] [timer.py:157:stop] 0/10400, SamplesPerSec=58.19454364980557
Tencent01:  iteration    72600/  320000 | elapsed time per iteration (ms): 8644.6 | learning rate 1.349E-04 | lm loss 2.978284E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2357.56 | backward: 6164.89 | optimizer: 121.75 | batch generator: 5.65 | data loader: 1.08
Tencent01: [2020-11-23 18:23:29,747] [INFO] [timer.py:157:stop] 0/10500, SamplesPerSec=58.17670879392643
Tencent01: [2020-11-23 18:27:36,836] [INFO] [timer.py:157:stop] 0/10600, SamplesPerSec=58.10967725724332
Tencent01: [2020-11-23 18:31:13,241] [INFO] [timer.py:157:stop] 0/10700, SamplesPerSec=58.11958884844321
Tencent01: [2020-11-23 18:34:49,396] [INFO] [logging.py:60:log_dist] [Rank 0] step=72700, skipped=75, lr=[0.0001348994745585293, 0.0001348994745585293], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 18:34:49,448] [INFO] [timer.py:157:stop] 0/10800, SamplesPerSec=58.129851263033096
Tencent01:  iteration    72700/  320000 | elapsed time per iteration (ms): 9069.0 | learning rate 1.349E-04 | lm loss 2.971183E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2359.41 | backward: 6587.35 | optimizer: 121.85 | batch generator: 6.04 | data loader: 1.14
Tencent01: [2020-11-23 18:38:25,618] [INFO] [timer.py:157:stop] 0/10900, SamplesPerSec=58.139983699096405
Tencent01: [2020-11-23 18:42:01,730] [INFO] [timer.py:157:stop] 0/11000, SamplesPerSec=58.150057424002455
Tencent01: [2020-11-23 18:45:37,895] [INFO] [timer.py:157:stop] 0/11100, SamplesPerSec=58.15985391294405
Tencent01: [2020-11-23 18:49:23,106] [INFO] [logging.py:60:log_dist] [Rank 0] step=72800, skipped=75, lr=[0.00013485767545731576, 0.00013485767545731576], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 18:49:23,159] [INFO] [timer.py:157:stop] 0/11200, SamplesPerSec=58.14800640346106
Tencent01:  iteration    72800/  320000 | elapsed time per iteration (ms): 8737.1 | learning rate 1.349E-04 | lm loss 2.977266E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2450.31 | backward: 6165.17 | optimizer: 121.26 | batch generator: 5.69 | data loader: 1.02
Tencent01: [2020-11-23 18:53:40,688] [INFO] [timer.py:157:stop] 0/11300, SamplesPerSec=58.061045132839695
Tencent01: [2020-11-23 18:57:17,194] [INFO] [timer.py:157:stop] 0/11400, SamplesPerSec=58.070499660439765
Tencent01: [2020-11-23 19:00:53,454] [INFO] [timer.py:157:stop] 0/11500, SamplesPerSec=58.080348112282095
Tencent01: [2020-11-23 19:04:29,514] [INFO] [logging.py:60:log_dist] [Rank 0] step=72900, skipped=75, lr=[0.00013481582589228362, 0.00013481582589228362], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 19:04:29,566] [INFO] [timer.py:157:stop] 0/11600, SamplesPerSec=58.090414622366424
Tencent01:  iteration    72900/  320000 | elapsed time per iteration (ms): 9064.1 | learning rate 1.348E-04 | lm loss 2.922864E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2357.66 | backward: 6584.83 | optimizer: 121.23 | batch generator: 5.34 | data loader: 0.96
Tencent01: [2020-11-23 19:08:05,687] [INFO] [timer.py:157:stop] 0/11700, SamplesPerSec=58.10032906842586
Tencent01: [2020-11-23 19:11:41,807] [INFO] [timer.py:157:stop] 0/11800, SamplesPerSec=58.110040499931586
Tencent01: [2020-11-23 19:15:17,966] [INFO] [timer.py:157:stop] 0/11900, SamplesPerSec=58.11949455318046
Tencent01: [2020-11-23 19:19:36,186] [INFO] [logging.py:60:log_dist] [Rank 0] step=73000, skipped=75, lr=[0.00013477392590376865, 0.00013477392590376865], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 19:19:36,238] [INFO] [timer.py:157:stop] 0/12000, SamplesPerSec=58.03634596540053
Tencent01:  iteration    73000/  320000 | elapsed time per iteration (ms): 9066.7 | learning rate 1.348E-04 | lm loss 2.975064E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2466.18 | backward: 6478.88 | optimizer: 121.28 | batch generator: 6.02 | data loader: 1.09
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: -----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 73000 | LM loss: 2.915036E+00 | LM PPL: 1.844948E+01
Tencent01: -----------------------------------------------------------------------------------
Tencent01: [2020-11-23 19:24:19,391] [INFO] [timer.py:157:stop] 0/12100, SamplesPerSec=58.0202843091099
Tencent01: [2020-11-23 19:27:55,614] [INFO] [timer.py:157:stop] 0/12200, SamplesPerSec=58.03005344815669
Tencent01: [2020-11-23 19:31:31,733] [INFO] [timer.py:157:stop] 0/12300, SamplesPerSec=58.03988304500024
Tencent01: [2020-11-23 19:35:07,792] [INFO] [logging.py:60:log_dist] [Rank 0] step=73100, skipped=75, lr=[0.00013473197553215524, 0.00013473197553215524], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 19:35:07,845] [INFO] [timer.py:157:stop] 0/12400, SamplesPerSec=58.04963329830452
Tencent01:  iteration    73100/  320000 | elapsed time per iteration (ms): 9316.1 | learning rate 1.347E-04 | lm loss 2.939553E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2358.37 | backward: 6284.98 | optimizer: 121.78 | batch generator: 6.49 | data loader: 1.14
Tencent01: [2020-11-23 19:38:43,930] [INFO] [timer.py:157:stop] 0/12500, SamplesPerSec=58.05926304949915
Tencent01: [2020-11-23 19:42:20,037] [INFO] [timer.py:157:stop] 0/12600, SamplesPerSec=58.0687171470391
Tencent01: [2020-11-23 19:46:24,901] [INFO] [timer.py:157:stop] 0/12700, SamplesPerSec=58.01838111766138
Tencent01: [2020-11-23 19:50:42,108] [INFO] [logging.py:60:log_dist] [Rank 0] step=73200, skipped=75, lr=[0.0001346899748178764, 0.0001346899748178764], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 19:50:42,160] [INFO] [timer.py:157:stop] 0/12800, SamplesPerSec=57.94350860491202
Tencent01:  iteration    73200/  320000 | elapsed time per iteration (ms): 9343.2 | learning rate 1.347E-04 | lm loss 3.006362E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2451.59 | backward: 6770.02 | optimizer: 121.19 | batch generator: 5.74 | data loader: 1.03
Tencent01: [2020-11-23 19:54:18,499] [INFO] [timer.py:157:stop] 0/12900, SamplesPerSec=57.95310914479138
Tencent01: [2020-11-23 19:57:54,647] [INFO] [timer.py:157:stop] 0/13000, SamplesPerSec=57.96295347415394
Tencent01: [2020-11-23 20:01:30,814] [INFO] [timer.py:157:stop] 0/13100, SamplesPerSec=57.97263529153284
Tencent01: [2020-11-23 20:05:06,839] [INFO] [logging.py:60:log_dist] [Rank 0] step=73300, skipped=75, lr=[0.0001346479238014136, 0.0001346479238014136], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 20:05:06,892] [INFO] [timer.py:157:stop] 0/13200, SamplesPerSec=57.98234063402772
Tencent01:  iteration    73300/  320000 | elapsed time per iteration (ms): 8647.3 | learning rate 1.346E-04 | lm loss 3.010078E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2357.85 | backward: 6167.46 | optimizer: 121.62 | batch generator: 5.83 | data loader: 1.04
Tencent01: [2020-11-23 20:08:43,028] [INFO] [timer.py:157:stop] 0/13300, SamplesPerSec=57.99177320346686
Tencent01: [2020-11-23 20:11:20,441] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-23 20:11:20,441] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-23 20:11:20,441] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-23 20:11:20,441] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-23 20:11:20,441] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-23 20:11:20,441] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-23 20:11:20,441] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-23 20:11:20,441] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-23 20:11:20,441] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-23 20:11:20,441] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-23 20:11:20,441] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-23 20:11:20,442] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-23 20:11:20,442] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-23 20:11:20,442] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-23 20:11:20,442] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-23 20:11:20,442] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-23 20:11:20,442] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-23 20:11:20,442] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-23 20:11:20,442] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-23 20:11:20,442] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-23 20:11:20,442] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-23 20:11:20,442] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-23 20:11:20,442] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-23 20:11:20,442] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-23 20:11:20,442] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-23 20:11:20,442] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-23 20:11:20,442] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-23 20:11:20,442] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-23 20:11:20,442] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-23 20:11:20,442] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-23 20:11:20,442] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-23 20:11:20,442] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-23 20:12:39,252] [INFO] [timer.py:157:stop] 0/13400, SamplesPerSec=57.96165522342915
Tencent01: [2020-11-23 20:17:06,701] [INFO] [timer.py:157:stop] 0/13500, SamplesPerSec=57.8714874091656
Tencent01: [2020-11-23 20:20:43,036] [INFO] [logging.py:60:log_dist] [Rank 0] step=73400, skipped=75, lr=[0.0001346058225232968, 0.0001346058225232968], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 20:20:43,088] [INFO] [timer.py:157:stop] 0/13600, SamplesPerSec=57.881025878614686
Tencent01:  iteration    73400/  320000 | elapsed time per iteration (ms): 9362.0 | learning rate 1.346E-04 | lm loss 3.006216E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2465.13 | backward: 6775.03 | optimizer: 121.42 | batch generator: 5.70 | data loader: 1.04
Tencent01: [2020-11-23 20:24:19,266] [INFO] [timer.py:157:stop] 0/13700, SamplesPerSec=57.8908090115747
Tencent01: [2020-11-23 20:27:55,439] [INFO] [timer.py:157:stop] 0/13800, SamplesPerSec=57.90049366802756
Tencent01: [2020-11-23 20:28:29,906] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 73453
Tencent01: [2020-11-23 20:28:29,906] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-23 20:28:29,906] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 73453
Tencent01: [2020-11-23 20:28:29,906] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 73453
Tencent01: [2020-11-23 20:28:29,906] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 73453
Tencent01: [2020-11-23 20:28:29,906] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 73453
Tencent01: [2020-11-23 20:28:29,906] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-23 20:28:29,906] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-23 20:28:29,906] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 73453
Tencent01: [2020-11-23 20:28:29,906] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-23 20:28:29,906] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-23 20:28:29,906] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 73453
Tencent01: [2020-11-23 20:28:29,906] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-23 20:28:29,906] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Tencent01: [2020-11-23 20:28:29,906] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 73453
Tencent01: [2020-11-23 20:28:29,907] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-23 20:28:29,907] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-23 20:28:29,907] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 73453
Tencent02: [2020-11-23 20:28:29,907] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 73453
Tencent02: [2020-11-23 20:28:29,907] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 73453
Tencent02: [2020-11-23 20:28:29,907] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-23 20:28:29,907] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-23 20:28:29,907] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-23 20:28:29,907] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 73453
Tencent02: [2020-11-23 20:28:29,907] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 73453
Tencent02: [2020-11-23 20:28:29,907] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 73453
Tencent02: [2020-11-23 20:28:29,907] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 73453
Tencent02: [2020-11-23 20:28:29,907] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 73453
Tencent02: [2020-11-23 20:28:29,907] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-23 20:28:29,907] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-23 20:28:29,907] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-23 20:28:29,907] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-23 20:28:29,907] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-23 20:31:31,426] [INFO] [timer.py:157:stop] 0/13900, SamplesPerSec=57.910356111856466
Tencent01: [2020-11-23 20:35:07,460] [INFO] [logging.py:60:log_dist] [Rank 0] step=73500, skipped=76, lr=[0.00013456409278755728, 0.00013456409278755728], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 20:35:07,513] [INFO] [timer.py:157:stop] 0/14000, SamplesPerSec=57.91987422066717
Tencent01:  iteration    73500/  320000 | elapsed time per iteration (ms): 8644.2 | learning rate 1.346E-04 | lm loss 3.018026E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2357.54 | backward: 6166.29 | optimizer: 120.05 | batch generator: 5.62 | data loader: 1.02
Tencent01: [2020-11-23 20:39:03,094] [INFO] [timer.py:157:stop] 0/14100, SamplesPerSec=57.893058586564074
Tencent01: [2020-11-23 20:43:29,289] [INFO] [timer.py:157:stop] 0/14200, SamplesPerSec=57.8102815369144
Tencent01: [2020-11-23 20:47:05,672] [INFO] [timer.py:157:stop] 0/14300, SamplesPerSec=57.81970427948224
Tencent01: [2020-11-23 20:50:41,764] [INFO] [logging.py:60:log_dist] [Rank 0] step=73600, skipped=76, lr=[0.00013452189160951936, 0.00013452189160951936], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 20:50:41,817] [INFO] [timer.py:157:stop] 0/14400, SamplesPerSec=57.829451421251676
Tencent01:  iteration    73600/  320000 | elapsed time per iteration (ms): 9343.0 | learning rate 1.345E-04 | lm loss 2.972985E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2451.19 | backward: 6770.37 | optimizer: 121.13 | batch generator: 5.22 | data loader: 0.94
Tencent01: [2020-11-23 20:54:17,905] [INFO] [timer.py:157:stop] 0/14500, SamplesPerSec=57.839195409040336
Tencent01: [2020-11-23 20:57:54,049] [INFO] [timer.py:157:stop] 0/14600, SamplesPerSec=57.84869541721371
Tencent01: [2020-11-23 21:01:30,187] [INFO] [timer.py:157:stop] 0/14700, SamplesPerSec=57.85813388367774
Tencent01: [2020-11-23 21:05:06,221] [INFO] [logging.py:60:log_dist] [Rank 0] step=73700, skipped=76, lr=[0.00013447964029130087, 0.00013447964029130087], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 21:05:06,274] [INFO] [timer.py:157:stop] 0/14800, SamplesPerSec=57.86751856804532
Tencent01:  iteration    73700/  320000 | elapsed time per iteration (ms): 8644.6 | learning rate 1.345E-04 | lm loss 2.987026E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2357.78 | backward: 6165.21 | optimizer: 121.20 | batch generator: 5.87 | data loader: 1.06
Tencent01: [2020-11-23 21:09:42,846] [INFO] [timer.py:157:stop] 0/14900, SamplesPerSec=57.770713506612374
Tencent01: [2020-11-23 21:13:19,231] [INFO] [timer.py:157:stop] 0/15000, SamplesPerSec=57.77998494810582
Tencent01: [2020-11-23 21:16:55,389] [INFO] [timer.py:157:stop] 0/15100, SamplesPerSec=57.78955969727105
Tencent01: [2020-11-23 21:20:31,429] [INFO] [logging.py:60:log_dist] [Rank 0] step=73800, skipped=76, lr=[0.0001344373388736248, 0.0001344373388736248], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 21:20:31,481] [INFO] [timer.py:157:stop] 0/15200, SamplesPerSec=57.79909686764607
Tencent01:  iteration    73800/  320000 | elapsed time per iteration (ms): 9252.1 | learning rate 1.344E-04 | lm loss 2.983610E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2435.92 | backward: 6694.68 | optimizer: 121.11 | batch generator: 5.79 | data loader: 1.03
Tencent01: [2020-11-23 21:24:07,588] [INFO] [timer.py:157:stop] 0/15300, SamplesPerSec=57.808466050457255
Tencent01: [2020-11-23 21:27:43,674] [INFO] [timer.py:157:stop] 0/15400, SamplesPerSec=57.81776735038276
Tencent01: [2020-11-23 21:31:19,795] [INFO] [timer.py:157:stop] 0/15500, SamplesPerSec=57.82688872341245
Tencent01: [2020-11-23 21:35:36,388] [INFO] [logging.py:60:log_dist] [Rank 0] step=73900, skipped=76, lr=[0.0001343949873972625, 0.0001343949873972625], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 21:35:36,440] [INFO] [timer.py:157:stop] 0/15600, SamplesPerSec=57.76809298750053
Tencent01:  iteration    73900/  320000 | elapsed time per iteration (ms): 9049.6 | learning rate 1.344E-04 | lm loss 3.010901E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2433.75 | backward: 6494.16 | optimizer: 121.32 | batch generator: 5.43 | data loader: 0.98
Tencent01: [2020-11-23 21:39:43,371] [INFO] [timer.py:157:stop] 0/15700, SamplesPerSec=57.72626870839691
Tencent01: [2020-11-23 21:43:19,633] [INFO] [timer.py:157:stop] 0/15800, SamplesPerSec=57.73554372058905
Tencent01: [2020-11-23 21:46:55,787] [INFO] [timer.py:157:stop] 0/15900, SamplesPerSec=57.74490612401559
Tencent01: [2020-11-23 21:50:31,873] [INFO] [logging.py:60:log_dist] [Rank 0] step=74000, skipped=76, lr=[0.0001343525859030335, 0.0001343525859030335], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 21:50:31,926] [INFO] [timer.py:157:stop] 0/16000, SamplesPerSec=57.75413699933797
Tencent01:  iteration    74000/  320000 | elapsed time per iteration (ms): 8954.9 | learning rate 1.344E-04 | lm loss 3.007727E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2358.68 | backward: 6474.21 | optimizer: 121.59 | batch generator: 5.63 | data loader: 1.01
Tencent01: [2020-11-23 21:50:31,929] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: /root/data/checkpoints/gpt-345M11-14-13-21/74000/mp_rank_00_model_states.pt
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: -----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 74000 | LM loss: 2.905354E+00 | LM PPL: 1.827172E+01
Tencent01: -----------------------------------------------------------------------------------
Tencent01: [2020-11-23 21:56:15,005] [INFO] [timer.py:157:stop] 0/16100, SamplesPerSec=57.76088743721801
Tencent01: [2020-11-23 21:59:51,146] [INFO] [timer.py:157:stop] 0/16200, SamplesPerSec=57.76991716271463
Tencent01: [2020-11-23 22:03:47,970] [INFO] [timer.py:157:stop] 0/16300, SamplesPerSec=57.74575981905484
Tencent01: [2020-11-23 22:08:14,560] [INFO] [logging.py:60:log_dist] [Rank 0] step=74100, skipped=76, lr=[0.00013431013443180556, 0.00013431013443180556], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 22:08:14,613] [INFO] [timer.py:157:stop] 0/16400, SamplesPerSec=57.674642243298344
Tencent01:  iteration    74100/  320000 | elapsed time per iteration (ms): 10626.9 | learning rate 1.343E-04 | lm loss 3.036336E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2464.09 | backward: 6786.42 | optimizer: 121.34 | batch generator: 6.66 | data loader: 1.17
Tencent01: [2020-11-23 22:11:50,929] [INFO] [timer.py:157:stop] 0/16500, SamplesPerSec=57.683740496132685
Tencent01: [2020-11-23 22:15:27,081] [INFO] [timer.py:157:stop] 0/16600, SamplesPerSec=57.692970627892535
Tencent01: [2020-11-23 22:19:03,143] [INFO] [timer.py:157:stop] 0/16700, SamplesPerSec=57.702269611157476
Tencent01: [2020-11-23 22:22:39,248] [INFO] [logging.py:60:log_dist] [Rank 0] step=74200, skipped=76, lr=[0.00013426763302449467, 0.00013426763302449467], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 22:22:39,300] [INFO] [timer.py:157:stop] 0/16800, SamplesPerSec=57.71129418985974
Tencent01:  iteration    74200/  320000 | elapsed time per iteration (ms): 8646.9 | learning rate 1.343E-04 | lm loss 2.943772E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2357.69 | backward: 6167.67 | optimizer: 121.14 | batch generator: 5.72 | data loader: 1.03
Tencent01: [2020-11-23 22:26:15,478] [INFO] [timer.py:157:stop] 0/16900, SamplesPerSec=57.720193177313
Tencent01: [2020-11-23 22:30:11,249] [INFO] [timer.py:157:stop] 0/17000, SamplesPerSec=57.69897822582772
Tencent01: [2020-11-23 22:34:38,059] [INFO] [timer.py:157:stop] 0/17100, SamplesPerSec=57.63089371810144
Tencent01: [2020-11-23 22:38:14,411] [INFO] [logging.py:60:log_dist] [Rank 0] step=74300, skipped=76, lr=[0.00013422508172206485, 0.00013422508172206485], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 22:38:14,464] [INFO] [timer.py:157:stop] 0/17200, SamplesPerSec=57.639726705990256
Tencent01:  iteration    74300/  320000 | elapsed time per iteration (ms): 9351.6 | learning rate 1.342E-04 | lm loss 3.023772E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2358.24 | backward: 6871.86 | optimizer: 121.18 | batch generator: 5.75 | data loader: 1.02
Tencent01: [2020-11-23 22:41:50,669] [INFO] [timer.py:157:stop] 0/17300, SamplesPerSec=57.64877411642757
Tencent01: [2020-11-23 22:45:26,787] [INFO] [timer.py:157:stop] 0/17400, SamplesPerSec=57.657825642769126
Tencent01: [2020-11-23 22:49:02,941] [INFO] [timer.py:157:stop] 0/17500, SamplesPerSec=57.66673253804392
Tencent01: [2020-11-23 22:52:38,992] [INFO] [logging.py:60:log_dist] [Rank 0] step=74400, skipped=76, lr=[0.00013418248056552829, 0.00013418248056552829], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 22:52:39,044] [INFO] [timer.py:157:stop] 0/17600, SamplesPerSec=57.67560659314643
Tencent01:  iteration    74400/  320000 | elapsed time per iteration (ms): 8645.8 | learning rate 1.342E-04 | lm loss 3.023528E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2357.62 | backward: 6166.73 | optimizer: 121.10 | batch generator: 5.63 | data loader: 1.00
Tencent01: [2020-11-23 22:56:22,584] [INFO] [timer.py:157:stop] 0/17700, SamplesPerSec=57.67344722710697
Tencent01: [2020-11-23 23:01:18,248] [INFO] [timer.py:157:stop] 0/17800, SamplesPerSec=57.56621795364893
Tencent01: [2020-11-23 23:02:01,532] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-23 23:02:01,532] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-23 23:02:01,532] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-23 23:02:01,532] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-23 23:02:01,532] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-23 23:02:01,532] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-23 23:02:01,532] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-23 23:02:01,532] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-23 23:02:01,532] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-23 23:02:01,532] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-23 23:02:01,532] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-23 23:02:01,532] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-23 23:02:01,532] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-23 23:02:01,532] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-23 23:02:01,532] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-23 23:02:01,533] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-23 23:02:01,533] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-23 23:02:01,533] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-23 23:02:01,532] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-23 23:02:01,532] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-23 23:02:01,533] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-23 23:02:01,532] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-23 23:02:01,532] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-23 23:02:01,532] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-23 23:02:01,533] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-23 23:02:01,533] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-23 23:02:01,532] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-23 23:02:01,533] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-23 23:02:01,533] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-23 23:02:01,533] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-23 23:02:01,533] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-23 23:02:01,533] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-23 23:04:54,880] [INFO] [timer.py:157:stop] 0/17900, SamplesPerSec=57.57470837515589
Tencent01: [2020-11-23 23:08:31,114] [INFO] [logging.py:60:log_dist] [Rank 0] step=74500, skipped=76, lr=[0.0001341398295959452, 0.0001341398295959452], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 23:08:31,167] [INFO] [timer.py:157:stop] 0/18000, SamplesPerSec=57.583609156206634
Tencent01:  iteration    74500/  320000 | elapsed time per iteration (ms): 9521.2 | learning rate 1.341E-04 | lm loss 2.965864E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2433.50 | backward: 6966.14 | optimizer: 121.23 | batch generator: 5.34 | data loader: 0.95
Tencent01: [2020-11-23 23:12:07,454] [INFO] [timer.py:157:stop] 0/18100, SamplesPerSec=57.592420610263886
Tencent01: [2020-11-23 23:15:43,623] [INFO] [timer.py:157:stop] 0/18200, SamplesPerSec=57.6013028615545
Tencent01: [2020-11-23 23:19:19,839] [INFO] [timer.py:157:stop] 0/18300, SamplesPerSec=57.61002943290533
Tencent01: [2020-11-23 23:22:55,974] [INFO] [logging.py:60:log_dist] [Rank 0] step=74600, skipped=76, lr=[0.00013409712885442373, 0.00013409712885442373], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 23:22:56,027] [INFO] [timer.py:157:stop] 0/18400, SamplesPerSec=57.61871973164893
Tencent01:  iteration    74600/  320000 | elapsed time per iteration (ms): 8648.6 | learning rate 1.341E-04 | lm loss 3.025559E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2357.41 | backward: 6169.26 | optimizer: 121.53 | batch generator: 5.82 | data loader: 1.08
Tencent01: [2020-11-23 23:27:37,467] [INFO] [timer.py:157:stop] 0/18500, SamplesPerSec=57.53591908555275
Tencent01: [2020-11-23 23:31:33,234] [INFO] [timer.py:157:stop] 0/18600, SamplesPerSec=57.51766981490559
Tencent01: [2020-11-23 23:35:09,519] [INFO] [timer.py:157:stop] 0/18700, SamplesPerSec=57.52654235220111
Tencent01: [2020-11-23 23:38:45,621] [INFO] [logging.py:60:log_dist] [Rank 0] step=74700, skipped=76, lr=[0.0001340543783821201, 0.0001340543783821201], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 23:38:45,673] [INFO] [timer.py:157:stop] 0/18800, SamplesPerSec=57.5354985303271
Tencent01:  iteration    74700/  320000 | elapsed time per iteration (ms): 9496.5 | learning rate 1.341E-04 | lm loss 2.975367E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2359.27 | backward: 7015.56 | optimizer: 121.26 | batch generator: 5.89 | data loader: 1.06
Tencent01: [2020-11-23 23:42:21,763] [INFO] [timer.py:157:stop] 0/18900, SamplesPerSec=57.54444517411232
Tencent01: [2020-11-23 23:45:57,902] [INFO] [timer.py:157:stop] 0/19000, SamplesPerSec=57.55322239179332
Tencent01: [2020-11-23 23:49:34,041] [INFO] [timer.py:157:stop] 0/19100, SamplesPerSec=57.56188223040447
Tencent01: [2020-11-23 23:53:57,875] [INFO] [logging.py:60:log_dist] [Rank 0] step=74800, skipped=76, lr=[0.0001340115782202385, 0.0001340115782202385], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-23 23:53:57,927] [INFO] [timer.py:157:stop] 0/19200, SamplesPerSec=57.50616065015001
Tencent01:  iteration    74800/  320000 | elapsed time per iteration (ms): 9122.5 | learning rate 1.340E-04 | lm loss 2.957684E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2356.60 | backward: 6644.35 | optimizer: 121.21 | batch generator: 5.28 | data loader: 0.95
Tencent01: [2020-11-23 23:58:13,288] [INFO] [timer.py:157:stop] 0/19300, SamplesPerSec=57.46253882623315
Tencent01: [2020-11-24 00:01:49,692] [INFO] [timer.py:157:stop] 0/19400, SamplesPerSec=57.47118927838712
Tencent01: [2020-11-24 00:05:25,934] [INFO] [timer.py:157:stop] 0/19500, SamplesPerSec=57.479967666785
Tencent01: [2020-11-24 00:09:02,099] [INFO] [logging.py:60:log_dist] [Rank 0] step=74900, skipped=76, lr=[0.0001339687284100309, 0.0001339687284100309], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 00:09:02,151] [INFO] [timer.py:157:stop] 0/19600, SamplesPerSec=57.48871028742229
Tencent01:  iteration    74900/  320000 | elapsed time per iteration (ms): 9042.2 | learning rate 1.340E-04 | lm loss 2.927841E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2359.04 | backward: 6561.28 | optimizer: 121.52 | batch generator: 5.86 | data loader: 1.08
Tencent01: [2020-11-24 00:12:38,321] [INFO] [timer.py:157:stop] 0/19700, SamplesPerSec=57.497416534911764
Tencent01: [2020-11-24 00:16:14,463] [INFO] [timer.py:157:stop] 0/19800, SamplesPerSec=57.50610252325923
Tencent01: [2020-11-24 00:20:08,144] [INFO] [timer.py:157:stop] 0/19900, SamplesPerSec=57.4919122744459
Tencent01: [2020-11-24 00:24:44,531] [INFO] [logging.py:60:log_dist] [Rank 0] step=75000, skipped=76, lr=[0.00013392582899279714, 0.00013392582899279714], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 00:24:44,582] [INFO] [timer.py:157:stop] 0/20000, SamplesPerSec=57.42273607901931
Tencent01:  iteration    75000/  320000 | elapsed time per iteration (ms): 9424.3 | learning rate 1.339E-04 | lm loss 2.961731E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2435.03 | backward: 6867.52 | optimizer: 121.37 | batch generator: 6.09 | data loader: 1.09
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: -----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 75000 | LM loss: 2.869672E+00 | LM PPL: 1.763124E+01
Tencent01: -----------------------------------------------------------------------------------
Tencent01: [2020-11-24 00:29:17,360] [INFO] [timer.py:157:stop] 0/20100, SamplesPerSec=57.429689231356356
Tencent01: [2020-11-24 00:32:53,502] [INFO] [timer.py:157:stop] 0/20200, SamplesPerSec=57.438485712455986
Tencent01: [2020-11-24 00:36:29,613] [INFO] [timer.py:157:stop] 0/20300, SamplesPerSec=57.447252777863724
Tencent01: [2020-11-24 00:40:05,764] [INFO] [logging.py:60:log_dist] [Rank 0] step=75100, skipped=76, lr=[0.00013388288000988494, 0.00013388288000988494], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 00:40:05,816] [INFO] [timer.py:157:stop] 0/20400, SamplesPerSec=57.45582615447492
Tencent01:  iteration    75100/  320000 | elapsed time per iteration (ms): 9212.3 | learning rate 1.339E-04 | lm loss 2.935309E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2358.09 | backward: 6180.71 | optimizer: 121.79 | batch generator: 6.97 | data loader: 1.25
Tencent01: [2020-11-24 00:43:41,966] [INFO] [timer.py:157:stop] 0/20500, SamplesPerSec=57.46435763214314
Tencent01: [2020-11-24 00:47:29,166] [INFO] [timer.py:157:stop] 0/20600, SamplesPerSec=57.45900750935334
Tencent01: [2020-11-24 00:52:25,169] [INFO] [timer.py:157:stop] 0/20700, SamplesPerSec=57.368109792008944
Tencent01: [2020-11-24 00:56:01,673] [INFO] [logging.py:60:log_dist] [Rank 0] step=75200, skipped=76, lr=[0.0001338398815026897, 0.0001338398815026897], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 00:56:01,725] [INFO] [timer.py:157:stop] 0/20800, SamplesPerSec=57.37642482490525
Tencent01:  iteration    75200/  320000 | elapsed time per iteration (ms): 9559.1 | learning rate 1.338E-04 | lm loss 3.014167E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2358.21 | backward: 7078.50 | optimizer: 121.96 | batch generator: 6.10 | data loader: 1.15
Tencent01: [2020-11-24 00:59:38,032] [INFO] [timer.py:157:stop] 0/20900, SamplesPerSec=57.38496151554841
Tencent01: [2020-11-24 01:03:14,192] [INFO] [timer.py:157:stop] 0/21000, SamplesPerSec=57.3936181053606
Tencent01: [2020-11-24 01:06:50,340] [INFO] [timer.py:157:stop] 0/21100, SamplesPerSec=57.40217370257385
Tencent01: [2020-11-24 01:10:26,416] [INFO] [logging.py:60:log_dist] [Rank 0] step=75300, skipped=76, lr=[0.00013379683351265467, 0.00013379683351265467], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 01:10:26,468] [INFO] [timer.py:157:stop] 0/21200, SamplesPerSec=57.41071954862627
Tencent01:  iteration    75300/  320000 | elapsed time per iteration (ms): 8647.4 | learning rate 1.338E-04 | lm loss 2.920693E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2358.20 | backward: 6167.08 | optimizer: 121.75 | batch generator: 5.83 | data loader: 1.11
Tencent01: [2020-11-24 01:14:13,165] [INFO] [timer.py:157:stop] 0/21300, SamplesPerSec=57.4063877814885
Tencent01: [2020-11-24 01:18:52,190] [INFO] [timer.py:157:stop] 0/21400, SamplesPerSec=57.3392128870115
Tencent01: [2020-11-24 01:22:47,860] [INFO] [timer.py:157:stop] 0/21500, SamplesPerSec=57.32454311749534
Tencent01: [2020-11-24 01:26:24,105] [INFO] [logging.py:60:log_dist] [Rank 0] step=75400, skipped=76, lr=[0.00013375373608127072, 0.00013375373608127072], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 01:26:24,157] [INFO] [timer.py:157:stop] 0/21600, SamplesPerSec=57.33304979172625
Tencent01:  iteration    75400/  320000 | elapsed time per iteration (ms): 9576.9 | learning rate 1.338E-04 | lm loss 3.015694E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2466.50 | backward: 6988.41 | optimizer: 121.59 | batch generator: 5.80 | data loader: 1.06
Tencent01: [2020-11-24 01:30:00,322] [INFO] [timer.py:157:stop] 0/21700, SamplesPerSec=57.34164441735514
Tencent01: [2020-11-24 01:33:36,442] [INFO] [timer.py:157:stop] 0/21800, SamplesPerSec=57.35019637476697
Tencent01: [2020-11-24 01:34:19,579] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 01:34:19,579] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 01:34:19,579] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 01:34:19,579] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 01:34:19,579] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 01:34:19,579] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 01:34:19,579] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 01:34:19,580] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 01:34:19,580] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 01:34:19,580] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 01:34:19,580] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 01:34:19,580] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 01:34:19,580] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 01:34:19,578] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 01:34:19,578] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 01:34:19,578] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 01:34:19,581] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 01:34:19,578] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 01:34:19,578] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 01:34:19,578] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 01:34:19,578] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 01:34:19,579] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 01:34:19,579] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 01:34:19,579] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 01:34:19,579] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 01:34:19,579] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 01:34:19,579] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 01:34:19,579] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 01:34:19,579] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 01:34:19,579] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 01:34:19,581] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 01:34:19,581] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 01:37:12,660] [INFO] [timer.py:157:stop] 0/21900, SamplesPerSec=57.35857439224725
Tencent01: [2020-11-24 01:40:48,797] [INFO] [logging.py:60:log_dist] [Rank 0] step=75500, skipped=76, lr=[0.00013371058925007635, 0.00013371058925007635], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 01:40:48,849] [INFO] [timer.py:157:stop] 0/22000, SamplesPerSec=57.36689647955528
Tencent01:  iteration    75500/  320000 | elapsed time per iteration (ms): 8646.9 | learning rate 1.337E-04 | lm loss 2.997887E+00 | loss scale 524288.0 |
Tencent01: time (ms) | forward: 2357.97 | backward: 6166.60 | optimizer: 121.94 | batch generator: 5.83 | data loader: 1.12
Tencent01: [2020-11-24 01:41:14,700] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 75502
Tencent01: [2020-11-24 01:41:14,700] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 01:41:14,700] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
Tencent01: [2020-11-24 01:41:14,700] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 75502
Tencent01: [2020-11-24 01:41:14,700] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 01:41:14,700] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 75502
Tencent01: [2020-11-24 01:41:14,700] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 01:41:14,700] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 75502
Tencent02: [2020-11-24 01:41:14,700] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 01:41:14,700] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 75502
Tencent02: [2020-11-24 01:41:14,700] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 75502
Tencent02: [2020-11-24 01:41:14,701] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 01:41:14,700] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 75502
Tencent02: [2020-11-24 01:41:14,701] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 01:41:14,701] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 75502
Tencent02: [2020-11-24 01:41:14,701] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 75502
Tencent02: [2020-11-24 01:41:14,701] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 01:41:14,701] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 01:41:14,701] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 01:41:14,701] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 75502
Tencent02: [2020-11-24 01:41:14,701] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 75502
Tencent02: [2020-11-24 01:41:14,701] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 01:41:14,701] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 01:41:14,700] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 75502
Tencent01: [2020-11-24 01:41:14,700] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 01:41:14,700] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 75502
Tencent01: [2020-11-24 01:41:14,700] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 75502
Tencent01: [2020-11-24 01:41:14,700] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 01:41:14,701] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 01:41:14,700] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 75502
Tencent01: [2020-11-24 01:41:14,701] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 75502
Tencent01: [2020-11-24 01:41:14,701] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 01:41:14,701] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 01:45:23,512] [INFO] [timer.py:157:stop] 0/22100, SamplesPerSec=57.307184357741164
Tencent01: [2020-11-24 01:49:30,952] [INFO] [timer.py:157:stop] 0/22200, SamplesPerSec=57.279529286680315
Tencent01: [2020-11-24 01:53:07,301] [INFO] [timer.py:157:stop] 0/22300, SamplesPerSec=57.287888828153775
Tencent01: [2020-11-24 01:56:43,367] [INFO] [logging.py:60:log_dist] [Rank 0] step=75600, skipped=77, lr=[0.00013366782526673843, 0.00013366782526673843], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 01:56:43,419] [INFO] [timer.py:157:stop] 0/22400, SamplesPerSec=57.296442214961374
Tencent01:  iteration    75600/  320000 | elapsed time per iteration (ms): 9545.7 | learning rate 1.337E-04 | lm loss 2.976844E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2451.66 | backward: 6972.67 | optimizer: 120.95 | batch generator: 5.61 | data loader: 1.05
Tencent01: [2020-11-24 02:00:19,498] [INFO] [timer.py:157:stop] 0/22500, SamplesPerSec=57.3049753711969
Tencent01: [2020-11-24 02:03:55,668] [INFO] [timer.py:157:stop] 0/22600, SamplesPerSec=57.31333679656842
Tencent01: [2020-11-24 02:07:31,747] [INFO] [timer.py:157:stop] 0/22700, SamplesPerSec=57.32172308725009
Tencent01: [2020-11-24 02:11:54,795] [INFO] [logging.py:60:log_dist] [Rank 0] step=75700, skipped=77, lr=[0.00013362458025368892, 0.00013362458025368892], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 02:11:54,847] [INFO] [timer.py:157:stop] 0/22800, SamplesPerSec=57.27713335708855
Tencent01:  iteration    75700/  320000 | elapsed time per iteration (ms): 9114.3 | learning rate 1.336E-04 | lm loss 2.970883E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2433.81 | backward: 6558.94 | optimizer: 121.18 | batch generator: 6.09 | data loader: 0.99
Tencent01: [2020-11-24 02:16:07,092] [INFO] [timer.py:157:stop] 0/22900, SamplesPerSec=57.245136013346304
Tencent01: [2020-11-24 02:19:43,584] [INFO] [timer.py:157:stop] 0/23000, SamplesPerSec=57.253235514155286
Tencent01: [2020-11-24 02:23:19,827] [INFO] [timer.py:157:stop] 0/23100, SamplesPerSec=57.261519873182785
Tencent01: [2020-11-24 02:26:55,963] [INFO] [logging.py:60:log_dist] [Rank 0] step=75800, skipped=77, lr=[0.00013358128596531308, 0.00013358128596531308], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 02:26:56,015] [INFO] [timer.py:157:stop] 0/23200, SamplesPerSec=57.26982697558316
Tencent01:  iteration    75800/  320000 | elapsed time per iteration (ms): 9011.7 | learning rate 1.336E-04 | lm loss 2.986792E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2360.48 | backward: 6529.28 | optimizer: 121.56 | batch generator: 6.01 | data loader: 1.00
Tencent01: [2020-11-24 02:30:32,162] [INFO] [timer.py:157:stop] 0/23300, SamplesPerSec=57.278098347896155
Tencent01: [2020-11-24 02:34:08,339] [INFO] [timer.py:157:stop] 0/23400, SamplesPerSec=57.286252026434745
Tencent01: [2020-11-24 02:37:54,667] [INFO] [timer.py:157:stop] 0/23500, SamplesPerSec=57.283270873393526
Tencent01: [2020-11-24 02:42:40,821] [INFO] [logging.py:60:log_dist] [Rank 0] step=75900, skipped=77, lr=[0.0001335379424433391, 0.0001335379424433391], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 02:42:40,873] [INFO] [timer.py:157:stop] 0/23600, SamplesPerSec=57.21536564566314
Tencent01:  iteration    75900/  320000 | elapsed time per iteration (ms): 9448.6 | learning rate 1.335E-04 | lm loss 2.936155E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2434.86 | backward: 6892.20 | optimizer: 121.17 | batch generator: 5.89 | data loader: 0.99
Tencent01: [2020-11-24 02:46:17,495] [INFO] [timer.py:157:stop] 0/23700, SamplesPerSec=57.22319531134305
Tencent01: [2020-11-24 02:49:53,755] [INFO] [timer.py:157:stop] 0/23800, SamplesPerSec=57.23135290102841
Tencent01: [2020-11-24 02:53:29,923] [INFO] [timer.py:157:stop] 0/23900, SamplesPerSec=57.23955045406076
Tencent01: [2020-11-24 02:57:05,980] [INFO] [logging.py:60:log_dist] [Rank 0] step=76000, skipped=77, lr=[0.00013349454972954282, 0.00013349454972954282], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 02:57:06,032] [INFO] [timer.py:157:stop] 0/24000, SamplesPerSec=57.24773059822887
Tencent01:  iteration    76000/  320000 | elapsed time per iteration (ms): 8651.6 | learning rate 1.335E-04 | lm loss 3.002773E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2359.12 | backward: 6170.78 | optimizer: 121.34 | batch generator: 5.94 | data loader: 1.02
Tencent01: [2020-11-24 02:57:06,036] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: /root/data/checkpoints/gpt-345M11-14-13-21/76000/mp_rank_00_model_states.pt
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: -----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 76000 | LM loss: 2.908934E+00 | LM PPL: 1.833725E+01
Tencent01: -----------------------------------------------------------------------------------
Tencent01: [2020-11-24 03:02:54,660] [INFO] [timer.py:157:stop] 0/24100, SamplesPerSec=57.25421307175871
Tencent01: [2020-11-24 03:06:30,877] [INFO] [timer.py:157:stop] 0/24200, SamplesPerSec=57.26215449348385
Tencent01: [2020-11-24 03:11:31,381] [INFO] [timer.py:157:stop] 0/24300, SamplesPerSec=57.18128366319556
Tencent01: [2020-11-24 03:15:26,603] [INFO] [logging.py:60:log_dist] [Rank 0] step=76100, skipped=77, lr=[0.0001334511078657473, 0.0001334511078657473], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 03:15:26,655] [INFO] [timer.py:157:stop] 0/24400, SamplesPerSec=57.169489630507016
Tencent01:  iteration    76100/  320000 | elapsed time per iteration (ms): 11006.2 | learning rate 1.335E-04 | lm loss 2.967921E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2434.52 | backward: 7140.27 | optimizer: 121.39 | batch generator: 6.89 | data loader: 1.19
Tencent01: [2020-11-24 03:19:02,946] [INFO] [timer.py:157:stop] 0/24500, SamplesPerSec=57.177583702221355
Tencent01: [2020-11-24 03:22:39,125] [INFO] [timer.py:157:stop] 0/24600, SamplesPerSec=57.18571191040648
Tencent01: [2020-11-24 03:26:15,329] [INFO] [timer.py:157:stop] 0/24700, SamplesPerSec=57.193776005558824
Tencent01: [2020-11-24 03:29:51,483] [INFO] [logging.py:60:log_dist] [Rank 0] step=76200, skipped=77, lr=[0.00013340761689382307, 0.00013340761689382307], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 03:29:51,535] [INFO] [timer.py:157:stop] 0/24800, SamplesPerSec=57.20176239153157
Tencent01:  iteration    76200/  320000 | elapsed time per iteration (ms): 8648.8 | learning rate 1.334E-04 | lm loss 3.006991E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2357.53 | backward: 6169.25 | optimizer: 121.64 | batch generator: 5.67 | data loader: 1.08
Tencent01: [2020-11-24 03:33:27,732] [INFO] [timer.py:157:stop] 0/24900, SamplesPerSec=57.209711438965996
Tencent01: [2020-11-24 03:38:18,943] [INFO] [timer.py:157:stop] 0/25000, SamplesPerSec=57.14094281565891
Tencent01: [2020-11-24 03:42:24,802] [INFO] [timer.py:157:stop] 0/25100, SamplesPerSec=57.11890426643638
Tencent01: [2020-11-24 03:46:01,096] [INFO] [logging.py:60:log_dist] [Rank 0] step=76300, skipped=77, lr=[0.00013336407685568796, 0.00013336407685568796], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 03:46:01,148] [INFO] [timer.py:157:stop] 0/25200, SamplesPerSec=57.126892105101305
Tencent01:  iteration    76300/  320000 | elapsed time per iteration (ms): 9696.1 | learning rate 1.334E-04 | lm loss 2.983376E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2358.36 | backward: 7216.34 | optimizer: 121.07 | batch generator: 5.84 | data loader: 1.01
Tencent01: [2020-11-24 03:49:37,346] [INFO] [timer.py:157:stop] 0/25300, SamplesPerSec=57.13497674979954
Tencent01: [2020-11-24 03:53:13,506] [INFO] [timer.py:157:stop] 0/25400, SamplesPerSec=57.1430384943033
Tencent01: [2020-11-24 03:56:49,706] [INFO] [timer.py:157:stop] 0/25500, SamplesPerSec=57.15099609691176
Tencent01: [2020-11-24 04:00:25,798] [INFO] [logging.py:60:log_dist] [Rank 0] step=76400, skipped=77, lr=[0.00013332048779330708, 0.00013332048779330708], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 04:00:25,850] [INFO] [timer.py:157:stop] 0/25600, SamplesPerSec=57.15894481999782
Tencent01:  iteration    76400/  320000 | elapsed time per iteration (ms): 8647.0 | learning rate 1.333E-04 | lm loss 2.967488E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2357.51 | backward: 6168.12 | optimizer: 121.05 | batch generator: 5.64 | data loader: 0.99
Tencent01: [2020-11-24 04:04:59,757] [INFO] [timer.py:157:stop] 0/25700, SamplesPerSec=57.10950558660354
Tencent01: [2020-11-24 04:09:25,768] [INFO] [timer.py:157:stop] 0/25800, SamplesPerSec=57.068324714171915
Tencent01: [2020-11-24 04:13:02,219] [INFO] [timer.py:157:stop] 0/25900, SamplesPerSec=57.07619268747261
Tencent01: [2020-11-24 04:16:38,336] [INFO] [logging.py:60:log_dist] [Rank 0] step=76500, skipped=77, lr=[0.0001332768497486929, 0.0001332768497486929], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 04:16:38,388] [INFO] [timer.py:157:stop] 0/26000, SamplesPerSec=57.08427515036448
Tencent01:  iteration    76500/  320000 | elapsed time per iteration (ms): 9725.4 | learning rate 1.333E-04 | lm loss 2.944719E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2358.51 | backward: 7245.28 | optimizer: 121.21 | batch generator: 5.81 | data loader: 1.04
Tencent01: [2020-11-24 04:17:12,882] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 04:17:12,882] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 04:17:12,882] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 04:17:12,882] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 04:17:12,882] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 04:17:12,882] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 04:17:12,882] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 04:17:12,883] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 04:17:12,882] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 04:17:12,882] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 04:17:12,883] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 04:17:12,883] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 04:17:12,883] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 04:17:12,883] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 04:17:12,883] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 04:17:12,883] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 04:17:12,883] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 04:17:12,883] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 04:17:12,883] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 04:17:12,883] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 04:17:12,883] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 04:17:12,883] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 04:17:12,883] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 04:17:12,883] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 04:17:12,883] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 04:17:12,883] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 04:17:12,883] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 04:17:12,883] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 04:17:12,884] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 04:17:12,885] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 04:17:12,885] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 04:17:12,885] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 04:17:38,806] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 76506
Tencent01: [2020-11-24 04:17:38,806] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 76506
Tencent01: [2020-11-24 04:17:38,806] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 04:17:38,806] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 04:17:38,806] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
Tencent01: [2020-11-24 04:17:38,806] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 76506
Tencent01: [2020-11-24 04:17:38,806] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 04:17:38,806] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 76506
Tencent01: [2020-11-24 04:17:38,806] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 76506
Tencent01: [2020-11-24 04:17:38,806] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 76506
Tencent01: [2020-11-24 04:17:38,806] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 04:17:38,806] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 04:17:38,806] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 04:17:38,806] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 76506
Tencent01: [2020-11-24 04:17:38,806] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 76506
Tencent01: [2020-11-24 04:17:38,806] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 04:17:38,806] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 04:17:38,806] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 76506
Tencent02: [2020-11-24 04:17:38,806] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 76506
Tencent02: [2020-11-24 04:17:38,806] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 76506
Tencent02: [2020-11-24 04:17:38,806] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 76506
Tencent02: [2020-11-24 04:17:38,806] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 04:17:38,806] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 04:17:38,806] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 04:17:38,806] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 76506
Tencent02: [2020-11-24 04:17:38,806] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 04:17:38,806] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 76506
Tencent02: [2020-11-24 04:17:38,806] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 04:17:38,806] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 04:17:38,806] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 76506
Tencent02: [2020-11-24 04:17:38,806] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 04:17:38,806] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 76506
Tencent02: [2020-11-24 04:17:38,806] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 04:20:14,442] [INFO] [timer.py:157:stop] 0/26100, SamplesPerSec=57.09238552651147
Tencent01: [2020-11-24 04:23:50,525] [INFO] [timer.py:157:stop] 0/26200, SamplesPerSec=57.10042062983245
Tencent01: [2020-11-24 04:27:26,683] [INFO] [timer.py:157:stop] 0/26300, SamplesPerSec=57.10832576430937
Tencent01: [2020-11-24 04:31:32,970] [INFO] [logging.py:60:log_dist] [Rank 0] step=76600, skipped=78, lr=[0.0001332335998758684, 0.0001332335998758684], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 04:31:33,022] [INFO] [timer.py:157:stop] 0/26400, SamplesPerSec=57.087041893691065
Tencent01:  iteration    76600/  320000 | elapsed time per iteration (ms): 8946.3 | learning rate 1.332E-04 | lm loss 2.962349E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2356.94 | backward: 6468.18 | optimizer: 120.84 | batch generator: 5.29 | data loader: 1.00
Tencent01: [2020-11-24 04:36:30,656] [INFO] [timer.py:157:stop] 0/26500, SamplesPerSec=57.0167457452785
Tencent01: [2020-11-24 04:40:07,178] [INFO] [timer.py:157:stop] 0/26600, SamplesPerSec=57.02450935752871
Tencent01: [2020-11-24 04:43:43,454] [INFO] [timer.py:157:stop] 0/26700, SamplesPerSec=57.03244723718195
Tencent01: [2020-11-24 04:47:19,587] [INFO] [logging.py:60:log_dist] [Rank 0] step=76700, skipped=78, lr=[0.00013318986448178545, 0.00013318986448178545], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 04:47:19,640] [INFO] [timer.py:157:stop] 0/26800, SamplesPerSec=57.040403390109184
Tencent01:  iteration    76700/  320000 | elapsed time per iteration (ms): 9466.2 | learning rate 1.332E-04 | lm loss 2.974329E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2358.10 | backward: 6986.37 | optimizer: 121.34 | batch generator: 5.60 | data loader: 1.00
Tencent01: [2020-11-24 04:50:55,791] [INFO] [timer.py:157:stop] 0/26900, SamplesPerSec=57.048345026661124
Tencent01: [2020-11-24 04:54:31,956] [INFO] [timer.py:157:stop] 0/27000, SamplesPerSec=57.05621972334194
Tencent01: [2020-11-24 04:58:17,253] [INFO] [timer.py:157:stop] 0/27100, SamplesPerSec=57.05546515832161
Tencent01: [2020-11-24 05:03:22,969] [INFO] [logging.py:60:log_dist] [Rank 0] step=76800, skipped=78, lr=[0.0001331460802313676, 0.0001331460802313676], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 05:03:23,021] [INFO] [timer.py:157:stop] 0/27200, SamplesPerSec=56.97956735295009
Tencent01:  iteration    76800/  320000 | elapsed time per iteration (ms): 9633.8 | learning rate 1.331E-04 | lm loss 2.959475E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2449.56 | backward: 7062.77 | optimizer: 121.13 | batch generator: 5.57 | data loader: 0.98
Tencent01: [2020-11-24 05:06:59,708] [INFO] [timer.py:157:stop] 0/27300, SamplesPerSec=56.987110854159376
Tencent01: [2020-11-24 05:10:35,926] [INFO] [timer.py:157:stop] 0/27400, SamplesPerSec=56.99502219217276
Tencent01: [2020-11-24 05:14:12,014] [INFO] [timer.py:157:stop] 0/27500, SamplesPerSec=57.00299357278685
Tencent01: [2020-11-24 05:17:48,129] [INFO] [logging.py:60:log_dist] [Rank 0] step=76900, skipped=78, lr=[0.00013310224716681538, 0.00013310224716681538], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 05:17:48,181] [INFO] [timer.py:157:stop] 0/27600, SamplesPerSec=57.01083752741603
Tencent01:  iteration    76900/  320000 | elapsed time per iteration (ms): 8651.6 | learning rate 1.331E-04 | lm loss 2.944219E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2358.16 | backward: 6171.98 | optimizer: 121.10 | batch generator: 5.46 | data loader: 0.98
Tencent01: [2020-11-24 05:21:24,354] [INFO] [timer.py:157:stop] 0/27700, SamplesPerSec=57.01862879086824
Tencent01: [2020-11-24 05:25:00,554] [INFO] [timer.py:157:stop] 0/27800, SamplesPerSec=57.02634933429611
Tencent01: [2020-11-24 05:29:47,384] [INFO] [timer.py:157:stop] 0/27900, SamplesPerSec=56.96975370000845
Tencent01: [2020-11-24 05:33:43,174] [INFO] [logging.py:60:log_dist] [Rank 0] step=77000, skipped=78, lr=[0.0001330583653303763, 0.0001330583653303763], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 05:33:43,226] [INFO] [timer.py:157:stop] 0/28000, SamplesPerSec=56.9597951376029
Tencent01:  iteration    77000/  320000 | elapsed time per iteration (ms): 9550.5 | learning rate 1.331E-04 | lm loss 2.978206E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2357.56 | backward: 7071.26 | optimizer: 121.26 | batch generator: 5.85 | data loader: 1.06
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: -----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 77000 | LM loss: 2.830306E+00 | LM PPL: 1.695065E+01
Tencent01: -----------------------------------------------------------------------------------
Tencent01: [2020-11-24 05:38:15,702] [INFO] [timer.py:157:stop] 0/28100, SamplesPerSec=56.96653562468354
Tencent01: [2020-11-24 05:41:51,886] [INFO] [timer.py:157:stop] 0/28200, SamplesPerSec=56.974324793454045
Tencent01: [2020-11-24 05:45:28,067] [INFO] [timer.py:157:stop] 0/28300, SamplesPerSec=56.98207170366722
Tencent01: [2020-11-24 05:49:04,163] [INFO] [logging.py:60:log_dist] [Rank 0] step=77100, skipped=78, lr=[0.00013301443476434498, 0.00013301443476434498], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 05:49:04,215] [INFO] [timer.py:157:stop] 0/28400, SamplesPerSec=56.989789640150306
Tencent01:  iteration    77100/  320000 | elapsed time per iteration (ms): 9209.9 | learning rate 1.330E-04 | lm loss 2.958119E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2358.09 | backward: 6179.48 | optimizer: 121.41 | batch generator: 6.95 | data loader: 1.19
Tencent01: [2020-11-24 05:52:40,408] [INFO] [timer.py:157:stop] 0/28500, SamplesPerSec=56.99740713148922
Tencent01: [2020-11-24 05:57:22,965] [INFO] [timer.py:157:stop] 0/28600, SamplesPerSec=56.94613099609721
Tencent01: [2020-11-24 06:01:30,365] [INFO] [timer.py:157:stop] 0/28700, SamplesPerSec=56.92629569984889
Tencent01: [2020-11-24 06:05:06,696] [INFO] [logging.py:60:log_dist] [Rank 0] step=77200, skipped=78, lr=[0.0001329704555110629, 0.0001329704555110629], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 06:05:06,749] [INFO] [timer.py:157:stop] 0/28800, SamplesPerSec=56.93388326186681
Tencent01:  iteration    77200/  320000 | elapsed time per iteration (ms): 9625.3 | learning rate 1.330E-04 | lm loss 2.971444E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2440.64 | backward: 7063.20 | optimizer: 121.13 | batch generator: 5.61 | data loader: 1.01
Tencent01: [2020-11-24 06:08:42,913] [INFO] [timer.py:157:stop] 0/28900, SamplesPerSec=56.94160933128452
Tencent01: [2020-11-24 06:12:19,051] [INFO] [timer.py:157:stop] 0/29000, SamplesPerSec=56.94929475903968
Tencent01: [2020-11-24 06:15:55,200] [INFO] [timer.py:157:stop] 0/29100, SamplesPerSec=56.95693911482384
Tencent01: [2020-11-24 06:19:31,330] [INFO] [logging.py:60:log_dist] [Rank 0] step=77300, skipped=78, lr=[0.0001329264276129185, 0.0001329264276129185], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 06:19:31,382] [INFO] [timer.py:157:stop] 0/29200, SamplesPerSec=56.9644945501778
Tencent01:  iteration    77300/  320000 | elapsed time per iteration (ms): 8646.3 | learning rate 1.329E-04 | lm loss 2.941064E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2356.84 | backward: 6168.08 | optimizer: 121.07 | batch generator: 5.69 | data loader: 0.98
Tencent01: [2020-11-24 06:24:00,411] [INFO] [timer.py:157:stop] 0/29300, SamplesPerSec=56.926315117609164
Tencent01: [2020-11-24 06:28:25,699] [INFO] [timer.py:157:stop] 0/29400, SamplesPerSec=56.891645527902504
Tencent01: [2020-11-24 06:32:02,231] [INFO] [timer.py:157:stop] 0/29500, SamplesPerSec=56.89901635425978
Tencent01: [2020-11-24 06:35:38,386] [INFO] [logging.py:60:log_dist] [Rank 0] step=77400, skipped=78, lr=[0.00013288235111234717, 0.00013288235111234717], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 06:35:38,438] [INFO] [timer.py:157:stop] 0/29600, SamplesPerSec=56.90661503299346
Tencent01:  iteration    77400/  320000 | elapsed time per iteration (ms): 9670.6 | learning rate 1.329E-04 | lm loss 2.908218E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2358.26 | backward: 7190.48 | optimizer: 121.43 | batch generator: 5.53 | data loader: 1.01
Tencent01: [2020-11-24 06:39:14,612] [INFO] [timer.py:157:stop] 0/29700, SamplesPerSec=56.91422785442901
Tencent01: [2020-11-24 06:42:50,816] [INFO] [timer.py:157:stop] 0/29800, SamplesPerSec=56.92175930112521
Tencent01: [2020-11-24 06:46:26,960] [INFO] [timer.py:157:stop] 0/29900, SamplesPerSec=56.929299109854675
Tencent01: [2020-11-24 06:50:21,654] [INFO] [logging.py:60:log_dist] [Rank 0] step=77500, skipped=78, lr=[0.00013283822605183107, 0.00013283822605183107], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 06:50:21,707] [INFO] [timer.py:157:stop] 0/30000, SamplesPerSec=56.921078525236986
Tencent01:  iteration    77500/  320000 | elapsed time per iteration (ms): 8832.7 | learning rate 1.328E-04 | lm loss 2.964335E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2451.54 | backward: 6259.66 | optimizer: 121.12 | batch generator: 6.39 | data loader: 1.09
Tencent01: [2020-11-24 06:52:20,979] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 06:52:20,979] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 06:52:20,980] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 06:52:20,980] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 06:52:20,979] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 06:52:20,980] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 06:52:20,979] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 06:52:20,980] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 06:52:20,980] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 06:52:20,980] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 06:52:20,980] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 06:52:20,980] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 06:52:20,980] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 06:52:20,980] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 06:52:20,980] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 06:52:20,980] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 06:52:20,980] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 06:52:20,980] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 06:52:20,980] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 06:52:20,980] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 06:52:20,980] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 06:52:20,980] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 06:52:20,980] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 06:52:20,980] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 06:52:20,980] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 06:52:20,980] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 06:52:20,980] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 06:52:20,980] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 06:52:20,981] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 06:52:20,981] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 06:52:20,981] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 06:52:20,981] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 06:55:27,278] [INFO] [timer.py:157:stop] 0/30100, SamplesPerSec=56.85343417661179
Tencent01: [2020-11-24 06:56:10,568] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 77529
Tencent01: [2020-11-24 06:56:10,569] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 06:56:10,569] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
Tencent01: [2020-11-24 06:56:10,569] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 77529
Tencent01: [2020-11-24 06:56:10,569] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 77529
Tencent01: [2020-11-24 06:56:10,569] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 06:56:10,569] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 06:56:10,569] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 77529
Tencent01: [2020-11-24 06:56:10,569] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 77529
Tencent01: [2020-11-24 06:56:10,569] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 77529
Tencent01: [2020-11-24 06:56:10,569] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 06:56:10,569] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 06:56:10,569] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 77529
Tencent01: [2020-11-24 06:56:10,569] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 06:56:10,569] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 06:56:10,569] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 77529
Tencent02: [2020-11-24 06:56:10,569] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 77529
Tencent02: [2020-11-24 06:56:10,569] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 77529
Tencent01: [2020-11-24 06:56:10,569] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 06:56:10,569] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 06:56:10,569] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 77529
Tencent02: [2020-11-24 06:56:10,569] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 06:56:10,569] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 06:56:10,569] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 77529
Tencent02: [2020-11-24 06:56:10,569] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 77529
Tencent02: [2020-11-24 06:56:10,569] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 77529
Tencent02: [2020-11-24 06:56:10,569] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 06:56:10,569] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 77529
Tencent02: [2020-11-24 06:56:10,569] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 06:56:10,569] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 06:56:10,569] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 77529
Tencent02: [2020-11-24 06:56:10,569] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 06:56:10,569] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 06:59:03,863] [INFO] [timer.py:157:stop] 0/30200, SamplesPerSec=56.86070998105682
Tencent01: [2020-11-24 07:02:40,081] [INFO] [timer.py:157:stop] 0/30300, SamplesPerSec=56.86824755549722
Tencent01: [2020-11-24 07:06:16,189] [INFO] [logging.py:60:log_dist] [Rank 0] step=77600, skipped=79, lr=[0.00013279449444969998, 0.00013279449444969998], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 07:06:16,241] [INFO] [timer.py:157:stop] 0/30400, SamplesPerSec=56.87578855686936
Tencent01:  iteration    77600/  320000 | elapsed time per iteration (ms): 9545.3 | learning rate 1.328E-04 | lm loss 2.946534E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2357.61 | backward: 7066.87 | optimizer: 120.49 | batch generator: 5.58 | data loader: 1.00
Tencent01: [2020-11-24 07:09:52,424] [INFO] [timer.py:157:stop] 0/30500, SamplesPerSec=56.88327806879798
Tencent01: [2020-11-24 07:13:28,529] [INFO] [timer.py:157:stop] 0/30600, SamplesPerSec=56.89077623539286
Tencent01: [2020-11-24 07:17:15,141] [INFO] [timer.py:157:stop] 0/30700, SamplesPerSec=56.88956828289867
Tencent01: [2020-11-24 07:22:03,040] [INFO] [logging.py:60:log_dist] [Rank 0] step=77700, skipped=79, lr=[0.0001327502728814656, 0.0001327502728814656], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 07:22:03,092] [INFO] [timer.py:157:stop] 0/30800, SamplesPerSec=56.838074978546764
Tencent01:  iteration    77700/  320000 | elapsed time per iteration (ms): 9468.5 | learning rate 1.327E-04 | lm loss 2.954848E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2464.56 | backward: 6881.66 | optimizer: 121.88 | batch generator: 5.94 | data loader: 1.14
Tencent01: [2020-11-24 07:26:08,475] [INFO] [timer.py:157:stop] 0/30900, SamplesPerSec=56.82173079000306
Tencent01: [2020-11-24 07:29:44,776] [INFO] [timer.py:157:stop] 0/31000, SamplesPerSec=56.829165412833454
Tencent01: [2020-11-24 07:33:21,032] [INFO] [timer.py:157:stop] 0/31100, SamplesPerSec=56.83657557730224
Tencent01: [2020-11-24 07:36:57,135] [INFO] [logging.py:60:log_dist] [Rank 0] step=77800, skipped=79, lr=[0.00013270600288058725, 0.00013270600288058725], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 07:36:57,187] [INFO] [timer.py:157:stop] 0/31200, SamplesPerSec=56.844038446791004
Tencent01:  iteration    77800/  320000 | elapsed time per iteration (ms): 8941.0 | learning rate 1.327E-04 | lm loss 2.952516E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2359.33 | backward: 6459.62 | optimizer: 121.60 | batch generator: 6.04 | data loader: 1.11
Tencent01: [2020-11-24 07:40:33,357] [INFO] [timer.py:157:stop] 0/31300, SamplesPerSec=56.85144199555141
Tencent01: [2020-11-24 07:44:20,609] [INFO] [timer.py:157:stop] 0/31400, SamplesPerSec=56.84986965918751
Tencent01: [2020-11-24 07:48:54,577] [INFO] [timer.py:157:stop] 0/31500, SamplesPerSec=56.810890959879536
Tencent01: [2020-11-24 07:53:00,175] [INFO] [logging.py:60:log_dist] [Rank 0] step=77900, skipped=79, lr=[0.00013266168448973357, 0.00013266168448973357], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 07:53:00,227] [INFO] [timer.py:157:stop] 0/31600, SamplesPerSec=56.794791899488466
Tencent01:  iteration    77900/  320000 | elapsed time per iteration (ms): 9630.4 | learning rate 1.327E-04 | lm loss 2.979403E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2357.29 | backward: 7150.77 | optimizer: 121.92 | batch generator: 5.76 | data loader: 1.13
Tencent01: [2020-11-24 07:56:36,563] [INFO] [timer.py:157:stop] 0/31700, SamplesPerSec=56.80210243629117
Tencent01: [2020-11-24 08:00:12,734] [INFO] [timer.py:157:stop] 0/31800, SamplesPerSec=56.80950270847626
Tencent01: [2020-11-24 08:03:48,855] [INFO] [timer.py:157:stop] 0/31900, SamplesPerSec=56.816894893338315
Tencent01: [2020-11-24 08:07:24,988] [INFO] [logging.py:60:log_dist] [Rank 0] step=78000, skipped=79, lr=[0.00013261731775161988, 0.00013261731775161988], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 08:07:25,041] [INFO] [timer.py:157:stop] 0/32000, SamplesPerSec=56.82420331886525
Tencent01:  iteration    78000/  320000 | elapsed time per iteration (ms): 8648.1 | learning rate 1.326E-04 | lm loss 3.007828E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2358.19 | backward: 6168.29 | optimizer: 121.29 | batch generator: 5.94 | data loader: 1.06
Tencent01: [2020-11-24 08:07:25,043] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: /root/data/checkpoints/gpt-345M11-14-13-21/78000/mp_rank_00_model_states.pt
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: -----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 78000 | LM loss: 2.889663E+00 | LM PPL: 1.798725E+01
Tencent01: -----------------------------------------------------------------------------------
Tencent01: [2020-11-24 08:13:22,036] [INFO] [timer.py:157:stop] 0/32100, SamplesPerSec=56.83025807877282
Tencent01: [2020-11-24 08:18:03,761] [INFO] [timer.py:157:stop] 0/32200, SamplesPerSec=56.786153969244694
Tencent01: [2020-11-24 08:22:26,787] [INFO] [timer.py:157:stop] 0/32300, SamplesPerSec=56.75694332696563
Tencent01: [2020-11-24 08:26:03,299] [INFO] [logging.py:60:log_dist] [Rank 0] step=78100, skipped=79, lr=[0.00013257290270900817, 0.00013257290270900817], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 08:26:03,350] [INFO] [timer.py:157:stop] 0/32400, SamplesPerSec=56.76402248708305
Tencent01:  iteration    78100/  320000 | elapsed time per iteration (ms): 11183.1 | learning rate 1.326E-04 | lm loss 2.965247E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2358.37 | backward: 7309.81 | optimizer: 121.75 | batch generator: 6.81 | data loader: 1.24
Tencent01: [2020-11-24 08:29:39,609] [INFO] [timer.py:157:stop] 0/32500, SamplesPerSec=56.77130595042232
Tencent01: [2020-11-24 08:33:15,746] [INFO] [timer.py:157:stop] 0/32600, SamplesPerSec=56.77865132169976
Tencent01: [2020-11-24 08:36:51,922] [INFO] [timer.py:157:stop] 0/32700, SamplesPerSec=56.78591139718099
Tencent01: [2020-11-24 08:40:28,048] [INFO] [logging.py:60:log_dist] [Rank 0] step=78200, skipped=79, lr=[0.00013252843940470685, 0.00013252843940470685], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 08:40:28,100] [INFO] [timer.py:157:stop] 0/32800, SamplesPerSec=56.79310776313379
Tencent01:  iteration    78200/  320000 | elapsed time per iteration (ms): 8647.5 | learning rate 1.325E-04 | lm loss 3.000971E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2357.79 | backward: 6167.97 | optimizer: 121.36 | batch generator: 5.91 | data loader: 1.07
Tencent01: [2020-11-24 08:44:53,258] [INFO] [timer.py:157:stop] 0/32900, SamplesPerSec=56.76277953635767
Tencent01: [2020-11-24 08:49:22,837] [INFO] [timer.py:157:stop] 0/33000, SamplesPerSec=56.7292920907981
Tencent01: [2020-11-24 08:53:08,871] [INFO] [timer.py:157:stop] 0/33100, SamplesPerSec=56.72911075120407
Tencent01: [2020-11-24 08:56:45,054] [INFO] [logging.py:60:log_dist] [Rank 0] step=78300, skipped=79, lr=[0.00013248392788157097, 0.00013248392788157097], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 08:56:45,106] [INFO] [timer.py:157:stop] 0/33200, SamplesPerSec=56.73634227626802
Tencent01:  iteration    78300/  320000 | elapsed time per iteration (ms): 9770.1 | learning rate 1.325E-04 | lm loss 2.958872E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2464.37 | backward: 7183.45 | optimizer: 121.82 | batch generator: 5.71 | data loader: 1.12
Tencent01: [2020-11-24 09:00:21,315] [INFO] [timer.py:157:stop] 0/33300, SamplesPerSec=56.74354634421712
Tencent01: [2020-11-24 09:03:57,503] [INFO] [timer.py:157:stop] 0/33400, SamplesPerSec=56.750729329609115
Tencent01: [2020-11-24 09:07:33,722] [INFO] [timer.py:157:stop] 0/33500, SamplesPerSec=56.75785294679884
Tencent01: [2020-11-24 09:11:39,202] [INFO] [logging.py:60:log_dist] [Rank 0] step=78400, skipped=79, lr=[0.00013243936818250197, 0.00013243936818250197], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 09:11:39,254] [INFO] [timer.py:157:stop] 0/33600, SamplesPerSec=56.742981955145886
Tencent01:  iteration    78400/  320000 | elapsed time per iteration (ms): 8941.5 | learning rate 1.324E-04 | lm loss 2.988989E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2449.85 | backward: 6369.93 | optimizer: 121.33 | batch generator: 5.46 | data loader: 1.00
Tencent01: [2020-11-24 09:15:58,558] [INFO] [timer.py:157:stop] 0/33700, SamplesPerSec=56.71793852606339
Tencent01: [2020-11-24 09:20:13,418] [INFO] [timer.py:157:stop] 0/33800, SamplesPerSec=56.69636642023904
Tencent01: [2020-11-24 09:23:49,807] [INFO] [timer.py:157:stop] 0/33900, SamplesPerSec=56.70342849219062
Tencent01: [2020-11-24 09:27:26,008] [INFO] [logging.py:60:log_dist] [Rank 0] step=78500, skipped=79, lr=[0.0001323947603504478, 0.0001323947603504478], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 09:27:26,060] [INFO] [timer.py:157:stop] 0/34000, SamplesPerSec=56.71056747615452
Tencent01:  iteration    78500/  320000 | elapsed time per iteration (ms): 9468.1 | learning rate 1.324E-04 | lm loss 2.916331E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2359.34 | backward: 6986.83 | optimizer: 121.51 | batch generator: 6.01 | data loader: 1.08
Tencent01: [2020-11-24 09:30:36,203] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 78521
Tencent01: [2020-11-24 09:30:36,203] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-24 09:30:36,203] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Tencent01: [2020-11-24 09:30:36,203] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 78521
Tencent01: [2020-11-24 09:30:36,203] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-24 09:30:36,203] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 78521
Tencent01: [2020-11-24 09:30:36,203] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 78521
Tencent01: [2020-11-24 09:30:36,203] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-24 09:30:36,203] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-24 09:30:36,203] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 78521
Tencent01: [2020-11-24 09:30:36,203] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 78521
Tencent01: [2020-11-24 09:30:36,204] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-24 09:30:36,204] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-24 09:30:36,204] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 78521
Tencent02: [2020-11-24 09:30:36,204] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 78521
Tencent02: [2020-11-24 09:30:36,204] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-24 09:30:36,204] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: Grad overflow on iteration 78521
Tencent02: [2020-11-24 09:30:36,204] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 78521
Tencent02: [2020-11-24 09:30:36,204] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-24 09:30:36,204] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-24 09:30:36,204] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-24 09:30:36,204] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 78521
Tencent02: [2020-11-24 09:30:36,204] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 78521
Tencent02: [2020-11-24 09:30:36,204] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-24 09:30:36,204] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-24 09:30:36,204] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 78521
Tencent02: [2020-11-24 09:30:36,205] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-24 09:30:36,204] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 78521
Tencent02: [2020-11-24 09:30:36,205] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-24 09:30:36,204] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 78521
Tencent02: [2020-11-24 09:30:36,205] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 78521
Tencent01: [2020-11-24 09:30:36,204] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-24 09:30:36,205] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-24 09:31:02,121] [INFO] [timer.py:157:stop] 0/34100, SamplesPerSec=56.71779737589883
Tencent01: [2020-11-24 09:34:38,212] [INFO] [timer.py:157:stop] 0/34200, SamplesPerSec=56.724956499371984
Tencent01: [2020-11-24 09:38:22,321] [INFO] [timer.py:157:stop] 0/34300, SamplesPerSec=56.726200303403374
Tencent01: [2020-11-24 09:42:59,909] [INFO] [logging.py:60:log_dist] [Rank 0] step=78600, skipped=80, lr=[0.00013235055122552732, 0.00013235055122552732], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 09:42:59,961] [INFO] [timer.py:157:stop] 0/34400, SamplesPerSec=56.68834541279656
Tencent01:  iteration    78600/  320000 | elapsed time per iteration (ms): 9339.0 | learning rate 1.324E-04 | lm loss 2.967981E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2357.36 | backward: 6861.04 | optimizer: 120.26 | batch generator: 5.84 | data loader: 1.03
Tencent01: [2020-11-24 09:47:15,992] [INFO] [timer.py:157:stop] 0/34500, SamplesPerSec=56.666472361661626
Tencent01: [2020-11-24 09:50:52,359] [INFO] [timer.py:157:stop] 0/34600, SamplesPerSec=56.673491739969066
Tencent01: [2020-11-24 09:54:28,468] [INFO] [timer.py:157:stop] 0/34700, SamplesPerSec=56.680658646351944
Tencent01: [2020-11-24 09:58:04,519] [INFO] [logging.py:60:log_dist] [Rank 0] step=78700, skipped=80, lr=[0.00013230584773678828, 0.00013230584773678828], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 09:58:04,571] [INFO] [timer.py:157:stop] 0/34800, SamplesPerSec=56.687801582708516
Tencent01:  iteration    78700/  320000 | elapsed time per iteration (ms): 9046.1 | learning rate 1.323E-04 | lm loss 2.962842E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2359.37 | backward: 6564.57 | optimizer: 121.76 | batch generator: 6.14 | data loader: 1.12
Tencent01: [2020-11-24 10:01:40,696] [INFO] [timer.py:157:stop] 0/34900, SamplesPerSec=56.694879386604555
Tencent01: [2020-11-24 10:05:27,693] [INFO] [timer.py:157:stop] 0/35000, SamplesPerSec=56.694104023744245
Tencent01: [2020-11-24 10:10:04,571] [INFO] [timer.py:157:stop] 0/35100, SamplesPerSec=56.657687264760916
Tencent01: [2020-11-24 10:14:19,598] [INFO] [logging.py:60:log_dist] [Rank 0] step=78800, skipped=80, lr=[0.0001322610962437549, 0.0001322610962437549], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 10:14:19,650] [INFO] [timer.py:157:stop] 0/35200, SamplesPerSec=56.6370339492838
Tencent01:  iteration    78800/  320000 | elapsed time per iteration (ms): 9750.8 | learning rate 1.323E-04 | lm loss 2.952502E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2357.92 | backward: 7270.94 | optimizer: 121.54 | batch generator: 5.90 | data loader: 1.10
Tencent01: [2020-11-24 10:17:56,134] [INFO] [timer.py:157:stop] 0/35300, SamplesPerSec=56.64390733876222
Tencent01: [2020-11-24 10:21:32,281] [INFO] [timer.py:157:stop] 0/35400, SamplesPerSec=56.65097369663764
Tencent01: [2020-11-24 10:25:08,403] [INFO] [timer.py:157:stop] 0/35500, SamplesPerSec=56.6580292079914
Tencent01: [2020-11-24 10:28:44,457] [INFO] [logging.py:60:log_dist] [Rank 0] step=78900, skipped=80, lr=[0.00013221629678955983, 0.00013221629678955983], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 10:28:44,509] [INFO] [timer.py:157:stop] 0/35600, SamplesPerSec=56.665054747482195
Tencent01:  iteration    78900/  320000 | elapsed time per iteration (ms): 8648.6 | learning rate 1.322E-04 | lm loss 2.998638E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2358.29 | backward: 6168.68 | optimizer: 121.25 | batch generator: 5.87 | data loader: 1.03
Tencent01: [2020-11-24 10:32:20,632] [INFO] [timer.py:157:stop] 0/35700, SamplesPerSec=56.672043630078875
Tencent01: [2020-11-24 10:37:05,879] [INFO] [timer.py:157:stop] 0/35800, SamplesPerSec=56.63056159354796
Tencent01: [2020-11-24 10:41:11,826] [INFO] [timer.py:157:stop] 0/35900, SamplesPerSec=56.616760306491074
Tencent01: [2020-11-24 10:44:58,063] [INFO] [logging.py:60:log_dist] [Rank 0] step=79000, skipped=80, lr=[0.00013217144941738215, 0.00013217144941738215], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 10:44:58,116] [INFO] [timer.py:157:stop] 0/36000, SamplesPerSec=56.616724321444664
Tencent01:  iteration    79000/  320000 | elapsed time per iteration (ms): 9736.1 | learning rate 1.322E-04 | lm loss 3.014976E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2451.66 | backward: 7162.96 | optimizer: 121.09 | batch generator: 5.84 | data loader: 1.02
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: -----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 79000 | LM loss: 2.891487E+00 | LM PPL: 1.802008E+01
Tencent01: -----------------------------------------------------------------------------------
Tencent01: [2020-11-24 10:49:30,544] [INFO] [timer.py:157:stop] 0/36100, SamplesPerSec=56.62285884853548
Tencent01: [2020-11-24 10:53:06,611] [INFO] [timer.py:157:stop] 0/36200, SamplesPerSec=56.62986780122564
Tencent01: [2020-11-24 10:56:42,667] [INFO] [timer.py:157:stop] 0/36300, SamplesPerSec=56.63686005237641
Tencent01: [2020-11-24 11:00:18,683] [INFO] [logging.py:60:log_dist] [Rank 0] step=79100, skipped=80, lr=[0.00013212655417044701, 0.00013212655417044701], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 11:00:18,735] [INFO] [timer.py:157:stop] 0/36400, SamplesPerSec=56.64380873414987
Tencent01:  iteration    79100/  320000 | elapsed time per iteration (ms): 9206.2 | learning rate 1.321E-04 | lm loss 2.981944E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2356.94 | backward: 6177.36 | optimizer: 121.09 | batch generator: 6.51 | data loader: 1.11
Tencent01: [2020-11-24 11:04:33,131] [INFO] [timer.py:157:stop] 0/36500, SamplesPerSec=56.62439283888747
Tencent01: [2020-11-24 11:08:59,675] [INFO] [timer.py:157:stop] 0/36600, SamplesPerSec=56.59679365271341
Tencent01: [2020-11-24 11:12:54,510] [INFO] [timer.py:157:stop] 0/36700, SamplesPerSec=56.59098977571198
Tencent01: [2020-11-24 11:16:30,659] [INFO] [logging.py:60:log_dist] [Rank 0] step=79200, skipped=80, lr=[0.00013208161109202576, 0.00013208161109202576], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 11:16:30,712] [INFO] [timer.py:157:stop] 0/36800, SamplesPerSec=56.59788322381243
Tencent01:  iteration    79200/  320000 | elapsed time per iteration (ms): 9719.8 | learning rate 1.321E-04 | lm loss 2.949648E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2434.79 | backward: 7163.46 | optimizer: 121.16 | batch generator: 5.57 | data loader: 0.98
Tencent01: [2020-11-24 11:20:06,821] [INFO] [timer.py:157:stop] 0/36900, SamplesPerSec=56.60480648658333
Tencent01: [2020-11-24 11:23:42,904] [INFO] [timer.py:157:stop] 0/37000, SamplesPerSec=56.61170937506245
Tencent01: [2020-11-24 11:27:18,983] [INFO] [timer.py:157:stop] 0/37100, SamplesPerSec=56.61857636708407
Tencent01: [2020-11-24 11:31:22,998] [INFO] [logging.py:60:log_dist] [Rank 0] step=79300, skipped=80, lr=[0.00013203662022543572, 0.00013203662022543572], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 11:31:23,051] [INFO] [timer.py:157:stop] 0/37200, SamplesPerSec=56.60657271699079
Tencent01:  iteration    79300/  320000 | elapsed time per iteration (ms): 8923.4 | learning rate 1.320E-04 | lm loss 2.941155E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2357.40 | backward: 6444.35 | optimizer: 121.28 | batch generator: 5.72 | data loader: 1.03
Tencent01: [2020-11-24 11:35:51,133] [INFO] [timer.py:157:stop] 0/37300, SamplesPerSec=56.57852697159612
Tencent01: [2020-11-24 11:39:45,139] [INFO] [timer.py:157:stop] 0/37400, SamplesPerSec=56.57342443358563
Tencent01: [2020-11-24 11:43:21,442] [INFO] [timer.py:157:stop] 0/37500, SamplesPerSec=56.580170818072816
Tencent01: [2020-11-24 11:46:57,449] [INFO] [logging.py:60:log_dist] [Rank 0] step=79400, skipped=80, lr=[0.00013199158161404046, 0.00013199158161404046], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 11:46:57,501] [INFO] [timer.py:157:stop] 0/37600, SamplesPerSec=56.58705117997485
Tencent01:  iteration    79400/  320000 | elapsed time per iteration (ms): 9344.5 | learning rate 1.320E-04 | lm loss 3.017524E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2359.25 | backward: 6863.41 | optimizer: 121.48 | batch generator: 5.79 | data loader: 1.03
Tencent01: [2020-11-24 11:50:33,588] [INFO] [timer.py:157:stop] 0/37700, SamplesPerSec=56.59387560517212
Tencent01: [2020-11-24 11:54:09,698] [INFO] [timer.py:157:stop] 0/37800, SamplesPerSec=56.60064466970428
Tencent01: [2020-11-24 11:57:53,963] [INFO] [timer.py:157:stop] 0/37900, SamplesPerSec=56.60196659500513
Tencent01: [2020-11-24 12:02:27,703] [INFO] [logging.py:60:log_dist] [Rank 0] step=79500, skipped=80, lr=[0.00013194649530124948, 0.00013194649530124948], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 12:02:27,755] [INFO] [timer.py:157:stop] 0/38000, SamplesPerSec=56.57069590488847
Tencent01:  iteration    79500/  320000 | elapsed time per iteration (ms): 9302.5 | learning rate 1.319E-04 | lm loss 2.969940E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2357.28 | backward: 6823.82 | optimizer: 121.09 | batch generator: 5.54 | data loader: 0.94
Tencent01: [2020-11-24 12:06:13,357] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 12:06:13,357] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-24 12:06:13,358] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 12:06:13,358] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-24 12:06:13,358] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 12:06:13,358] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-24 12:06:13,358] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 12:06:13,358] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 12:06:13,358] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-24 12:06:13,358] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-24 12:06:13,358] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 12:06:13,359] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-24 12:06:13,359] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 12:06:13,359] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-24 12:06:13,359] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 12:06:13,359] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-24 12:06:13,358] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 12:06:13,358] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 12:06:13,358] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 12:06:13,357] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 12:06:13,358] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 12:06:13,358] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 12:06:13,358] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 12:06:13,358] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 12:06:13,359] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-24 12:06:13,359] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-24 12:06:13,359] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-24 12:06:13,359] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-24 12:06:13,359] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-24 12:06:13,359] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-24 12:06:13,359] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-24 12:06:13,359] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-24 12:06:30,771] [INFO] [timer.py:157:stop] 0/38100, SamplesPerSec=56.55980790970002
Tencent01: [2020-11-24 12:10:07,198] [INFO] [timer.py:157:stop] 0/38200, SamplesPerSec=56.56637850272345
Tencent01: [2020-11-24 12:13:43,390] [INFO] [timer.py:157:stop] 0/38300, SamplesPerSec=56.57306828816311
Tencent01: [2020-11-24 12:17:19,462] [INFO] [logging.py:60:log_dist] [Rank 0] step=79600, skipped=80, lr=[0.00013190136133051816, 0.00013190136133051816], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 12:17:19,514] [INFO] [timer.py:157:stop] 0/38400, SamplesPerSec=56.57977952400131
Tencent01:  iteration    79600/  320000 | elapsed time per iteration (ms): 8917.6 | learning rate 1.319E-04 | lm loss 2.946548E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2359.66 | backward: 6436.04 | optimizer: 121.51 | batch generator: 5.90 | data loader: 1.07
Tencent01: [2020-11-24 12:20:55,725] [INFO] [timer.py:157:stop] 0/38500, SamplesPerSec=56.58639328136137
Tencent01: [2020-11-24 12:24:42,946] [INFO] [timer.py:157:stop] 0/38600, SamplesPerSec=56.58583097572661
Tencent01: [2020-11-24 12:29:14,728] [INFO] [timer.py:157:stop] 0/38700, SamplesPerSec=56.556498621325986
Tencent01: [2020-11-24 12:33:09,926] [INFO] [logging.py:60:log_dist] [Rank 0] step=79700, skipped=80, lr=[0.00013185617974534805, 0.00013185617974534805], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 12:33:09,977] [INFO] [timer.py:157:stop] 0/38800, SamplesPerSec=56.55084815769918
Tencent01:  iteration    79700/  320000 | elapsed time per iteration (ms): 9504.6 | learning rate 1.319E-04 | lm loss 2.980981E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2357.93 | backward: 7024.26 | optimizer: 122.02 | batch generator: 5.82 | data loader: 1.15
Tencent01: [2020-11-24 12:36:54,729] [INFO] [timer.py:157:stop] 0/38900, SamplesPerSec=56.5519642280979
Tencent01: [2020-11-24 12:40:30,973] [INFO] [timer.py:157:stop] 0/39000, SamplesPerSec=56.55853946052316
Tencent01: [2020-11-24 12:44:07,154] [INFO] [timer.py:157:stop] 0/39100, SamplesPerSec=56.56511691630142
Tencent01: [2020-11-24 12:47:43,309] [INFO] [logging.py:60:log_dist] [Rank 0] step=79800, skipped=80, lr=[0.0001318109505892863, 0.0001318109505892863], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 12:47:43,361] [INFO] [timer.py:157:stop] 0/39200, SamplesPerSec=56.571647197054666
Tencent01:  iteration    79800/  320000 | elapsed time per iteration (ms): 8733.8 | learning rate 1.318E-04 | lm loss 2.946769E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2358.91 | backward: 6252.63 | optimizer: 121.88 | batch generator: 5.65 | data loader: 1.13
Tencent01: [2020-11-24 12:51:19,550] [INFO] [timer.py:157:stop] 0/39300, SamplesPerSec=56.57815501578447
Tencent01: [2020-11-24 12:55:52,117] [INFO] [timer.py:157:stop] 0/39400, SamplesPerSec=56.54886681825608
Tencent01: [2020-11-24 13:00:10,175] [INFO] [timer.py:157:stop] 0/39500, SamplesPerSec=56.52893285998463
Tencent01: [2020-11-24 13:03:54,905] [INFO] [logging.py:60:log_dist] [Rank 0] step=79900, skipped=80, lr=[0.00013176567390592612, 0.00013176567390592612], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 13:03:54,957] [INFO] [timer.py:157:stop] 0/39600, SamplesPerSec=56.53007404076908
Tencent01:  iteration    79900/  320000 | elapsed time per iteration (ms): 9716.0 | learning rate 1.318E-04 | lm loss 2.974983E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2358.22 | backward: 7235.30 | optimizer: 122.04 | batch generator: 6.02 | data loader: 1.12
Tencent01: [2020-11-24 13:07:31,297] [INFO] [timer.py:157:stop] 0/39700, SamplesPerSec=56.5365334954849
Tencent01: [2020-11-24 13:11:07,465] [INFO] [timer.py:157:stop] 0/39800, SamplesPerSec=56.543060413961136
Tencent01: [2020-11-24 13:14:43,683] [INFO] [timer.py:157:stop] 0/39900, SamplesPerSec=56.5495264081522
Tencent01: [2020-11-24 13:18:19,858] [INFO] [logging.py:60:log_dist] [Rank 0] step=80000, skipped=80, lr=[0.0001317203497389065, 0.0001317203497389065], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 13:18:19,910] [INFO] [timer.py:157:stop] 0/40000, SamplesPerSec=56.55594831972893
Tencent01:  iteration    80000/  320000 | elapsed time per iteration (ms): 8649.5 | learning rate 1.317E-04 | lm loss 2.954391E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2358.66 | backward: 6168.84 | optimizer: 121.63 | batch generator: 6.17 | data loader: 1.12
Tencent01: [2020-11-24 13:18:19,913] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: /root/data/checkpoints/gpt-345M11-14-13-21/80000/mp_rank_00_model_states.pt
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: -----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 80000 | LM loss: 2.885219E+00 | LM PPL: 1.790749E+01
Tencent01: -----------------------------------------------------------------------------------
Tencent01: [2020-11-24 13:24:42,826] [INFO] [timer.py:157:stop] 0/40100, SamplesPerSec=56.53690164189931
Tencent01: [2020-11-24 13:29:16,094] [INFO] [timer.py:157:stop] 0/40200, SamplesPerSec=56.50791135382227
Tencent01: [2020-11-24 13:33:10,259] [INFO] [timer.py:157:stop] 0/40300, SamplesPerSec=56.50327343218536
Tencent01: [2020-11-24 13:36:46,502] [INFO] [logging.py:60:log_dist] [Rank 0] step=80100, skipped=80, lr=[0.00013167497813191216, 0.00013167497813191216], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 13:36:46,553] [INFO] [timer.py:157:stop] 0/40400, SamplesPerSec=56.509689664053816
Tencent01:  iteration    80100/  320000 | elapsed time per iteration (ms): 11066.4 | learning rate 1.317E-04 | lm loss 2.913051E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2358.69 | backward: 7326.77 | optimizer: 121.94 | batch generator: 7.55 | data loader: 1.28
Tencent01: [2020-11-24 13:40:22,674] [INFO] [timer.py:157:stop] 0/40500, SamplesPerSec=56.516186059990936
Tencent01: [2020-11-24 13:43:58,800] [INFO] [timer.py:157:stop] 0/40600, SamplesPerSec=56.5226543338246
Tencent01: [2020-11-24 13:47:34,929] [INFO] [timer.py:157:stop] 0/40700, SamplesPerSec=56.529090440155585
Tencent01: [2020-11-24 13:51:31,313] [INFO] [logging.py:60:log_dist] [Rank 0] step=80200, skipped=80, lr=[0.00013162955912867348, 0.00013162955912867348], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 13:51:31,365] [INFO] [timer.py:157:stop] 0/40800, SamplesPerSec=56.52306816247869
Tencent01:  iteration    80200/  320000 | elapsed time per iteration (ms): 8848.1 | learning rate 1.316E-04 | lm loss 3.015606E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2357.31 | backward: 6368.23 | optimizer: 122.16 | batch generator: 5.86 | data loader: 1.15
Tencent01: [2020-11-24 13:56:02,030] [INFO] [timer.py:157:stop] 0/40900, SamplesPerSec=56.49619870706649
Tencent01: [2020-11-24 14:00:16,549] [INFO] [timer.py:157:stop] 0/41000, SamplesPerSec=56.47928623375109
Tencent01: [2020-11-24 14:03:52,967] [INFO] [timer.py:157:stop] 0/41100, SamplesPerSec=56.485580150449636
Tencent01: [2020-11-24 14:07:29,114] [INFO] [logging.py:60:log_dist] [Rank 0] step=80300, skipped=80, lr=[0.00013158409277296663, 0.00013158409277296663], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 14:07:29,166] [INFO] [timer.py:157:stop] 0/41200, SamplesPerSec=56.491976274884685
Tencent01:  iteration    80300/  320000 | elapsed time per iteration (ms): 9578.0 | learning rate 1.316E-04 | lm loss 2.896236E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2358.69 | backward: 7097.37 | optimizer: 121.57 | batch generator: 5.73 | data loader: 1.05
Tencent01: [2020-11-24 14:11:05,373] [INFO] [timer.py:157:stop] 0/41300, SamplesPerSec=56.49832568481372
Tencent01: [2020-11-24 14:14:41,623] [INFO] [timer.py:157:stop] 0/41400, SamplesPerSec=56.5046308097843
Tencent01: [2020-11-24 14:18:27,660] [INFO] [timer.py:157:stop] 0/41500, SamplesPerSec=56.50499864781599
Tencent01: [2020-11-24 14:23:00,483] [INFO] [logging.py:60:log_dist] [Rank 0] step=80400, skipped=80, lr=[0.00013153857910861339, 0.00013153857910861339], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 14:23:00,535] [INFO] [timer.py:157:stop] 0/41600, SamplesPerSec=56.4773249479982
Tencent01:  iteration    80400/  320000 | elapsed time per iteration (ms): 9313.7 | learning rate 1.315E-04 | lm loss 2.981330E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2357.15 | backward: 6835.16 | optimizer: 121.02 | batch generator: 5.46 | data loader: 0.90
Tencent01: [2020-11-24 14:27:07,101] [INFO] [timer.py:157:stop] 0/41700, SamplesPerSec=56.46552137990337
Tencent01: [2020-11-24 14:30:52,974] [INFO] [timer.py:157:stop] 0/41800, SamplesPerSec=56.46610670023378
Tencent01: [2020-11-24 14:34:29,139] [INFO] [timer.py:157:stop] 0/41900, SamplesPerSec=56.472455063376614
Tencent01: [2020-11-24 14:38:05,270] [INFO] [logging.py:60:log_dist] [Rank 0] step=80500, skipped=80, lr=[0.00013149301817948116, 0.00013149301817948116], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 14:38:05,322] [INFO] [timer.py:157:stop] 0/42000, SamplesPerSec=56.47876317176811
Tencent01:  iteration    80500/  320000 | elapsed time per iteration (ms): 9047.9 | learning rate 1.315E-04 | lm loss 3.011067E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2359.13 | backward: 6566.66 | optimizer: 121.68 | batch generator: 5.95 | data loader: 1.11
Tencent01: [2020-11-24 14:41:24,171] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 14:41:24,171] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 14:41:24,171] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 14:41:24,171] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 14:41:24,171] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 14:41:24,171] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 14:41:24,171] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 14:41:24,171] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 14:41:24,171] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 14:41:24,171] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 14:41:24,172] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 14:41:24,172] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 14:41:24,172] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 14:41:24,172] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 14:41:24,172] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 14:41:24,172] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 14:41:24,172] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 14:41:24,172] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 14:41:24,172] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 14:41:24,172] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 14:41:24,172] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 14:41:24,172] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 14:41:24,172] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 14:41:24,172] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 14:41:24,172] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 14:41:24,172] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 14:41:24,172] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 14:41:24,172] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 14:41:24,172] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 14:41:24,172] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 14:41:24,172] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 14:41:24,173] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 14:41:41,558] [INFO] [timer.py:157:stop] 0/42100, SamplesPerSec=56.48500921374755
Tencent01: [2020-11-24 14:45:28,978] [INFO] [timer.py:157:stop] 0/42200, SamplesPerSec=56.48461519536275
Tencent01: [2020-11-24 14:48:49,769] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 80569
Tencent01: [2020-11-24 14:48:49,769] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 14:48:49,769] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 80569
Tencent01: [2020-11-24 14:48:49,769] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 14:48:49,769] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 80569
Tencent01: [2020-11-24 14:48:49,769] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 14:48:49,769] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 80569
Tencent01: [2020-11-24 14:48:49,769] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 14:48:49,769] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 80569
Tencent02: [2020-11-24 14:48:49,769] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 80569
Tencent02: [2020-11-24 14:48:49,769] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 80569
Tencent02: [2020-11-24 14:48:49,769] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 14:48:49,769] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 14:48:49,769] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-24 14:48:49,769] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 80569
Tencent01: Grad overflow on iteration 80569
Tencent02: [2020-11-24 14:48:49,769] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 14:48:49,769] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 80569
Tencent02: [2020-11-24 14:48:49,769] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 80569
Tencent02: [2020-11-24 14:48:49,769] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 14:48:49,769] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 14:48:49,769] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 14:48:49,769] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 80569
Tencent02: [2020-11-24 14:48:49,769] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 14:48:49,769] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 14:48:49,769] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
Tencent01: [2020-11-24 14:48:49,769] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 80569
Tencent01: [2020-11-24 14:48:49,770] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 14:48:49,770] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 80569
Tencent01: [2020-11-24 14:48:49,770] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 14:48:49,770] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 80569
Tencent02: [2020-11-24 14:48:49,770] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 14:48:49,773] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 80569
Tencent01: [2020-11-24 14:48:49,773] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 14:49:52,755] [INFO] [timer.py:157:stop] 0/42300, SamplesPerSec=56.46282368524907
Tencent01: [2020-11-24 14:53:58,174] [INFO] [logging.py:60:log_dist] [Rank 0] step=80600, skipped=81, lr=[0.00013144786634458172, 0.00013144786634458172], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 14:53:58,226] [INFO] [timer.py:157:stop] 0/42400, SamplesPerSec=56.45190600451169
Tencent01:  iteration    80600/  320000 | elapsed time per iteration (ms): 9529.0 | learning rate 1.314E-04 | lm loss 2.973947E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2358.38 | backward: 7049.25 | optimizer: 121.00 | batch generator: 5.96 | data loader: 1.14
Tencent01: [2020-11-24 14:57:45,327] [INFO] [timer.py:157:stop] 0/42500, SamplesPerSec=56.4517903951316
Tencent01: [2020-11-24 15:01:21,561] [INFO] [timer.py:157:stop] 0/42600, SamplesPerSec=56.45802434262032
Tencent01: [2020-11-24 15:04:57,714] [INFO] [timer.py:157:stop] 0/42700, SamplesPerSec=56.464276841659704
Tencent01: [2020-11-24 15:08:33,817] [INFO] [logging.py:60:log_dist] [Rank 0] step=80700, skipped=81, lr=[0.000131402211489227, 0.000131402211489227], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 15:08:33,869] [INFO] [timer.py:157:stop] 0/42800, SamplesPerSec=56.47049878867296
Tencent01:  iteration    80700/  320000 | elapsed time per iteration (ms): 8756.4 | learning rate 1.314E-04 | lm loss 2.960074E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2358.38 | backward: 6275.47 | optimizer: 122.17 | batch generator: 5.69 | data loader: 1.09
Tencent01: [2020-11-24 15:12:20,522] [INFO] [timer.py:157:stop] 0/42900, SamplesPerSec=56.4705965767796
Tencent01: [2020-11-24 15:16:34,689] [INFO] [timer.py:157:stop] 0/43000, SamplesPerSec=56.45475970077734
Tencent01: [2020-11-24 15:20:47,761] [INFO] [timer.py:157:stop] 0/43100, SamplesPerSec=56.43964211596328
Tencent01: [2020-11-24 15:24:44,209] [INFO] [logging.py:60:log_dist] [Rank 0] step=80800, skipped=81, lr=[0.00013135650950052823, 0.00013135650950052823], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 15:24:44,261] [INFO] [timer.py:157:stop] 0/43200, SamplesPerSec=56.434132317706066
Tencent01:  iteration    80800/  320000 | elapsed time per iteration (ms): 9703.9 | learning rate 1.314E-04 | lm loss 2.932368E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2465.54 | backward: 7116.02 | optimizer: 121.94 | batch generator: 5.68 | data loader: 1.13
Tencent01: [2020-11-24 15:28:20,609] [INFO] [timer.py:157:stop] 0/43300, SamplesPerSec=56.440233684378654
Tencent01: [2020-11-24 15:31:56,853] [INFO] [timer.py:157:stop] 0/43400, SamplesPerSec=56.446367744157946
Tencent01: [2020-11-24 15:35:33,055] [INFO] [timer.py:157:stop] 0/43500, SamplesPerSec=56.4525013237233
Tencent01: [2020-11-24 15:39:09,136] [INFO] [logging.py:60:log_dist] [Rank 0] step=80900, skipped=81, lr=[0.00013131076042253428, 0.00013131076042253428], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 15:39:09,188] [INFO] [timer.py:157:stop] 0/43600, SamplesPerSec=56.45864960510418
Tencent01:  iteration    80900/  320000 | elapsed time per iteration (ms): 8649.3 | learning rate 1.313E-04 | lm loss 2.978066E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2357.60 | backward: 6169.39 | optimizer: 121.87 | batch generator: 5.59 | data loader: 1.12
Tencent01: [2020-11-24 15:43:15,126] [INFO] [timer.py:157:stop] 0/43700, SamplesPerSec=56.447794090799924
Tencent01: [2020-11-24 15:47:37,178] [INFO] [timer.py:157:stop] 0/43800, SamplesPerSec=56.42782830296089
Tencent01: [2020-11-24 15:51:40,808] [INFO] [timer.py:157:stop] 0/43900, SamplesPerSec=56.41841786366351
Tencent01: [2020-11-24 15:55:17,161] [INFO] [logging.py:60:log_dist] [Rank 0] step=81000, skipped=81, lr=[0.00013126496429933948, 0.00013126496429933948], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 15:55:17,213] [INFO] [timer.py:157:stop] 0/44000, SamplesPerSec=56.42442633466955
Tencent01:  iteration    81000/  320000 | elapsed time per iteration (ms): 9680.2 | learning rate 1.313E-04 | lm loss 2.982626E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2466.49 | backward: 7091.12 | optimizer: 122.21 | batch generator: 6.04 | data loader: 1.18
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: -----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 81000 | LM loss: 2.815697E+00 | LM PPL: 1.670482E+01
Tencent01: -----------------------------------------------------------------------------------
Tencent01: [2020-11-24 15:59:49,651] [INFO] [timer.py:157:stop] 0/44100, SamplesPerSec=56.429833092989895
Tencent01: [2020-11-24 16:03:25,823] [INFO] [timer.py:157:stop] 0/44200, SamplesPerSec=56.43592576923129
Tencent01: [2020-11-24 16:07:01,969] [INFO] [timer.py:157:stop] 0/44300, SamplesPerSec=56.442009936184895
Tencent01: [2020-11-24 16:10:57,768] [INFO] [logging.py:60:log_dist] [Rank 0] step=81100, skipped=81, lr=[0.00013121912117508336, 0.00013121912117508336], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 16:10:57,815] [INFO] [timer.py:157:stop] 0/44400, SamplesPerSec=56.43702011656177
Tencent01:  iteration    81100/  320000 | elapsed time per iteration (ms): 9406.0 | learning rate 1.312E-04 | lm loss 2.926201E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2357.52 | backward: 6375.96 | optimizer: 121.96 | batch generator: 7.07 | data loader: 1.23
Tencent01: [2020-11-24 16:15:32,700] [INFO] [timer.py:157:stop] 0/44500, SamplesPerSec=56.41023628420812
Tencent01: [2020-11-24 16:19:42,942] [INFO] [timer.py:157:stop] 0/44600, SamplesPerSec=56.3973263769868
Tencent01: [2020-11-24 16:23:19,370] [INFO] [timer.py:157:stop] 0/44700, SamplesPerSec=56.40327470194109
Tencent01: [2020-11-24 16:26:55,501] [INFO] [logging.py:60:log_dist] [Rank 0] step=81200, skipped=81, lr=[0.00013117323109395083, 0.00013117323109395083], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 16:26:55,553] [INFO] [timer.py:157:stop] 0/44800, SamplesPerSec=56.40933881803805
Tencent01:  iteration    81200/  320000 | elapsed time per iteration (ms): 9577.4 | learning rate 1.312E-04 | lm loss 3.014817E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2360.68 | backward: 7094.60 | optimizer: 121.72 | batch generator: 6.17 | data loader: 1.11
Tencent01: [2020-11-24 16:30:31,676] [INFO] [timer.py:157:stop] 0/44900, SamplesPerSec=56.415412396827556
Tencent01: [2020-11-24 16:34:07,766] [INFO] [timer.py:157:stop] 0/45000, SamplesPerSec=56.421472541766825
Tencent01: [2020-11-24 16:37:51,925] [INFO] [timer.py:157:stop] 0/45100, SamplesPerSec=56.42305379100731
Tencent01: [2020-11-24 16:42:23,576] [INFO] [logging.py:60:log_dist] [Rank 0] step=81300, skipped=81, lr=[0.00013112729410017212, 0.00013112729410017212], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 16:42:23,628] [INFO] [timer.py:157:stop] 0/45200, SamplesPerSec=56.39847103764828
Tencent01:  iteration    81300/  320000 | elapsed time per iteration (ms): 9280.8 | learning rate 1.311E-04 | lm loss 3.017061E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2357.86 | backward: 6801.46 | optimizer: 121.08 | batch generator: 6.08 | data loader: 1.05
Tencent01: [2020-11-24 16:46:28,836] [INFO] [timer.py:157:stop] 0/45300, SamplesPerSec=56.388547202150136
Tencent01: [2020-11-24 16:50:22,018] [INFO] [timer.py:157:stop] 0/45400, SamplesPerSec=56.385241380209635
Tencent01: [2020-11-24 16:53:58,314] [INFO] [timer.py:157:stop] 0/45500, SamplesPerSec=56.39117968558153
Tencent01: [2020-11-24 16:57:34,393] [INFO] [logging.py:60:log_dist] [Rank 0] step=81400, skipped=81, lr=[0.00013108131023802254, 0.00013108131023802254], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 16:57:34,445] [INFO] [timer.py:157:stop] 0/45600, SamplesPerSec=56.39718108820776
Tencent01:  iteration    81400/  320000 | elapsed time per iteration (ms): 9108.2 | learning rate 1.311E-04 | lm loss 2.899355E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2360.67 | backward: 6626.08 | optimizer: 121.06 | batch generator: 5.72 | data loader: 1.01
Tencent01: [2020-11-24 17:01:10,595] [INFO] [timer.py:157:stop] 0/45700, SamplesPerSec=56.40315074685275
Tencent01: [2020-11-24 17:04:54,353] [INFO] [timer.py:157:stop] 0/45800, SamplesPerSec=56.40496030374333
Tencent01: [2020-11-24 17:09:19,921] [INFO] [timer.py:157:stop] 0/45900, SamplesPerSec=56.384114844014626
Tencent01: [2020-11-24 17:13:38,478] [INFO] [logging.py:60:log_dist] [Rank 0] step=81500, skipped=81, lr=[0.00013103527955182274, 0.00013103527955182274], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 17:13:38,530] [INFO] [timer.py:157:stop] 0/46000, SamplesPerSec=56.367140748992625
Tencent01:  iteration    81500/  320000 | elapsed time per iteration (ms): 9640.8 | learning rate 1.310E-04 | lm loss 2.942432E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2435.81 | backward: 7083.23 | optimizer: 121.43 | batch generator: 5.47 | data loader: 0.98
Tencent01: [2020-11-24 17:17:34,070] [INFO] [timer.py:157:stop] 0/46100, SamplesPerSec=56.3626709988947
Tencent01: [2020-11-24 17:21:10,409] [INFO] [timer.py:157:stop] 0/46200, SamplesPerSec=56.36854085418019
Tencent01: [2020-11-24 17:24:11,948] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 17:24:11,948] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 17:24:11,948] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 17:24:11,948] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 17:24:11,948] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 17:24:11,948] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 17:24:11,948] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 17:24:11,948] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 17:24:11,948] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 17:24:11,949] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 17:24:11,949] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 17:24:11,949] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 17:24:11,948] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 17:24:11,948] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 17:24:11,948] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 17:24:11,949] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 17:24:11,948] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 17:24:11,948] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 17:24:11,948] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 17:24:11,949] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 17:24:11,948] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 17:24:11,948] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 17:24:11,949] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 17:24:11,949] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 17:24:11,949] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 17:24:11,949] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 17:24:11,949] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 17:24:11,949] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 17:24:11,949] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 17:24:11,949] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 17:24:11,949] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 17:24:11,949] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 17:24:20,581] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 81571
Tencent01: [2020-11-24 17:24:20,581] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 17:24:20,581] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 81571
Tencent01: [2020-11-24 17:24:20,581] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
Tencent01: [2020-11-24 17:24:20,581] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 17:24:20,581] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 81571
Tencent01: [2020-11-24 17:24:20,581] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 17:24:20,581] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 81571
Tencent01: [2020-11-24 17:24:20,581] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 81571
Tencent01: [2020-11-24 17:24:20,581] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 17:24:20,581] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 81571
Tencent01: [2020-11-24 17:24:20,581] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 17:24:20,582] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 17:24:20,581] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 81571
Tencent01: [2020-11-24 17:24:20,581] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 81571
Tencent01: [2020-11-24 17:24:20,582] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 17:24:20,582] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 17:24:20,582] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 81571
Tencent02: [2020-11-24 17:24:20,582] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 81571
Tencent02: [2020-11-24 17:24:20,582] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 81571
Tencent02: [2020-11-24 17:24:20,582] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 81571
Tencent02: [2020-11-24 17:24:20,582] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 17:24:20,582] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 17:24:20,582] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 17:24:20,582] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 17:24:20,582] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 81571
Tencent02: [2020-11-24 17:24:20,582] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 81571
Tencent02: [2020-11-24 17:24:20,582] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 81571
Tencent02: [2020-11-24 17:24:20,582] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 17:24:20,582] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 17:24:20,582] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 17:24:20,582] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 81571
Tencent02: [2020-11-24 17:24:20,582] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 17:24:46,508] [INFO] [timer.py:157:stop] 0/46300, SamplesPerSec=56.37449924976285
Tencent01: [2020-11-24 17:28:22,635] [INFO] [logging.py:60:log_dist] [Rank 0] step=81600, skipped=82, lr=[0.00013098966309201083, 0.00013098966309201083], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 17:28:22,687] [INFO] [timer.py:157:stop] 0/46400, SamplesPerSec=56.38040538876457
Tencent01:  iteration    81600/  320000 | elapsed time per iteration (ms): 8841.6 | learning rate 1.310E-04 | lm loss 2.973231E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2358.89 | backward: 6361.40 | optimizer: 120.89 | batch generator: 5.59 | data loader: 1.03
Tencent01: [2020-11-24 17:31:58,817] [INFO] [timer.py:157:stop] 0/46500, SamplesPerSec=56.38629851849397
Tencent01: [2020-11-24 17:36:24,581] [INFO] [timer.py:157:stop] 0/46600, SamplesPerSec=56.36572014336289
Tencent01: [2020-11-24 17:40:41,147] [INFO] [timer.py:157:stop] 0/46700, SamplesPerSec=56.35014249410985
Tencent01: [2020-11-24 17:44:35,540] [INFO] [logging.py:60:log_dist] [Rank 0] step=81700, skipped=82, lr=[0.00013094353935798534, 0.00013094353935798534], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 17:44:35,593] [INFO] [timer.py:157:stop] 0/46800, SamplesPerSec=56.3463556154338
Tencent01:  iteration    81700/  320000 | elapsed time per iteration (ms): 9729.1 | learning rate 1.309E-04 | lm loss 2.967464E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2465.39 | backward: 7141.69 | optimizer: 121.61 | batch generator: 5.34 | data loader: 0.98
Tencent01: [2020-11-24 17:48:11,972] [INFO] [timer.py:157:stop] 0/46900, SamplesPerSec=56.352146467413895
Tencent01: [2020-11-24 17:51:48,203] [INFO] [timer.py:157:stop] 0/47000, SamplesPerSec=56.35798460778189
Tencent01: [2020-11-24 17:55:24,350] [INFO] [timer.py:157:stop] 0/47100, SamplesPerSec=56.363839614175056
Tencent01: [2020-11-24 17:59:00,475] [INFO] [logging.py:60:log_dist] [Rank 0] step=81800, skipped=82, lr=[0.00013089736893269707, 0.00013089736893269707], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 17:59:00,527] [INFO] [timer.py:157:stop] 0/47200, SamplesPerSec=56.36965914123396
Tencent01:  iteration    81800/  320000 | elapsed time per iteration (ms): 8649.3 | learning rate 1.309E-04 | lm loss 3.021357E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2357.55 | backward: 6170.34 | optimizer: 121.10 | batch generator: 5.42 | data loader: 0.97
Tencent01: [2020-11-24 18:03:04,904] [INFO] [timer.py:157:stop] 0/47300, SamplesPerSec=56.36065041590621
Tencent01: [2020-11-24 18:07:21,824] [INFO] [timer.py:157:stop] 0/47400, SamplesPerSec=56.3451268548966
Tencent01: [2020-11-24 18:11:26,729] [INFO] [timer.py:157:stop] 0/47500, SamplesPerSec=56.33593803453372
Tencent01: [2020-11-24 18:15:03,209] [INFO] [logging.py:60:log_dist] [Rank 0] step=81900, skipped=82, lr=[0.00013085115186064642, 0.00013085115186064642], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 18:15:03,261] [INFO] [timer.py:157:stop] 0/47600, SamplesPerSec=56.341588980887785
Tencent01:  iteration    81900/  320000 | elapsed time per iteration (ms): 9627.3 | learning rate 1.309E-04 | lm loss 2.956252E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2452.07 | backward: 7053.50 | optimizer: 121.40 | batch generator: 5.44 | data loader: 0.99
Tencent01: [2020-11-24 18:18:39,530] [INFO] [timer.py:157:stop] 0/47700, SamplesPerSec=56.347352957906864
Tencent01: [2020-11-24 18:22:15,682] [INFO] [timer.py:157:stop] 0/47800, SamplesPerSec=56.35314303640556
Tencent01: [2020-11-24 18:25:51,857] [INFO] [timer.py:157:stop] 0/47900, SamplesPerSec=56.358912931646366
Tencent01: [2020-11-24 18:29:49,208] [INFO] [logging.py:60:log_dist] [Rank 0] step=82000, skipped=82, lr=[0.00013080488818637868, 0.00013080488818637868], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 18:29:49,259] [INFO] [timer.py:157:stop] 0/48000, SamplesPerSec=56.35367441736695
Tencent01:  iteration    82000/  320000 | elapsed time per iteration (ms): 8860.0 | learning rate 1.308E-04 | lm loss 2.918922E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2357.59 | backward: 6380.19 | optimizer: 121.80 | batch generator: 5.85 | data loader: 1.06
Tencent01: [2020-11-24 18:29:49,263] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: /root/data/checkpoints/gpt-345M11-14-13-21/82000/mp_rank_00_model_states.pt
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: -----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 82000 | LM loss: 2.901404E+00 | LM PPL: 1.819968E+01
Tencent01: -----------------------------------------------------------------------------------
Tencent01: [2020-11-24 18:36:26,754] [INFO] [timer.py:157:stop] 0/48100, SamplesPerSec=56.33300640424808
Tencent01: [2020-11-24 18:40:31,972] [INFO] [timer.py:157:stop] 0/48200, SamplesPerSec=56.32383297473174
Tencent01: [2020-11-24 18:44:19,198] [INFO] [timer.py:157:stop] 0/48300, SamplesPerSec=56.32392841335515
Tencent01: [2020-11-24 18:47:55,313] [INFO] [logging.py:60:log_dist] [Rank 0] step=82100, skipped=82, lr=[0.00013075857795448413, 0.00013075857795448413], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 18:47:55,365] [INFO] [timer.py:157:stop] 0/48400, SamplesPerSec=56.32968413747603
Tencent01:  iteration    82100/  320000 | elapsed time per iteration (ms): 10861.1 | learning rate 1.308E-04 | lm loss 2.933010E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2358.59 | backward: 7079.05 | optimizer: 121.84 | batch generator: 6.97 | data loader: 1.22
Tencent01: [2020-11-24 18:51:31,492] [INFO] [timer.py:157:stop] 0/48500, SamplesPerSec=56.33544201449458
Tencent01: [2020-11-24 18:55:07,612] [INFO] [timer.py:157:stop] 0/48600, SamplesPerSec=56.34118400635402
Tencent01: [2020-11-24 18:58:54,736] [INFO] [timer.py:157:stop] 0/48700, SamplesPerSec=56.34129264513252
Tencent01: [2020-11-24 19:03:08,920] [INFO] [logging.py:60:log_dist] [Rank 0] step=82200, skipped=82, lr=[0.00013071222120959784, 0.00013071222120959784], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 19:03:08,972] [INFO] [timer.py:157:stop] 0/48800, SamplesPerSec=56.32763032680898
Tencent01:  iteration    82200/  320000 | elapsed time per iteration (ms): 9136.1 | learning rate 1.307E-04 | lm loss 2.979123E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2357.08 | backward: 6656.75 | optimizer: 121.83 | batch generator: 5.72 | data loader: 1.11
Tencent01: [2020-11-24 19:07:14,939] [INFO] [timer.py:157:stop] 0/48900, SamplesPerSec=56.31822074598185
Tencent01: [2020-11-24 19:11:10,475] [INFO] [timer.py:157:stop] 0/49000, SamplesPerSec=56.31412424070834
Tencent01: [2020-11-24 19:14:46,678] [INFO] [timer.py:157:stop] 0/49100, SamplesPerSec=56.31979874329402
Tencent01: [2020-11-24 19:18:22,827] [INFO] [logging.py:60:log_dist] [Rank 0] step=82300, skipped=82, lr=[0.0001306658179963998, 0.0001306658179963998], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 19:18:22,879] [INFO] [timer.py:157:stop] 0/49200, SamplesPerSec=56.32545382929056
Tencent01:  iteration    82300/  320000 | elapsed time per iteration (ms): 9139.1 | learning rate 1.307E-04 | lm loss 2.918470E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2359.34 | backward: 6657.44 | optimizer: 121.89 | batch generator: 5.62 | data loader: 1.12
Tencent01: [2020-11-24 19:21:59,036] [INFO] [timer.py:157:stop] 0/49300, SamplesPerSec=56.33111797487562
Tencent01: [2020-11-24 19:25:45,639] [INFO] [timer.py:157:stop] 0/49400, SamplesPerSec=56.331516911434626
Tencent01: [2020-11-24 19:29:49,799] [INFO] [timer.py:157:stop] 0/49500, SamplesPerSec=56.323118617946875
Tencent01: [2020-11-24 19:34:04,306] [INFO] [logging.py:60:log_dist] [Rank 0] step=82400, skipped=82, lr=[0.00013061936835961473, 0.00013061936835961473], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 19:34:04,358] [INFO] [timer.py:157:stop] 0/49600, SamplesPerSec=56.30956224574834
Tencent01:  iteration    82400/  320000 | elapsed time per iteration (ms): 9414.8 | learning rate 1.306E-04 | lm loss 2.972798E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2465.25 | backward: 6827.20 | optimizer: 121.91 | batch generator: 6.06 | data loader: 1.17
Tencent01: [2020-11-24 19:37:59,551] [INFO] [timer.py:157:stop] 0/49700, SamplesPerSec=56.30571277636327
Tencent01: [2020-11-24 19:41:35,830] [INFO] [timer.py:157:stop] 0/49800, SamplesPerSec=56.311281704411876
Tencent01: [2020-11-24 19:45:11,997] [INFO] [timer.py:157:stop] 0/49900, SamplesPerSec=56.31689145623379
Tencent01: [2020-11-24 19:48:48,112] [INFO] [logging.py:60:log_dist] [Rank 0] step=82500, skipped=82, lr=[0.00013057287234401215, 0.00013057287234401215], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 19:48:48,164] [INFO] [timer.py:157:stop] 0/50000, SamplesPerSec=56.32248699896146
Tencent01:  iteration    82500/  320000 | elapsed time per iteration (ms): 8838.1 | learning rate 1.306E-04 | lm loss 2.969345E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2359.10 | backward: 6356.96 | optimizer: 121.61 | batch generator: 5.69 | data loader: 1.10
Tencent01: [2020-11-24 19:52:24,368] [INFO] [timer.py:157:stop] 0/50100, SamplesPerSec=56.32803911916371
Tencent01: [2020-11-24 19:56:30,180] [INFO] [timer.py:157:stop] 0/50200, SamplesPerSec=56.31895139132133
Tencent01: [2020-11-24 20:00:26,468] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 20:00:26,468] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 20:00:26,468] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 20:00:26,468] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 20:00:26,468] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 20:00:26,468] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 20:00:26,469] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 20:00:26,469] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 20:00:26,469] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 20:00:26,469] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 20:00:26,469] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 20:00:26,469] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 20:00:26,469] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 20:00:26,469] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 20:00:26,469] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 20:00:26,469] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 20:00:26,468] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 20:00:26,469] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 20:00:26,468] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 20:00:26,468] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 20:00:26,469] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 20:00:26,469] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 20:00:26,469] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 20:00:26,470] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 20:00:26,470] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 20:00:26,470] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 20:00:26,470] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 20:00:26,470] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 20:00:26,469] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 20:00:26,470] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 20:00:26,470] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 20:00:26,470] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 20:00:43,854] [INFO] [timer.py:157:stop] 0/50300, SamplesPerSec=56.30603294187656
Tencent01: [2020-11-24 20:00:52,413] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 82575
Tencent01: [2020-11-24 20:00:52,413] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 20:00:52,413] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
Tencent01: [2020-11-24 20:00:52,413] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 82575
Tencent01: [2020-11-24 20:00:52,413] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 20:00:52,413] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 82575
Tencent01: [2020-11-24 20:00:52,414] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 20:00:52,414] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 82575
Tencent01: [2020-11-24 20:00:52,414] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 82575
Tencent01: [2020-11-24 20:00:52,414] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 82575
Tencent01: [2020-11-24 20:00:52,414] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 20:00:52,414] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 20:00:52,414] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 20:00:52,414] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 82575
Tencent01: [2020-11-24 20:00:52,414] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 20:00:52,414] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 82575
Tencent02: [2020-11-24 20:00:52,414] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 20:00:52,414] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 82575
Tencent02: [2020-11-24 20:00:52,414] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 20:00:52,414] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 82575
Tencent02: [2020-11-24 20:00:52,414] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 20:00:52,414] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 82575
Tencent01: [2020-11-24 20:00:52,414] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 82575
Tencent02: [2020-11-24 20:00:52,414] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 82575
Tencent01: [2020-11-24 20:00:52,414] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 20:00:52,414] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 82575
Tencent02: [2020-11-24 20:00:52,414] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 82575
Tencent02: [2020-11-24 20:00:52,414] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 20:00:52,414] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 82575
Tencent02: [2020-11-24 20:00:52,414] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 20:00:52,414] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 20:00:52,414] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 20:00:52,414] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 20:04:48,801] [INFO] [logging.py:60:log_dist] [Rank 0] step=82600, skipped=83, lr=[0.00013052679564710831, 0.00013052679564710831], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 20:04:48,854] [INFO] [timer.py:157:stop] 0/50400, SamplesPerSec=56.297431127808814
Tencent01:  iteration    82600/  320000 | elapsed time per iteration (ms): 9606.9 | learning rate 1.305E-04 | lm loss 2.916404E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2452.90 | backward: 7033.38 | optimizer: 120.25 | batch generator: 6.09 | data loader: 1.06
Tencent01: [2020-11-24 20:08:25,311] [INFO] [timer.py:157:stop] 0/50500, SamplesPerSec=56.30285965724665
Tencent01: [2020-11-24 20:12:01,538] [INFO] [timer.py:157:stop] 0/50600, SamplesPerSec=56.30838088226056
Tencent01: [2020-11-24 20:15:37,802] [INFO] [timer.py:157:stop] 0/50700, SamplesPerSec=56.31385747519384
Tencent01: [2020-11-24 20:19:13,916] [INFO] [logging.py:60:log_dist] [Rank 0] step=82700, skipped=83, lr=[0.0001304802074710271, 0.0001304802074710271], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 20:19:13,968] [INFO] [timer.py:157:stop] 0/50800, SamplesPerSec=56.31936441289715
Tencent01:  iteration    82700/  320000 | elapsed time per iteration (ms): 8651.1 | learning rate 1.305E-04 | lm loss 2.975582E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2358.49 | backward: 6170.56 | optimizer: 121.69 | batch generator: 5.81 | data loader: 1.11
Tencent01: [2020-11-24 20:23:07,014] [INFO] [timer.py:157:stop] 0/50900, SamplesPerSec=56.316641188131015
Tencent01: [2020-11-24 20:27:21,480] [INFO] [timer.py:157:stop] 0/51000, SamplesPerSec=56.30351793474139
Tencent01: [2020-11-24 20:31:27,749] [INFO] [timer.py:157:stop] 0/51100, SamplesPerSec=56.29442413931814
Tencent01: [2020-11-24 20:35:14,349] [INFO] [logging.py:60:log_dist] [Rank 0] step=82800, skipped=83, lr=[0.00013043357305025555, 0.00013043357305025555], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 20:35:14,401] [INFO] [timer.py:157:stop] 0/51200, SamplesPerSec=56.29484376890795
Tencent01:  iteration    82800/  320000 | elapsed time per iteration (ms): 9604.3 | learning rate 1.304E-04 | lm loss 2.989508E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2437.69 | backward: 7044.44 | optimizer: 121.79 | batch generator: 5.90 | data loader: 1.13
Tencent01: [2020-11-24 20:38:50,592] [INFO] [timer.py:157:stop] 0/51300, SamplesPerSec=56.300308326185316
Tencent01: [2020-11-24 20:42:26,772] [INFO] [timer.py:157:stop] 0/51400, SamplesPerSec=56.30576402199322
Tencent01: [2020-11-24 20:46:02,929] [INFO] [timer.py:157:stop] 0/51500, SamplesPerSec=56.31122466255264
Tencent01: [2020-11-24 20:49:49,486] [INFO] [logging.py:60:log_dist] [Rank 0] step=82900, skipped=83, lr=[0.00013038689242974132, 0.00013038689242974132], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 20:49:49,538] [INFO] [timer.py:157:stop] 0/51600, SamplesPerSec=56.31163483596652
Tencent01:  iteration    82900/  320000 | elapsed time per iteration (ms): 8751.4 | learning rate 1.304E-04 | lm loss 2.938474E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2464.13 | backward: 6164.92 | optimizer: 121.92 | batch generator: 5.65 | data loader: 1.11
Tencent01: [2020-11-24 20:54:23,005] [INFO] [timer.py:157:stop] 0/51700, SamplesPerSec=56.28961767378853
Tencent01: [2020-11-24 20:58:28,616] [INFO] [timer.py:157:stop] 0/51800, SamplesPerSec=56.280994416234535
Tencent01: [2020-11-24 21:02:14,381] [INFO] [timer.py:157:stop] 0/51900, SamplesPerSec=56.2818721263107
Tencent01: [2020-11-24 21:05:50,666] [INFO] [logging.py:60:log_dist] [Rank 0] step=83000, skipped=83, lr=[0.00013034016565447644, 0.00013034016565447644], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 21:05:50,717] [INFO] [timer.py:157:stop] 0/52000, SamplesPerSec=56.28722893708072
Tencent01:  iteration    83000/  320000 | elapsed time per iteration (ms): 9611.8 | learning rate 1.303E-04 | lm loss 2.965538E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2361.10 | backward: 7128.75 | optimizer: 121.54 | batch generator: 6.46 | data loader: 1.17
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: -----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 83000 | LM loss: 2.819259E+00 | LM PPL: 1.676443E+01
Tencent01: -----------------------------------------------------------------------------------
Tencent01: [2020-11-24 21:10:23,159] [INFO] [timer.py:157:stop] 0/52100, SamplesPerSec=56.29203029591836
Tencent01: [2020-11-24 21:13:59,298] [INFO] [timer.py:157:stop] 0/52200, SamplesPerSec=56.29743680686879
Tencent01: [2020-11-24 21:17:45,903] [INFO] [timer.py:157:stop] 0/52300, SamplesPerSec=56.2978755452178
Tencent01: [2020-11-24 21:22:00,440] [INFO] [logging.py:60:log_dist] [Rank 0] step=83100, skipped=83, lr=[0.00013029339276949756, 0.00013029339276949756], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 21:22:00,492] [INFO] [timer.py:157:stop] 0/52400, SamplesPerSec=56.285092196472405
Tencent01:  iteration    83100/  320000 | elapsed time per iteration (ms): 9697.7 | learning rate 1.303E-04 | lm loss 2.967905E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2464.29 | backward: 6561.52 | optimizer: 121.80 | batch generator: 7.10 | data loader: 1.30
Tencent01: [2020-11-24 21:26:17,500] [INFO] [timer.py:157:stop] 0/52500, SamplesPerSec=56.27122327744554
Tencent01: [2020-11-24 21:30:14,834] [INFO] [timer.py:157:stop] 0/52600, SamplesPerSec=56.26666002005189
Tencent01: [2020-11-24 21:33:51,228] [INFO] [timer.py:157:stop] 0/52700, SamplesPerSec=56.27194458902622
Tencent01: [2020-11-24 21:37:27,294] [INFO] [logging.py:60:log_dist] [Rank 0] step=83200, skipped=83, lr=[0.00013024657381988566, 0.00013024657381988566], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 21:37:27,345] [INFO] [timer.py:157:stop] 0/52800, SamplesPerSec=56.27733176966137
Tencent01:  iteration    83200/  320000 | elapsed time per iteration (ms): 9268.5 | learning rate 1.302E-04 | lm loss 2.949716E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2360.29 | backward: 6786.26 | optimizer: 121.59 | batch generator: 5.86 | data loader: 1.08
Tencent01: [2020-11-24 21:41:03,572] [INFO] [timer.py:157:stop] 0/52900, SamplesPerSec=56.28264944518545
Tencent01: [2020-11-24 21:44:39,766] [INFO] [timer.py:157:stop] 0/53000, SamplesPerSec=56.28796027481174
Tencent01: [2020-11-24 21:48:52,736] [INFO] [timer.py:157:stop] 0/53100, SamplesPerSec=56.27611424699032
Tencent01: [2020-11-24 21:53:17,944] [INFO] [logging.py:60:log_dist] [Rank 0] step=83300, skipped=83, lr=[0.00013019970885076626, 0.00013019970885076626], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 21:53:17,997] [INFO] [timer.py:157:stop] 0/53200, SamplesPerSec=56.25861357271631
Tencent01:  iteration    83300/  320000 | elapsed time per iteration (ms): 9506.5 | learning rate 1.302E-04 | lm loss 2.978457E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2357.53 | backward: 7026.40 | optimizer: 122.18 | batch generator: 5.56 | data loader: 1.10
Tencent01: [2020-11-24 21:57:12,903] [INFO] [timer.py:157:stop] 0/53300, SamplesPerSec=56.255268178658625
Tencent01: [2020-11-24 22:00:49,255] [INFO] [timer.py:157:stop] 0/53400, SamplesPerSec=56.26051726637847
Tencent01: [2020-11-24 22:04:25,476] [INFO] [timer.py:157:stop] 0/53500, SamplesPerSec=56.26581735196004
Tencent01: [2020-11-24 22:08:01,606] [INFO] [logging.py:60:log_dist] [Rank 0] step=83400, skipped=83, lr=[0.00013015279790730907, 0.00013015279790730907], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 22:08:01,657] [INFO] [timer.py:157:stop] 0/53600, SamplesPerSec=56.271117536643224
Tencent01:  iteration    83400/  320000 | elapsed time per iteration (ms): 8836.6 | learning rate 1.302E-04 | lm loss 2.923875E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2359.47 | backward: 6354.89 | optimizer: 121.83 | batch generator: 6.14 | data loader: 1.19
Tencent01: [2020-11-24 22:11:37,888] [INFO] [timer.py:157:stop] 0/53700, SamplesPerSec=56.276371214344316
Tencent01: [2020-11-24 22:15:41,039] [INFO] [timer.py:157:stop] 0/53800, SamplesPerSec=56.2692304791238
Tencent01: [2020-11-24 22:20:05,431] [INFO] [timer.py:157:stop] 0/53900, SamplesPerSec=56.252373262609176
Tencent01: [2020-11-24 22:24:00,643] [INFO] [logging.py:60:log_dist] [Rank 0] step=83500, skipped=83, lr=[0.00013010584103472823, 0.00013010584103472823], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 22:24:00,695] [INFO] [timer.py:157:stop] 0/54000, SamplesPerSec=56.24891844974302
Tencent01:  iteration    83500/  320000 | elapsed time per iteration (ms): 9590.4 | learning rate 1.301E-04 | lm loss 2.957563E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2437.43 | backward: 7030.96 | optimizer: 121.59 | batch generator: 6.24 | data loader: 1.09
Tencent01: [2020-11-24 22:27:45,124] [INFO] [timer.py:157:stop] 0/54100, SamplesPerSec=56.25042776280963
Tencent01: [2020-11-24 22:31:21,405] [INFO] [timer.py:157:stop] 0/54200, SamplesPerSec=56.25564928181052
Tencent01: [2020-11-24 22:34:57,529] [INFO] [timer.py:157:stop] 0/54300, SamplesPerSec=56.26092750881636
Tencent01: [2020-11-24 22:35:14,747] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 22:35:14,747] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 22:35:14,748] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 22:35:14,749] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 22:35:14,749] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 22:35:14,749] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 22:35:14,748] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 22:35:14,749] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 22:35:14,749] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 22:35:14,749] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 22:35:14,749] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 22:35:14,749] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 22:35:14,749] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 22:35:14,748] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 22:35:14,748] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 22:35:14,748] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 22:35:14,748] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 22:35:14,748] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 22:35:14,748] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 22:35:14,748] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 22:35:14,749] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 22:35:14,749] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 22:35:14,749] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 22:35:14,749] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 22:35:14,749] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 22:35:14,749] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-24 22:35:14,749] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-24 22:35:14,750] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 22:35:14,749] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 22:35:14,749] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 22:35:14,749] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-24 22:35:14,749] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-24 22:38:33,702] [INFO] [logging.py:60:log_dist] [Rank 0] step=83600, skipped=83, lr=[0.00013005883827828214, 0.00013005883827828214], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 22:38:33,755] [INFO] [timer.py:157:stop] 0/54400, SamplesPerSec=56.26613867697788
Tencent01:  iteration    83600/  320000 | elapsed time per iteration (ms): 8730.6 | learning rate 1.301E-04 | lm loss 2.962613E+00 | loss scale 524288.0 |
Tencent01: time (ms) | forward: 2359.61 | backward: 6249.14 | optimizer: 121.47 | batch generator: 6.49 | data loader: 1.13
Tencent01: [2020-11-24 22:38:42,294] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 83600
Tencent01: [2020-11-24 22:38:42,294] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 83600
Tencent01: [2020-11-24 22:38:42,295] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 22:38:42,295] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 22:38:42,295] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 83600
Tencent01: [2020-11-24 22:38:42,295] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 22:38:42,295] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 83600
Tencent01: [2020-11-24 22:38:42,295] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 83600
Tencent01: [2020-11-24 22:38:42,295] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 83600
Tencent01: [2020-11-24 22:38:42,295] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 22:38:42,295] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 22:38:42,295] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 22:38:42,295] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 83600
Tencent02: [2020-11-24 22:38:42,295] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 83600
Tencent02: [2020-11-24 22:38:42,295] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 83600
Tencent02: [2020-11-24 22:38:42,295] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 83600
Tencent01: [2020-11-24 22:38:42,295] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 22:38:42,295] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 83600
Tencent02: [2020-11-24 22:38:42,295] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 22:38:42,295] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 22:38:42,295] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 22:38:42,295] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 83600
Tencent02: [2020-11-24 22:38:42,295] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 22:38:42,295] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 22:38:42,295] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 83600
Tencent02: [2020-11-24 22:38:42,295] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 83600
Tencent01: [2020-11-24 22:38:42,295] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
Tencent02: [2020-11-24 22:38:42,296] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 22:38:42,296] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-24 22:38:42,295] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 83600
Tencent02: [2020-11-24 22:38:42,296] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 22:38:42,295] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 83600
Tencent01: [2020-11-24 22:38:42,295] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-24 22:42:27,007] [INFO] [timer.py:157:stop] 0/54500, SamplesPerSec=56.26361228488452
Tencent01: [2020-11-24 22:46:43,304] [INFO] [timer.py:157:stop] 0/54600, SamplesPerSec=56.25065761774786
Tencent01: [2020-11-24 22:50:56,255] [INFO] [timer.py:157:stop] 0/54700, SamplesPerSec=56.239259343894275
Tencent01: [2020-11-24 22:54:41,082] [INFO] [logging.py:60:log_dist] [Rank 0] step=83700, skipped=84, lr=[0.00013001226039597547, 0.00013001226039597547], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 22:54:41,134] [INFO] [timer.py:157:stop] 0/54800, SamplesPerSec=56.24056348459861
Tencent01:  iteration    83700/  320000 | elapsed time per iteration (ms): 9673.8 | learning rate 1.300E-04 | lm loss 2.959742E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2437.84 | backward: 7114.99 | optimizer: 120.56 | batch generator: 6.70 | data loader: 1.18
Tencent01: [2020-11-24 22:58:17,380] [INFO] [timer.py:157:stop] 0/54900, SamplesPerSec=56.24574196465118
Tencent01: [2020-11-24 23:01:53,540] [INFO] [timer.py:157:stop] 0/55000, SamplesPerSec=56.25094654835672
Tencent01: [2020-11-24 23:05:29,704] [INFO] [timer.py:157:stop] 0/55100, SamplesPerSec=56.256131766296704
Tencent01: [2020-11-24 23:05:38,258] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 83775
Tencent01: [2020-11-24 23:05:38,259] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-24 23:05:38,259] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Tencent01: [2020-11-24 23:05:38,259] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 83775
Tencent01: [2020-11-24 23:05:38,259] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-24 23:05:38,259] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 83775
Tencent01: [2020-11-24 23:05:38,260] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-24 23:05:38,260] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 83775
Tencent01: [2020-11-24 23:05:38,260] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-24 23:05:38,260] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 83775
Tencent01: [2020-11-24 23:05:38,260] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 83775
Tencent01: [2020-11-24 23:05:38,260] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-24 23:05:38,260] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-24 23:05:38,260] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 83775
Tencent01: [2020-11-24 23:05:38,260] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-24 23:05:38,260] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 83775
Tencent01: [2020-11-24 23:05:38,260] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-24 23:05:38,262] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 83775
Tencent02: [2020-11-24 23:05:38,262] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 83775
Tencent02: [2020-11-24 23:05:38,262] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 83775
Tencent02: [2020-11-24 23:05:38,262] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 83775
Tencent02: [2020-11-24 23:05:38,262] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 83775
Tencent02: [2020-11-24 23:05:38,262] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 83775
Tencent02: [2020-11-24 23:05:38,262] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 83775
Tencent02: [2020-11-24 23:05:38,262] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 83775
Tencent02: [2020-11-24 23:05:38,270] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-24 23:05:38,270] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-24 23:05:38,270] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-24 23:05:38,270] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-24 23:05:38,270] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-24 23:05:38,270] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-24 23:05:38,270] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent02: [2020-11-24 23:05:38,270] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Tencent01: [2020-11-24 23:09:24,309] [INFO] [logging.py:60:log_dist] [Rank 0] step=83800, skipped=85, lr=[0.00012996563763129318, 0.00012996563763129318], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 23:09:24,361] [INFO] [timer.py:157:stop] 0/55200, SamplesPerSec=56.253008876061664
Tencent01:  iteration    83800/  320000 | elapsed time per iteration (ms): 8832.3 | learning rate 1.300E-04 | lm loss 2.948825E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2464.19 | backward: 6246.56 | optimizer: 121.10 | batch generator: 5.86 | data loader: 1.16
Tencent01: [2020-11-24 23:13:31,964] [INFO] [timer.py:157:stop] 0/55300, SamplesPerSec=56.24411828975422
Tencent01: [2020-11-24 23:17:34,090] [INFO] [timer.py:157:stop] 0/55400, SamplesPerSec=56.237700346799535
Tencent01: [2020-11-24 23:21:28,935] [INFO] [timer.py:157:stop] 0/55500, SamplesPerSec=56.234554645642135
Tencent01: [2020-11-24 23:25:05,098] [INFO] [logging.py:60:log_dist] [Rank 0] step=83900, skipped=85, lr=[0.00012991849840975485, 0.00012991849840975485], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 23:25:05,150] [INFO] [timer.py:157:stop] 0/55600, SamplesPerSec=56.239691953854184
Tencent01:  iteration    83900/  320000 | elapsed time per iteration (ms): 9407.9 | learning rate 1.299E-04 | lm loss 2.936168E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2359.96 | backward: 6925.51 | optimizer: 121.99 | batch generator: 5.99 | data loader: 1.15
Tencent01: [2020-11-24 23:28:41,236] [INFO] [timer.py:157:stop] 0/55700, SamplesPerSec=56.24487325483824
Tencent01: [2020-11-24 23:32:17,379] [INFO] [timer.py:157:stop] 0/55800, SamplesPerSec=56.25002638361742
Tencent01: [2020-11-24 23:36:03,073] [INFO] [timer.py:157:stop] 0/55900, SamplesPerSec=56.25092212566002
Tencent01: [2020-11-24 23:40:29,908] [INFO] [logging.py:60:log_dist] [Rank 0] step=84000, skipped=85, lr=[0.00012987131348491743, 0.00012987131348491743], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 23:40:29,961] [INFO] [timer.py:157:stop] 0/56000, SamplesPerSec=56.233639877199636
Tencent01:  iteration    84000/  320000 | elapsed time per iteration (ms): 9248.1 | learning rate 1.299E-04 | lm loss 2.919630E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2358.66 | backward: 6767.89 | optimizer: 121.19 | batch generator: 6.49 | data loader: 1.12
Tencent01: [2020-11-24 23:40:29,964] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: /root/data/checkpoints/gpt-345M11-14-13-21/84000/mp_rank_00_model_states.pt
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: -----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 84000 | LM loss: 2.874561E+00 | LM PPL: 1.771764E+01
Tencent01: -----------------------------------------------------------------------------------
Tencent01: [2020-11-24 23:46:53,762] [INFO] [timer.py:157:stop] 0/56100, SamplesPerSec=56.22517260910277
Tencent01: [2020-11-24 23:50:50,123] [INFO] [timer.py:157:stop] 0/56200, SamplesPerSec=56.22141052206248
Tencent01: [2020-11-24 23:54:26,373] [INFO] [timer.py:157:stop] 0/56300, SamplesPerSec=56.22649811846975
Tencent01: [2020-11-24 23:58:02,467] [INFO] [logging.py:60:log_dist] [Rank 0] step=84100, skipped=85, lr=[0.00012982408290225907, 0.00012982408290225907], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-24 23:58:02,519] [INFO] [timer.py:157:stop] 0/56400, SamplesPerSec=56.23161415417701
Tencent01:  iteration    84100/  320000 | elapsed time per iteration (ms): 10525.6 | learning rate 1.298E-04 | lm loss 3.006190E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2358.88 | backward: 6677.11 | optimizer: 121.37 | batch generator: 7.45 | data loader: 1.34
Tencent01: [2020-11-25 00:01:38,576] [INFO] [timer.py:157:stop] 0/56500, SamplesPerSec=56.23674819413909
Tencent01: [2020-11-25 00:05:25,148] [INFO] [timer.py:157:stop] 0/56600, SamplesPerSec=56.237260848700224
Tencent01: [2020-11-25 00:09:29,842] [INFO] [timer.py:157:stop] 0/56700, SamplesPerSec=56.22988449696633
Tencent01: [2020-11-25 00:13:56,848] [INFO] [logging.py:60:log_dist] [Rank 0] step=84200, skipped=85, lr=[0.0001297768067073019, 0.0001297768067073019], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 00:13:56,900] [INFO] [timer.py:157:stop] 0/56800, SamplesPerSec=56.21281557614523
Tencent01:  iteration    84200/  320000 | elapsed time per iteration (ms): 9543.8 | learning rate 1.298E-04 | lm loss 2.976743E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2464.94 | backward: 6956.57 | optimizer: 121.89 | batch generator: 5.68 | data loader: 1.10
Tencent01: [2020-11-25 00:17:43,253] [INFO] [timer.py:157:stop] 0/56900, SamplesPerSec=56.213466307811494
Tencent01: [2020-11-25 00:21:29,839] [INFO] [timer.py:157:stop] 0/57000, SamplesPerSec=56.21402313993746
Tencent01: [2020-11-25 00:25:05,982] [INFO] [timer.py:157:stop] 0/57100, SamplesPerSec=56.219096756201246
Tencent01: [2020-11-25 00:28:42,043] [INFO] [logging.py:60:log_dist] [Rank 0] step=84300, skipped=85, lr=[0.00012972948494561213, 0.00012972948494561213], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 00:28:42,096] [INFO] [timer.py:157:stop] 0/57200, SamplesPerSec=56.22416491677218
Tencent01:  iteration    84300/  320000 | elapsed time per iteration (ms): 8852.0 | learning rate 1.297E-04 | lm loss 2.905879E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2360.34 | backward: 6370.16 | optimizer: 121.11 | batch generator: 6.10 | data loader: 1.02
Tencent01: [2020-11-25 00:32:18,175] [INFO] [timer.py:157:stop] 0/57300, SamplesPerSec=56.229238778532775
Tencent01: [2020-11-25 00:36:22,158] [INFO] [timer.py:157:stop] 0/57400, SamplesPerSec=56.222287011363946
Tencent01: [2020-11-25 00:40:27,070] [INFO] [timer.py:157:stop] 0/57500, SamplesPerSec=56.2149527701796
Tencent01: [2020-11-25 00:44:31,565] [INFO] [logging.py:60:log_dist] [Rank 0] step=84400, skipped=85, lr=[0.0001296821176627998, 0.0001296821176627998], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 00:44:31,616] [INFO] [timer.py:157:stop] 0/57600, SamplesPerSec=56.207796580380155
Tencent01:  iteration    84400/  320000 | elapsed time per iteration (ms): 9495.2 | learning rate 1.297E-04 | lm loss 2.891843E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2455.39 | backward: 6917.76 | optimizer: 121.65 | batch generator: 6.35 | data loader: 1.17
Tencent01: [2020-11-25 00:48:17,703] [INFO] [timer.py:157:stop] 0/57700, SamplesPerSec=56.20856744297618
Tencent01: [2020-11-25 00:51:53,885] [INFO] [timer.py:157:stop] 0/57800, SamplesPerSec=56.21356433143099
Tencent01: [2020-11-25 00:55:30,025] [INFO] [timer.py:157:stop] 0/57900, SamplesPerSec=56.218566407530346
Tencent01: [2020-11-25 00:59:06,037] [INFO] [logging.py:60:log_dist] [Rank 0] step=84500, skipped=85, lr=[0.00012963470490451884, 0.00012963470490451884], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 00:59:06,089] [INFO] [timer.py:157:stop] 0/58000, SamplesPerSec=56.22357983305127
Tencent01:  iteration    84500/  320000 | elapsed time per iteration (ms): 8744.7 | learning rate 1.296E-04 | lm loss 2.939290E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2358.60 | backward: 6264.01 | optimizer: 121.71 | batch generator: 5.86 | data loader: 1.13
Tencent01: [2020-11-25 01:03:01,237] [INFO] [timer.py:157:stop] 0/58100, SamplesPerSec=56.220471006113875
Tencent01: [2020-11-25 01:07:23,461] [INFO] [timer.py:157:stop] 0/58200, SamplesPerSec=56.20588817273792
Tencent01: [2020-11-25 01:11:35,431] [INFO] [timer.py:157:stop] 0/58300, SamplesPerSec=56.19570264598679
Tencent01: [2020-11-25 01:15:22,129] [INFO] [logging.py:60:log_dist] [Rank 0] step=84600, skipped=85, lr=[0.00012958724671646704, 0.00012958724671646704], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 01:15:22,180] [INFO] [timer.py:157:stop] 0/58400, SamplesPerSec=56.1962049083035
Tencent01:  iteration    84600/  320000 | elapsed time per iteration (ms): 9760.9 | learning rate 1.296E-04 | lm loss 2.938080E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2466.14 | backward: 7172.33 | optimizer: 122.03 | batch generator: 6.17 | data loader: 1.18
Tencent01: [2020-11-25 01:18:58,415] [INFO] [timer.py:157:stop] 0/58500, SamplesPerSec=56.20114691100447
Tencent01: [2020-11-25 01:22:34,487] [INFO] [timer.py:157:stop] 0/58600, SamplesPerSec=56.206133774706224
Tencent01: [2020-11-25 01:26:10,577] [INFO] [timer.py:157:stop] 0/58700, SamplesPerSec=56.21110123354594
Tencent01: [2020-11-25 01:30:04,916] [INFO] [logging.py:60:log_dist] [Rank 0] step=84700, skipped=85, lr=[0.00012953974314438592, 0.00012953974314438592], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 01:30:04,968] [INFO] [timer.py:157:stop] 0/58800, SamplesPerSec=56.20836436806402
Tencent01:  iteration    84700/  320000 | elapsed time per iteration (ms): 8827.9 | learning rate 1.295E-04 | lm loss 2.952380E+00 | loss scale 131072.0 |
Tencent01: time (ms) | forward: 2457.90 | backward: 6247.70 | optimizer: 121.87 | batch generator: 6.06 | data loader: 1.18
Tencent01: [2020-11-25 01:34:16,882] [INFO] [timer.py:157:stop] 0/58900, SamplesPerSec=56.1983017959106
Tencent01: [2020-11-25 01:38:20,709] [INFO] [timer.py:157:stop] 0/59000, SamplesPerSec=56.19164329797954
Tencent01: [2020-11-25 01:42:15,350] [INFO] [timer.py:157:stop] 0/59100, SamplesPerSec=56.18884286482319
Tencent02: [2020-11-25 01:42:32,562] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 01:42:32,562] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-25 01:42:32,562] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 01:42:32,562] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-25 01:42:32,562] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 01:42:32,562] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-25 01:42:32,562] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 01:42:32,562] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-25 01:42:32,562] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 01:42:32,563] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-25 01:42:32,562] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 01:42:32,562] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 01:42:32,563] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 01:42:32,563] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-25 01:42:32,563] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-25 01:42:32,562] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 01:42:32,563] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-25 01:42:32,562] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-25 01:42:32,563] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 01:42:32,562] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 01:42:32,563] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-25 01:42:32,563] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 01:42:32,563] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-25 01:42:32,562] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-25 01:42:32,562] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 01:42:32,562] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-25 01:42:32,563] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 01:42:32,562] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 01:42:32,563] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-25 01:42:32,563] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent02: [2020-11-25 01:42:32,563] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 01:42:32,563] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Tencent01: [2020-11-25 01:45:51,602] [INFO] [logging.py:60:log_dist] [Rank 0] step=84800, skipped=85, lr=[0.00012949219423406076, 0.00012949219423406076], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 01:45:51,653] [INFO] [timer.py:157:stop] 0/59200, SamplesPerSec=56.19370264900121
Tencent01:  iteration    84800/  320000 | elapsed time per iteration (ms): 9466.9 | learning rate 1.295E-04 | lm loss 2.977762E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2359.56 | backward: 6985.01 | optimizer: 121.88 | batch generator: 5.58 | data loader: 1.11
Tencent01: [2020-11-25 01:49:27,882] [INFO] [timer.py:157:stop] 0/59300, SamplesPerSec=56.19857711269323
Tencent01: [2020-11-25 01:53:04,013] [INFO] [timer.py:157:stop] 0/59400, SamplesPerSec=56.203484523889216
Tencent01: [2020-11-25 01:56:50,704] [INFO] [timer.py:157:stop] 0/59500, SamplesPerSec=56.20399242071124
Tencent01: [2020-11-25 02:01:07,565] [INFO] [logging.py:60:log_dist] [Rank 0] step=84900, skipped=85, lr=[0.00012944460003132063, 0.00012944460003132063], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 02:01:07,617] [INFO] [timer.py:157:stop] 0/59600, SamplesPerSec=56.19198239062063
Tencent01:  iteration    84900/  320000 | elapsed time per iteration (ms): 9159.6 | learning rate 1.294E-04 | lm loss 2.953936E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2465.62 | backward: 6571.77 | optimizer: 121.83 | batch generator: 6.14 | data loader: 1.14
Tencent01: [2020-11-25 02:05:02,553] [INFO] [timer.py:157:stop] 0/59700, SamplesPerSec=56.18910176918416
Tencent01: [2020-11-25 02:08:57,723] [INFO] [timer.py:157:stop] 0/59800, SamplesPerSec=56.18613914674211
Tencent01: [2020-11-25 02:12:34,030] [INFO] [timer.py:157:stop] 0/59900, SamplesPerSec=56.19096242962994
Tencent01: [2020-11-25 02:16:10,116] [INFO] [logging.py:60:log_dist] [Rank 0] step=85000, skipped=85, lr=[0.0001293969605820381, 0.0001293969605820381], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 02:16:10,168] [INFO] [timer.py:157:stop] 0/60000, SamplesPerSec=56.195825669645615
Tencent01:  iteration    85000/  320000 | elapsed time per iteration (ms): 9025.5 | learning rate 1.294E-04 | lm loss 2.928352E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2361.51 | backward: 6542.44 | optimizer: 121.19 | batch generator: 6.79 | data loader: 1.15
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: -----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 85000 | LM loss: 2.916443E+00 | LM PPL: 1.847544E+01
Tencent01: -----------------------------------------------------------------------------------
Tencent01: [2020-11-25 02:20:42,569] [INFO] [timer.py:157:stop] 0/60100, SamplesPerSec=56.20013037600137
Tencent01: [2020-11-25 02:24:18,761] [INFO] [timer.py:157:stop] 0/60200, SamplesPerSec=56.20494763243181
Tencent01: [2020-11-25 02:28:24,990] [INFO] [timer.py:157:stop] 0/60300, SamplesPerSec=56.197448562491005
Tencent01: [2020-11-25 02:32:38,386] [INFO] [logging.py:60:log_dist] [Rank 0] step=85100, skipped=85, lr=[0.00012934927593212948, 0.00012934927593212948], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 02:32:38,439] [INFO] [timer.py:157:stop] 0/60400, SamplesPerSec=56.18703023661245
Tencent01:  iteration    85100/  320000 | elapsed time per iteration (ms): 9882.7 | learning rate 1.293E-04 | lm loss 2.953051E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2359.01 | backward: 6852.70 | optimizer: 121.44 | batch generator: 7.60 | data loader: 1.27
Tencent01: [2020-11-25 02:36:35,351] [INFO] [timer.py:157:stop] 0/60500, SamplesPerSec=56.18339526658853
Tencent01: [2020-11-25 02:40:22,535] [INFO] [timer.py:157:stop] 0/60600, SamplesPerSec=56.183732344011524
Tencent01: [2020-11-25 02:43:58,756] [INFO] [timer.py:157:stop] 0/60700, SamplesPerSec=56.188525059740066
Tencent01: [2020-11-25 02:47:34,854] [INFO] [logging.py:60:log_dist] [Rank 0] step=85200, skipped=85, lr=[0.00012930154612755452, 0.00012930154612755452], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 02:47:34,905] [INFO] [timer.py:157:stop] 0/60800, SamplesPerSec=56.19332729144522
Tencent01:  iteration    85200/  320000 | elapsed time per iteration (ms): 8964.7 | learning rate 1.293E-04 | lm loss 2.916307E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2361.64 | backward: 6481.31 | optimizer: 121.32 | batch generator: 6.87 | data loader: 1.18
Tencent01: [2020-11-25 02:51:11,078] [INFO] [timer.py:157:stop] 0/60900, SamplesPerSec=56.19809830802419
Tencent01: [2020-11-25 02:55:14,847] [INFO] [timer.py:157:stop] 0/61000, SamplesPerSec=56.191684837939576
Tencent01: [2020-11-25 02:59:32,565] [INFO] [timer.py:157:stop] 0/61100, SamplesPerSec=56.17966702449146
Tencent01: [2020-11-25 03:03:29,652] [INFO] [logging.py:60:log_dist] [Rank 0] step=85300, skipped=85, lr=[0.00012925377121431666, 0.00012925377121431666], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 03:03:29,704] [INFO] [timer.py:157:stop] 0/61200, SamplesPerSec=56.175978662012824
Tencent01:  iteration    85300/  320000 | elapsed time per iteration (ms): 9548.0 | learning rate 1.293E-04 | lm loss 2.910153E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2452.52 | backward: 6973.45 | optimizer: 121.63 | batch generator: 5.53 | data loader: 1.03
Tencent01: [2020-11-25 03:07:15,960] [INFO] [timer.py:157:stop] 0/61300, SamplesPerSec=56.17668910708942
Tencent01: [2020-11-25 03:10:52,187] [INFO] [timer.py:157:stop] 0/61400, SamplesPerSec=56.181422355697336
Tencent01: [2020-11-25 03:14:28,334] [INFO] [timer.py:157:stop] 0/61500, SamplesPerSec=56.18617352849413
Tencent01: [2020-11-25 03:18:04,492] [INFO] [logging.py:60:log_dist] [Rank 0] step=85400, skipped=85, lr=[0.0001292059512384627, 0.0001292059512384627], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 03:18:04,545] [INFO] [timer.py:157:stop] 0/61600, SamplesPerSec=56.190886855594115
Tencent01:  iteration    85400/  320000 | elapsed time per iteration (ms): 8748.4 | learning rate 1.292E-04 | lm loss 2.918212E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2358.79 | backward: 6268.03 | optimizer: 121.21 | batch generator: 6.02 | data loader: 1.07
Tencent01: [2020-11-25 03:21:58,364] [INFO] [timer.py:157:stop] 0/61700, SamplesPerSec=56.18854492916737
Tencent01: [2020-11-25 03:26:15,017] [INFO] [timer.py:157:stop] 0/61800, SamplesPerSec=56.17710284161911
Tencent01: [2020-11-25 03:30:10,826] [INFO] [timer.py:157:stop] 0/61900, SamplesPerSec=56.173997631542214
Tencent01: [2020-11-25 03:34:05,295] [INFO] [logging.py:60:log_dist] [Rank 0] step=85500, skipped=85, lr=[0.00012915808624608285, 0.00012915808624608285], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 03:34:05,346] [INFO] [timer.py:157:stop] 0/62000, SamplesPerSec=56.17141556528001
Tencent01:  iteration    85500/  320000 | elapsed time per iteration (ms): 9608.0 | learning rate 1.292E-04 | lm loss 2.999855E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2437.98 | backward: 7047.79 | optimizer: 121.84 | batch generator: 6.08 | data loader: 1.12
Tencent01: [2020-11-25 03:37:41,603] [INFO] [timer.py:157:stop] 0/62100, SamplesPerSec=56.176094701358096
Tencent01: [2020-11-25 03:41:17,767] [INFO] [timer.py:157:stop] 0/62200, SamplesPerSec=56.18079999001405
Tencent01: [2020-11-25 03:44:53,957] [INFO] [timer.py:157:stop] 0/62300, SamplesPerSec=56.185480105854026
Tencent01: [2020-11-25 03:48:46,845] [INFO] [logging.py:60:log_dist] [Rank 0] step=85600, skipped=85, lr=[0.00012911017628331077, 0.00012911017628331077], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 03:48:46,897] [INFO] [timer.py:157:stop] 0/62400, SamplesPerSec=56.183521615312344
Tencent01:  iteration    85600/  320000 | elapsed time per iteration (ms): 8815.5 | learning rate 1.291E-04 | lm loss 2.919207E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2436.45 | backward: 6257.10 | optimizer: 121.56 | batch generator: 6.35 | data loader: 1.13
Tencent01: [2020-11-25 03:53:12,005] [INFO] [timer.py:157:stop] 0/62500, SamplesPerSec=56.1688840098001
Tencent01: [2020-11-25 03:57:06,711] [INFO] [timer.py:157:stop] 0/62600, SamplesPerSec=56.16625858112994
Tencent01: [2020-11-25 04:01:01,898] [INFO] [timer.py:157:stop] 0/62700, SamplesPerSec=56.16345883808126
Tencent01: [2020-11-25 04:04:38,185] [INFO] [logging.py:60:log_dist] [Rank 0] step=85700, skipped=85, lr=[0.00012906222139632344, 0.00012906222139632344], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 04:04:38,237] [INFO] [timer.py:157:stop] 0/62800, SamplesPerSec=56.168061320861305
Tencent01:  iteration    85700/  320000 | elapsed time per iteration (ms): 9513.4 | learning rate 1.291E-04 | lm loss 3.023034E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2360.07 | backward: 7031.44 | optimizer: 121.50 | batch generator: 6.04 | data loader: 1.11
Tencent01: [2020-11-25 04:08:14,415] [INFO] [timer.py:157:stop] 0/62900, SamplesPerSec=56.17271539649641
Tencent01: [2020-11-25 04:11:50,643] [INFO] [timer.py:157:stop] 0/63000, SamplesPerSec=56.1773317538004
Tencent01: [2020-11-25 04:15:37,460] [INFO] [timer.py:157:stop] 0/63100, SamplesPerSec=56.177807076023754
Tencent01: [2020-11-25 04:15:54,671] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 04:15:54,671] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 04:15:54,671] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 04:15:54,671] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 04:15:54,671] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 04:15:54,671] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 04:15:54,672] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 04:15:54,672] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 04:15:54,672] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 04:15:54,672] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 04:15:54,671] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 04:15:54,671] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 04:15:54,671] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 04:15:54,672] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 04:15:54,672] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 04:15:54,672] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 04:15:54,672] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 04:15:54,672] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 04:15:54,672] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 04:15:54,672] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 04:15:54,672] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 04:15:54,672] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 04:15:54,672] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 04:15:54,672] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 04:15:54,672] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 04:15:54,672] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 04:15:54,672] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 04:15:54,672] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 04:15:54,672] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 04:15:54,672] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 04:15:54,673] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 04:15:54,673] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 04:16:13,383] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 85777
Tencent01: [2020-11-25 04:16:13,383] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 04:16:13,383] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 85777
Tencent01: [2020-11-25 04:16:13,384] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 85777
Tencent02: [2020-11-25 04:16:13,383] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 85777
Tencent01: [2020-11-25 04:16:13,384] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 85777
Tencent02: [2020-11-25 04:16:13,383] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-25 04:16:13,384] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: Grad overflow on iteration 85777
Tencent01: [2020-11-25 04:16:13,384] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 85777
Tencent02: [2020-11-25 04:16:13,383] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 04:16:13,384] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 04:16:13,383] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 04:16:13,384] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 85777
Tencent01: [2020-11-25 04:16:13,384] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 04:16:13,383] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 04:16:13,383] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-25 04:16:13,384] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 04:16:13,384] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 85777
Tencent02: Grad overflow on iteration 85777
Tencent01: [2020-11-25 04:16:13,384] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: [2020-11-25 04:16:13,383] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 85777
Tencent02: Grad overflow on iteration 85777
Tencent01: [2020-11-25 04:16:13,384] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 04:16:13,384] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 04:16:13,384] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: [2020-11-25 04:16:13,383] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 04:16:13,384] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: Grad overflow on iteration 85777
Tencent02: [2020-11-25 04:16:13,383] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 85777
Tencent01: [2020-11-25 04:16:13,384] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
Tencent02: [2020-11-25 04:16:13,384] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 04:16:13,384] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 04:16:13,384] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 85777
Tencent02: [2020-11-25 04:16:13,384] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 04:16:13,384] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 85777
Tencent02: [2020-11-25 04:16:13,384] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 04:19:53,842] [INFO] [logging.py:60:log_dist] [Rank 0] step=85800, skipped=86, lr=[0.00012901470185098523, 0.00012901470185098523], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 04:19:53,894] [INFO] [timer.py:157:stop] 0/63200, SamplesPerSec=56.16671931420051
Tencent01:  iteration    85800/  320000 | elapsed time per iteration (ms): 9156.6 | learning rate 1.290E-04 | lm loss 2.916105E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2466.62 | backward: 6569.43 | optimizer: 120.15 | batch generator: 6.10 | data loader: 1.09
Tencent01: [2020-11-25 04:24:09,076] [INFO] [timer.py:157:stop] 0/63300, SamplesPerSec=56.15615641251566
Tencent01: [2020-11-25 04:27:55,980] [INFO] [timer.py:157:stop] 0/63400, SamplesPerSec=56.156616552224456
Tencent01: [2020-11-25 04:31:40,099] [INFO] [timer.py:157:stop] 0/63500, SamplesPerSec=56.158158999237756
Tencent01: [2020-11-25 04:35:16,271] [INFO] [logging.py:60:log_dist] [Rank 0] step=85900, skipped=86, lr=[0.00012896665770235967, 0.00012896665770235967], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 04:35:16,323] [INFO] [timer.py:157:stop] 0/63600, SamplesPerSec=56.162753370884616
Tencent01:  iteration    85900/  320000 | elapsed time per iteration (ms): 9224.3 | learning rate 1.290E-04 | lm loss 2.956156E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2359.68 | backward: 6743.15 | optimizer: 121.10 | batch generator: 5.70 | data loader: 1.00
Tencent01: [2020-11-25 04:38:52,532] [INFO] [timer.py:157:stop] 0/63700, SamplesPerSec=56.167341883975006
Tencent01: [2020-11-25 04:42:28,722] [INFO] [timer.py:157:stop] 0/63800, SamplesPerSec=56.17192031337817
Tencent01: [2020-11-25 04:46:43,584] [INFO] [timer.py:157:stop] 0/63900, SamplesPerSec=56.161568831439574
Tencent01: [2020-11-25 04:51:00,217] [INFO] [logging.py:60:log_dist] [Rank 0] step=86000, skipped=86, lr=[0.00012891856876784627, 0.00012891856876784627], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 04:51:00,269] [INFO] [timer.py:157:stop] 0/64000, SamplesPerSec=56.15055181715575
Tencent01:  iteration    86000/  320000 | elapsed time per iteration (ms): 9439.5 | learning rate 1.289E-04 | lm loss 2.929027E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2358.33 | backward: 6959.67 | optimizer: 121.10 | batch generator: 5.63 | data loader: 0.99
Tencent01: [2020-11-25 04:51:00,273] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: /root/data/checkpoints/gpt-345M11-14-13-21/86000/mp_rank_00_model_states.pt
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: -----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 86000 | LM loss: 2.867209E+00 | LM PPL: 1.758786E+01
Tencent01: -----------------------------------------------------------------------------------
Tencent01: [2020-11-25 04:57:28,795] [INFO] [timer.py:157:stop] 0/64100, SamplesPerSec=56.146784176772314
Tencent01: [2020-11-25 05:01:13,498] [INFO] [timer.py:157:stop] 0/64200, SamplesPerSec=56.14809154316016
Tencent01: [2020-11-25 05:04:49,722] [INFO] [timer.py:157:stop] 0/64300, SamplesPerSec=56.152651121300174
Tencent01: [2020-11-25 05:08:25,815] [INFO] [logging.py:60:log_dist] [Rank 0] step=86100, skipped=86, lr=[0.00012887043509379433, 0.00012887043509379433], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 05:08:25,867] [INFO] [timer.py:157:stop] 0/64400, SamplesPerSec=56.15722440300989
Tencent01:  iteration    86100/  320000 | elapsed time per iteration (ms): 10456.0 | learning rate 1.289E-04 | lm loss 2.918391E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2358.04 | backward: 6470.49 | optimizer: 121.14 | batch generator: 6.60 | data loader: 1.12
Tencent01: [2020-11-25 05:12:09,365] [INFO] [timer.py:157:stop] 0/64500, SamplesPerSec=56.15897194716314
Tencent01: [2020-11-25 05:16:04,630] [INFO] [timer.py:157:stop] 0/64600, SamplesPerSec=56.15622973311802
Tencent01: [2020-11-25 05:20:21,536] [INFO] [timer.py:157:stop] 0/64700, SamplesPerSec=56.14525647947869
Tencent01: [2020-11-25 05:24:27,551] [INFO] [logging.py:60:log_dist] [Rank 0] step=86200, skipped=86, lr=[0.00012882225672659659, 0.00012882225672659659], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 05:24:27,603] [INFO] [timer.py:157:stop] 0/64800, SamplesPerSec=56.13844115262959
Tencent01:  iteration    86200/  320000 | elapsed time per iteration (ms): 9617.4 | learning rate 1.288E-04 | lm loss 2.935393E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2434.13 | backward: 7061.74 | optimizer: 121.13 | batch generator: 5.42 | data loader: 0.96
Tencent01: [2020-11-25 05:28:12,331] [INFO] [timer.py:157:stop] 0/64900, SamplesPerSec=56.13974332873003
Tencent01: [2020-11-25 05:31:48,533] [INFO] [timer.py:157:stop] 0/65000, SamplesPerSec=56.14427186597634
Tencent01: [2020-11-25 05:35:24,628] [INFO] [timer.py:157:stop] 0/65100, SamplesPerSec=56.14882772023716
Tencent01: [2020-11-25 05:39:00,744] [INFO] [logging.py:60:log_dist] [Rank 0] step=86300, skipped=86, lr=[0.00012877403371268863, 0.00012877403371268863], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 05:39:00,796] [INFO] [timer.py:157:stop] 0/65200, SamplesPerSec=56.15334309893726
Tencent01:  iteration    86300/  320000 | elapsed time per iteration (ms): 8731.9 | learning rate 1.288E-04 | lm loss 2.980728E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2358.00 | backward: 6252.41 | optimizer: 121.16 | batch generator: 5.60 | data loader: 1.01
Tencent01: [2020-11-25 05:42:54,987] [INFO] [timer.py:157:stop] 0/65300, SamplesPerSec=56.15104577170474
Tencent01: [2020-11-25 05:47:21,059] [INFO] [timer.py:157:stop] 0/65400, SamplesPerSec=56.136751172800196
Tencent01: [2020-11-25 05:51:26,581] [INFO] [timer.py:157:stop] 0/65500, SamplesPerSec=56.1302318481345
Tencent01: [2020-11-25 05:55:13,191] [INFO] [logging.py:60:log_dist] [Rank 0] step=86400, skipped=86, lr=[0.00012872576609854926, 0.00012872576609854926], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 05:55:13,243] [INFO] [timer.py:157:stop] 0/65600, SamplesPerSec=56.13081487810837
Tencent01:  iteration    86400/  320000 | elapsed time per iteration (ms): 9724.5 | learning rate 1.287E-04 | lm loss 2.935060E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2360.21 | backward: 7242.57 | optimizer: 121.31 | batch generator: 5.89 | data loader: 1.08
Tencent01: [2020-11-25 05:58:49,566] [INFO] [timer.py:157:stop] 0/65700, SamplesPerSec=56.1352608815516
Tencent01: [2020-11-25 06:02:25,716] [INFO] [timer.py:157:stop] 0/65800, SamplesPerSec=56.13976321948404
Tencent01: [2020-11-25 06:06:01,880] [INFO] [timer.py:157:stop] 0/65900, SamplesPerSec=56.14423907205333
Tencent01: [2020-11-25 06:09:56,081] [INFO] [logging.py:60:log_dist] [Rank 0] step=86500, skipped=86, lr=[0.00012867745393070013, 0.00012867745393070013], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 06:09:56,133] [INFO] [timer.py:157:stop] 0/66000, SamplesPerSec=56.14195811399882
Tencent01:  iteration    86500/  320000 | elapsed time per iteration (ms): 8828.9 | learning rate 1.287E-04 | lm loss 2.985471E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2357.27 | backward: 6350.00 | optimizer: 121.25 | batch generator: 5.54 | data loader: 1.04
Tencent01: [2020-11-25 06:14:02,263] [INFO] [timer.py:157:stop] 0/66100, SamplesPerSec=56.13526489973494
Tencent01: [2020-11-25 06:18:17,494] [INFO] [timer.py:157:stop] 0/66200, SamplesPerSec=56.125202476432634
Tencent01: [2020-11-25 06:22:03,792] [INFO] [timer.py:157:stop] 0/66300, SamplesPerSec=56.12591363128798
Tencent01: [2020-11-25 06:25:49,956] [INFO] [logging.py:60:log_dist] [Rank 0] step=86600, skipped=86, lr=[0.00012862909725570585, 0.00012862909725570585], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 06:25:50,008] [INFO] [timer.py:157:stop] 0/66400, SamplesPerSec=56.12665296092927
Tencent01:  iteration    86600/  320000 | elapsed time per iteration (ms): 9538.7 | learning rate 1.286E-04 | lm loss 2.893720E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2360.48 | backward: 7056.55 | optimizer: 121.35 | batch generator: 5.67 | data loader: 1.01
Tencent01: [2020-11-25 06:29:26,305] [INFO] [timer.py:157:stop] 0/66500, SamplesPerSec=56.13106278120911
Tencent01: [2020-11-25 06:33:02,530] [INFO] [timer.py:157:stop] 0/66600, SamplesPerSec=56.13548493574247
Tencent01: [2020-11-25 06:36:46,655] [INFO] [timer.py:157:stop] 0/66700, SamplesPerSec=56.136986635491915
Tencent01: [2020-11-25 06:41:01,091] [INFO] [logging.py:60:log_dist] [Rank 0] step=86700, skipped=86, lr=[0.00012858069612017403, 0.00012858069612017403], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 06:41:01,143] [INFO] [timer.py:157:stop] 0/66800, SamplesPerSec=56.127288939304826
Tencent01:  iteration    86700/  320000 | elapsed time per iteration (ms): 9111.4 | learning rate 1.286E-04 | lm loss 2.989098E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2357.69 | backward: 6632.12 | optimizer: 121.17 | batch generator: 5.85 | data loader: 1.04
Tencent01: [2020-11-25 06:45:07,571] [INFO] [timer.py:157:stop] 0/66900, SamplesPerSec=56.120587443766816
Tencent01: [2020-11-25 06:49:14,251] [INFO] [timer.py:157:stop] 0/67000, SamplesPerSec=56.11381406550495
Tencent01: [2020-11-25 06:52:59,892] [INFO] [timer.py:157:stop] 0/67100, SamplesPerSec=56.114776544063155
Tencent01: [2020-11-25 06:53:34,421] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 06:53:34,421] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 06:53:34,421] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 06:53:34,421] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 06:53:34,422] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 06:53:34,422] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 06:53:34,422] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 06:53:34,422] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 06:53:34,422] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 06:53:34,422] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 06:53:34,422] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 06:53:34,422] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 06:53:34,422] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 06:53:34,422] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 06:53:34,422] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 06:53:34,422] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 06:53:34,422] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 06:53:34,422] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 06:53:34,422] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 06:53:34,422] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 06:53:34,422] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 06:53:34,422] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 06:53:34,422] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 06:53:34,422] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 06:53:34,423] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 06:53:34,422] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 06:53:34,423] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 06:53:34,423] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 06:53:34,423] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 06:53:34,423] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 06:53:34,423] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 06:53:34,423] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 06:55:09,628] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 86789
Tencent01: [2020-11-25 06:55:09,628] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 06:55:09,628] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 86789
Tencent01: [2020-11-25 06:55:09,628] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 06:55:09,629] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
Tencent01: [2020-11-25 06:55:09,629] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 86789
Tencent01: [2020-11-25 06:55:09,629] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 86789
Tencent01: [2020-11-25 06:55:09,629] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 06:55:09,629] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 86789
Tencent01: [2020-11-25 06:55:09,629] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 06:55:09,629] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 86789
Tencent01: [2020-11-25 06:55:09,629] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 06:55:09,629] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 86789
Tencent01: [2020-11-25 06:55:09,629] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 06:55:09,629] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 06:55:09,629] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 86789
Tencent01: [2020-11-25 06:55:09,629] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 06:55:09,629] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 86789
Tencent02: [2020-11-25 06:55:09,629] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 86789
Tencent02: [2020-11-25 06:55:09,629] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 86789
Tencent02: [2020-11-25 06:55:09,629] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 06:55:09,629] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 06:55:09,629] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 86789
Tencent02: [2020-11-25 06:55:09,629] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 06:55:09,629] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 06:55:09,629] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 86789
Tencent02: [2020-11-25 06:55:09,629] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 86789
Tencent02: [2020-11-25 06:55:09,629] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 86789
Tencent02: [2020-11-25 06:55:09,629] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 06:55:09,629] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 86789
Tencent02: [2020-11-25 06:55:09,629] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 06:55:09,629] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 06:55:09,629] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 06:56:36,122] [INFO] [logging.py:60:log_dist] [Rank 0] step=86800, skipped=87, lr=[0.0001285327352459447, 0.0001285327352459447], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 06:56:36,174] [INFO] [timer.py:157:stop] 0/67200, SamplesPerSec=56.1191574114469
Tencent01:  iteration    86800/  320000 | elapsed time per iteration (ms): 9350.3 | learning rate 1.285E-04 | lm loss 2.971266E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2359.49 | backward: 6870.54 | optimizer: 119.93 | batch generator: 5.62 | data loader: 0.99
Tencent01: [2020-11-25 07:00:12,368] [INFO] [timer.py:157:stop] 0/67300, SamplesPerSec=56.12356153363771
Tencent01: [2020-11-25 07:03:56,029] [INFO] [timer.py:157:stop] 0/67400, SamplesPerSec=56.125228611223214
Tencent01: [2020-11-25 07:07:50,993] [INFO] [timer.py:157:stop] 0/67500, SamplesPerSec=56.12277376037884
Tencent01: [2020-11-25 07:12:16,219] [INFO] [logging.py:60:log_dist] [Rank 0] step=86900, skipped=87, lr=[0.00012848424577277238, 0.00012848424577277238], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 07:12:16,271] [INFO] [timer.py:157:stop] 0/67600, SamplesPerSec=56.10929211072
Tencent01:  iteration    86900/  320000 | elapsed time per iteration (ms): 9401.0 | learning rate 1.285E-04 | lm loss 2.910022E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2435.91 | backward: 6843.41 | optimizer: 121.29 | batch generator: 5.91 | data loader: 1.03
Tencent01: [2020-11-25 07:16:22,966] [INFO] [timer.py:157:stop] 0/67700, SamplesPerSec=56.10260739266078
Tencent01: [2020-11-25 07:20:07,225] [INFO] [timer.py:157:stop] 0/67800, SamplesPerSec=56.10408415366196
Tencent01: [2020-11-25 07:23:43,519] [INFO] [timer.py:157:stop] 0/67900, SamplesPerSec=56.108425466383686
Tencent01: [2020-11-25 07:27:19,600] [INFO] [logging.py:60:log_dist] [Rank 0] step=87000, skipped=87, lr=[0.0001284357119786745, 0.0001284357119786745], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 07:27:19,652] [INFO] [timer.py:157:stop] 0/68000, SamplesPerSec=56.11282062072862
Tencent01:  iteration    87000/  320000 | elapsed time per iteration (ms): 9033.8 | learning rate 1.284E-04 | lm loss 2.947754E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2359.06 | backward: 6553.25 | optimizer: 121.14 | batch generator: 5.79 | data loader: 0.99
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: -----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 87000 | LM loss: 2.933530E+00 | LM PPL: 1.879385E+01
Tencent01: -----------------------------------------------------------------------------------
Tencent01: [2020-11-25 07:32:03,246] [INFO] [timer.py:157:stop] 0/68100, SamplesPerSec=56.11269409589936
Tencent01: [2020-11-25 07:35:49,531] [INFO] [timer.py:157:stop] 0/68200, SamplesPerSec=56.113417029469844
Tencent01: [2020-11-25 07:40:28,212] [INFO] [timer.py:157:stop] 0/68300, SamplesPerSec=56.095266767411765
Tencent01: [2020-11-25 07:44:42,615] [INFO] [logging.py:60:log_dist] [Rank 0] step=87100, skipped=87, lr=[0.00012838713391042923, 0.00012838713391042923], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 07:44:42,667] [INFO] [timer.py:157:stop] 0/68400, SamplesPerSec=56.08588285260308
Tencent01:  iteration    87100/  320000 | elapsed time per iteration (ms): 10430.1 | learning rate 1.284E-04 | lm loss 2.983281E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2358.55 | backward: 7399.59 | optimizer: 121.80 | batch generator: 6.90 | data loader: 1.23
Tencent01: [2020-11-25 07:48:29,855] [INFO] [timer.py:157:stop] 0/68500, SamplesPerSec=56.08631270674276
Tencent01: [2020-11-25 07:52:06,153] [INFO] [timer.py:157:stop] 0/68600, SamplesPerSec=56.09065602211546
Tencent01: [2020-11-25 07:55:42,357] [INFO] [timer.py:157:stop] 0/68700, SamplesPerSec=56.09501647689311
Tencent01: [2020-11-25 07:59:18,443] [INFO] [logging.py:60:log_dist] [Rank 0] step=87200, skipped=87, lr=[0.0001283385116148576, 0.0001283385116148576], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 07:59:18,495] [INFO] [timer.py:157:stop] 0/68800, SamplesPerSec=56.0993847449415
Tencent01:  iteration    87200/  320000 | elapsed time per iteration (ms): 8758.3 | learning rate 1.283E-04 | lm loss 2.943210E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2360.30 | backward: 6275.49 | optimizer: 122.07 | batch generator: 6.69 | data loader: 1.23
Tencent01: [2020-11-25 08:03:15,262] [INFO] [timer.py:157:stop] 0/68900, SamplesPerSec=56.09637398005997
Tencent01: [2020-11-25 08:07:50,054] [INFO] [timer.py:157:stop] 0/69000, SamplesPerSec=56.07981880956325
Tencent01: [2020-11-25 08:12:05,864] [INFO] [timer.py:157:stop] 0/69100, SamplesPerSec=56.070070768838164
Tencent01: [2020-11-25 08:15:42,258] [INFO] [logging.py:60:log_dist] [Rank 0] step=87300, skipped=87, lr=[0.0001282898451388231, 0.0001282898451388231], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 08:15:42,310] [INFO] [timer.py:157:stop] 0/69200, SamplesPerSec=56.0743189944536
Tencent01:  iteration    87300/  320000 | elapsed time per iteration (ms): 9838.2 | learning rate 1.283E-04 | lm loss 2.949569E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2358.41 | backward: 7357.92 | optimizer: 121.45 | batch generator: 5.21 | data loader: 0.97
Tencent01: [2020-11-25 08:19:28,402] [INFO] [timer.py:157:stop] 0/69300, SamplesPerSec=56.07514777107493
Tencent01: [2020-11-25 08:23:04,619] [INFO] [timer.py:157:stop] 0/69400, SamplesPerSec=56.07946982514422
Tencent01: [2020-11-25 08:26:40,773] [INFO] [timer.py:157:stop] 0/69500, SamplesPerSec=56.08379648869705
Tencent01: [2020-11-25 08:30:36,632] [INFO] [logging.py:60:log_dist] [Rank 0] step=87400, skipped=87, lr=[0.00012824113452923188, 0.00012824113452923188], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 08:30:36,684] [INFO] [timer.py:157:stop] 0/69600, SamplesPerSec=56.08113920729638
Tencent01:  iteration    87400/  320000 | elapsed time per iteration (ms): 8943.7 | learning rate 1.282E-04 | lm loss 2.913557E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2451.67 | backward: 6370.60 | optimizer: 121.10 | batch generator: 5.68 | data loader: 1.01
Tencent01: [2020-11-25 08:34:43,852] [INFO] [timer.py:157:stop] 0/69700, SamplesPerSec=56.07451988820233
Tencent01: [2020-11-25 08:38:48,346] [INFO] [timer.py:157:stop] 0/69800, SamplesPerSec=56.068868307196915
Tencent01: [2020-11-25 08:42:54,285] [INFO] [timer.py:157:stop] 0/69900, SamplesPerSec=56.062722602211856
Tencent01: [2020-11-25 08:46:38,900] [INFO] [logging.py:60:log_dist] [Rank 0] step=87500, skipped=87, lr=[0.00012819237983303266, 0.00012819237983303266], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 08:46:38,953] [INFO] [timer.py:157:stop] 0/70000, SamplesPerSec=56.064060303860224
Tencent01:  iteration    87500/  320000 | elapsed time per iteration (ms): 9622.7 | learning rate 1.282E-04 | lm loss 2.929610E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2358.97 | backward: 7141.95 | optimizer: 121.38 | batch generator: 5.82 | data loader: 1.06
Tencent01: [2020-11-25 08:50:15,144] [INFO] [timer.py:157:stop] 0/70100, SamplesPerSec=56.068353288930226
Tencent01: [2020-11-25 08:53:51,239] [INFO] [timer.py:157:stop] 0/70200, SamplesPerSec=56.07267376790811
Tencent01: [2020-11-25 08:57:34,919] [INFO] [timer.py:157:stop] 0/70300, SamplesPerSec=56.074334600979945
Tencent01: [2020-11-25 09:01:40,656] [INFO] [logging.py:60:log_dist] [Rank 0] step=87600, skipped=87, lr=[0.00012814358109721648, 0.00012814358109721648], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 09:01:40,701] [INFO] [timer.py:157:stop] 0/70400, SamplesPerSec=56.068277208283234
Tencent01:  iteration    87600/  320000 | elapsed time per iteration (ms): 9017.5 | learning rate 1.281E-04 | lm loss 2.980815E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2434.80 | backward: 6460.95 | optimizer: 121.38 | batch generator: 5.44 | data loader: 0.97
Tencent01: [2020-11-25 09:06:08,485] [INFO] [timer.py:157:stop] 0/70500, SamplesPerSec=56.05457756367481
Tencent01: [2020-11-25 09:10:14,015] [INFO] [timer.py:157:stop] 0/70600, SamplesPerSec=56.048663232217145
Tencent01: [2020-11-25 09:14:00,628] [INFO] [timer.py:157:stop] 0/70700, SamplesPerSec=56.049332433761414
Tencent01: [2020-11-25 09:17:36,884] [INFO] [logging.py:60:log_dist] [Rank 0] step=87700, skipped=87, lr=[0.00012809473836881705, 0.00012809473836881705], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 09:17:36,937] [INFO] [timer.py:157:stop] 0/70800, SamplesPerSec=56.053564959884675
Tencent01:  iteration    87700/  320000 | elapsed time per iteration (ms): 9562.4 | learning rate 1.281E-04 | lm loss 2.942552E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2358.62 | backward: 7082.25 | optimizer: 121.12 | batch generator: 5.88 | data loader: 1.03
Tencent01: [2020-11-25 09:21:13,171] [INFO] [timer.py:157:stop] 0/70900, SamplesPerSec=56.05781310844014
Tencent01: [2020-11-25 09:24:57,232] [INFO] [timer.py:157:stop] 0/71000, SamplesPerSec=56.05934448787317
Tencent01: [2020-11-25 09:28:52,529] [INFO] [timer.py:157:stop] 0/71100, SamplesPerSec=56.05699322518469
Tencent01: [2020-11-25 09:31:51,838] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 09:31:51,838] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 09:31:51,838] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 09:31:51,838] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 09:31:51,838] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 09:31:51,839] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 09:31:51,839] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 09:31:51,839] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 09:31:51,839] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 09:31:51,839] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 09:31:51,839] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 09:31:51,839] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 09:31:51,839] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 09:31:51,839] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 09:31:51,839] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 09:31:51,839] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 09:31:51,838] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 09:31:51,838] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 09:31:51,838] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 09:31:51,838] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 09:31:51,838] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 09:31:51,838] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 09:31:51,838] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 09:31:51,838] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 09:31:51,839] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 09:31:51,839] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 09:31:51,839] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 09:31:51,839] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 09:31:51,839] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 09:31:51,839] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 09:31:51,839] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 09:31:51,839] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 09:32:46,089] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 87794
Tencent01: [2020-11-25 09:32:46,089] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 09:32:46,089] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 87794
Tencent01: [2020-11-25 09:32:46,090] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 09:32:46,090] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
Tencent01: [2020-11-25 09:32:46,090] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 87794
Tencent01: [2020-11-25 09:32:46,090] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 09:32:46,089] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 87794
Tencent02: [2020-11-25 09:32:46,089] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 09:32:46,089] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 87794
Tencent02: [2020-11-25 09:32:46,089] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 09:32:46,090] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 87794
Tencent02: [2020-11-25 09:32:46,089] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-25 09:32:46,090] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: Grad overflow on iteration 87794
Tencent02: [2020-11-25 09:32:46,089] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 09:32:46,090] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 87794
Tencent01: [2020-11-25 09:32:46,090] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 87794
Tencent01: [2020-11-25 09:32:46,091] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 09:32:46,089] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-25 09:32:46,091] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: Grad overflow on iteration 87794
Tencent02: [2020-11-25 09:32:46,089] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 09:32:46,089] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 87794
Tencent02: [2020-11-25 09:32:46,089] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: [2020-11-25 09:32:46,091] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 87794
Tencent01: Grad overflow on iteration 87794
Tencent01: [2020-11-25 09:32:46,091] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 87794
Tencent02: [2020-11-25 09:32:46,090] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 09:32:46,089] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 87794
Tencent01: [2020-11-25 09:32:46,091] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 09:32:46,090] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 09:32:46,090] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 09:32:46,091] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 09:32:46,090] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 87794
Tencent02: [2020-11-25 09:32:46,090] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 09:33:29,356] [INFO] [logging.py:60:log_dist] [Rank 0] step=87800, skipped=88, lr=[0.00012804634077902505, 0.00012804634077902505], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 09:33:29,409] [INFO] [timer.py:157:stop] 0/71200, SamplesPerSec=56.04031220365985
Tencent01:  iteration    87800/  320000 | elapsed time per iteration (ms): 9524.7 | learning rate 1.280E-04 | lm loss 2.946564E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2357.13 | backward: 7046.94 | optimizer: 120.29 | batch generator: 5.59 | data loader: 1.00
Tencent01: [2020-11-25 09:37:35,356] [INFO] [timer.py:157:stop] 0/71300, SamplesPerSec=56.03433073211692
Tencent01: [2020-11-25 09:41:21,606] [INFO] [timer.py:157:stop] 0/71400, SamplesPerSec=56.03513191128923
Tencent01: [2020-11-25 09:44:57,847] [INFO] [timer.py:157:stop] 0/71500, SamplesPerSec=56.03937491499797
Tencent01: [2020-11-25 09:48:34,030] [INFO] [logging.py:60:log_dist] [Rank 0] step=87900, skipped=88, lr=[0.00012799741064548, 0.00012799741064548], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 09:48:34,082] [INFO] [timer.py:157:stop] 0/71600, SamplesPerSec=56.043600807931135
Tencent01:  iteration    87900/  320000 | elapsed time per iteration (ms): 9046.7 | learning rate 1.280E-04 | lm loss 2.996205E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2359.53 | backward: 6565.37 | optimizer: 121.47 | batch generator: 5.88 | data loader: 1.04
Tencent01: [2020-11-25 09:52:18,339] [INFO] [timer.py:157:stop] 0/71700, SamplesPerSec=56.045068990096574
Tencent01: [2020-11-25 09:56:04,580] [INFO] [timer.py:157:stop] 0/71800, SamplesPerSec=56.045850652771634
Tencent01: [2020-11-25 10:00:51,589] [INFO] [timer.py:157:stop] 0/71900, SamplesPerSec=56.025896521629086
Tencent01: [2020-11-25 10:04:58,687] [INFO] [logging.py:60:log_dist] [Rank 0] step=88000, skipped=88, lr=[0.00012794843666023495, 0.00012794843666023495], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 10:04:58,739] [INFO] [timer.py:157:stop] 0/72000, SamplesPerSec=56.019583256234704
Tencent01:  iteration    88000/  320000 | elapsed time per iteration (ms): 9846.6 | learning rate 1.279E-04 | lm loss 2.908720E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2357.00 | backward: 7367.73 | optimizer: 121.46 | batch generator: 5.29 | data loader: 0.97
Tencent01: [2020-11-25 10:04:58,742] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: /root/data/checkpoints/gpt-345M11-14-13-21/88000/mp_rank_00_model_states.pt
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: -----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 88000 | LM loss: 2.856430E+00 | LM PPL: 1.739930E+01
Tencent01: -----------------------------------------------------------------------------------
Tencent01: [2020-11-25 10:11:13,474] [INFO] [timer.py:157:stop] 0/72100, SamplesPerSec=56.02327502206067
Tencent01: [2020-11-25 10:14:59,629] [INFO] [timer.py:157:stop] 0/72200, SamplesPerSec=56.02412299134086
Tencent01: [2020-11-25 10:18:35,778] [INFO] [timer.py:157:stop] 0/72300, SamplesPerSec=56.028363344257436
Tencent01: [2020-11-25 10:22:20,140] [INFO] [logging.py:60:log_dist] [Rank 0] step=88100, skipped=88, lr=[0.00012789941887049233, 0.00012789941887049233], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 10:22:20,192] [INFO] [timer.py:157:stop] 0/72400, SamplesPerSec=56.02979811690817
Tencent01:  iteration    88100/  320000 | elapsed time per iteration (ms): 10414.5 | learning rate 1.279E-04 | lm loss 2.965457E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2358.37 | backward: 6365.29 | optimizer: 121.14 | batch generator: 8.21 | data loader: 1.26
Tencent01: [2020-11-25 10:26:06,106] [INFO] [timer.py:157:stop] 0/72500, SamplesPerSec=56.03071194426975
Tencent01: [2020-11-25 10:30:32,336] [INFO] [timer.py:157:stop] 0/72600, SamplesPerSec=56.018005583247366
Tencent01: [2020-11-25 10:34:48,850] [INFO] [timer.py:157:stop] 0/72700, SamplesPerSec=56.0086155232278
Tencent01: [2020-11-25 10:38:34,294] [INFO] [logging.py:60:log_dist] [Rank 0] step=88200, skipped=88, lr=[0.000127850357323497, 0.000127850357323497], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 10:38:34,347] [INFO] [timer.py:157:stop] 0/72800, SamplesPerSec=56.00969102992411
Tencent01:  iteration    88200/  320000 | elapsed time per iteration (ms): 9741.5 | learning rate 1.278E-04 | lm loss 2.865900E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2359.50 | backward: 7260.00 | optimizer: 121.65 | batch generator: 5.96 | data loader: 1.08
Tencent01: [2020-11-25 10:42:19,878] [INFO] [timer.py:157:stop] 0/72900, SamplesPerSec=56.01075326772926
Tencent01: [2020-11-25 10:45:56,044] [INFO] [timer.py:157:stop] 0/73000, SamplesPerSec=56.01496793021854
Tencent01: [2020-11-25 10:49:32,196] [INFO] [timer.py:157:stop] 0/73100, SamplesPerSec=56.01917536071768
Tencent01: [2020-11-25 10:53:29,588] [INFO] [logging.py:60:log_dist] [Rank 0] step=88300, skipped=88, lr=[0.00012780125206653578, 0.00012780125206653578], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 10:53:29,640] [INFO] [timer.py:157:stop] 0/73200, SamplesPerSec=56.01624136036901
Tencent01:  iteration    88300/  320000 | elapsed time per iteration (ms): 8952.9 | learning rate 1.278E-04 | lm loss 2.930343E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2359.66 | backward: 6470.92 | optimizer: 121.93 | batch generator: 6.65 | data loader: 1.24
Tencent01: [2020-11-25 10:57:25,715] [INFO] [timer.py:157:stop] 0/73300, SamplesPerSec=56.01377096742396
Tencent01: [2020-11-25 11:01:40,565] [INFO] [timer.py:157:stop] 0/73400, SamplesPerSec=56.005030329995634
Tencent01: [2020-11-25 11:05:46,632] [INFO] [timer.py:157:stop] 0/73500, SamplesPerSec=55.99925069264221
Tencent01: [2020-11-25 11:09:31,307] [INFO] [logging.py:60:log_dist] [Rank 0] step=88400, skipped=88, lr=[0.00012775210314693782, 0.00012775210314693782], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 11:09:31,359] [INFO] [timer.py:157:stop] 0/73600, SamplesPerSec=56.00058419193197
Tencent01:  iteration    88400/  320000 | elapsed time per iteration (ms): 9617.2 | learning rate 1.278E-04 | lm loss 2.952924E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2360.52 | backward: 7134.72 | optimizer: 121.54 | batch generator: 6.20 | data loader: 1.13
Tencent01: [2020-11-25 11:13:07,627] [INFO] [timer.py:157:stop] 0/73700, SamplesPerSec=56.00473089630486
Tencent01: [2020-11-25 11:16:43,858] [INFO] [timer.py:157:stop] 0/73800, SamplesPerSec=56.00888770329459
Tencent01: [2020-11-25 11:20:30,536] [INFO] [timer.py:157:stop] 0/73900, SamplesPerSec=56.00955857113335
Tencent01: [2020-11-25 11:24:27,032] [INFO] [logging.py:60:log_dist] [Rank 0] step=88500, skipped=88, lr=[0.00012770291061207413, 0.00012770291061207413], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 11:24:27,084] [INFO] [timer.py:157:stop] 0/74000, SamplesPerSec=56.00696388595581
Tencent01:  iteration    88500/  320000 | elapsed time per iteration (ms): 8957.2 | learning rate 1.277E-04 | lm loss 3.012054E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2466.37 | backward: 6368.75 | optimizer: 121.71 | batch generator: 6.54 | data loader: 1.21
Tencent01: [2020-11-25 11:29:01,943] [INFO] [timer.py:157:stop] 0/74100, SamplesPerSec=55.991707045559
Tencent01: [2020-11-25 11:33:07,334] [INFO] [timer.py:157:stop] 0/74200, SamplesPerSec=55.9862305643093
Tencent01: [2020-11-25 11:36:53,889] [INFO] [timer.py:157:stop] 0/74300, SamplesPerSec=55.986966970087344
Tencent01: [2020-11-25 11:40:30,097] [INFO] [logging.py:60:log_dist] [Rank 0] step=88600, skipped=88, lr=[0.00012765367450935796, 0.00012765367450935796], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 11:40:30,149] [INFO] [timer.py:157:stop] 0/74400, SamplesPerSec=55.99109492442676
Tencent01:  iteration    88600/  320000 | elapsed time per iteration (ms): 9630.7 | learning rate 1.277E-04 | lm loss 2.959670E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2360.53 | backward: 7147.85 | optimizer: 121.85 | batch generator: 6.41 | data loader: 1.22
Tencent01: [2020-11-25 11:44:06,281] [INFO] [timer.py:157:stop] 0/74500, SamplesPerSec=55.99525594839203
Tencent01: [2020-11-25 11:47:50,705] [INFO] [timer.py:157:stop] 0/74600, SamplesPerSec=55.99667710637326
Tencent01: [2020-11-25 11:51:36,553] [INFO] [timer.py:157:stop] 0/74700, SamplesPerSec=55.99763112720415
Tencent01: [2020-11-25 11:56:20,960] [INFO] [logging.py:60:log_dist] [Rank 0] step=88700, skipped=88, lr=[0.00012760439488624444, 0.00012760439488624444], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 11:56:21,012] [INFO] [timer.py:157:stop] 0/74800, SamplesPerSec=55.979392705310076
Tencent01:  iteration    88700/  320000 | elapsed time per iteration (ms): 9508.6 | learning rate 1.276E-04 | lm loss 2.988591E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2443.07 | backward: 6943.75 | optimizer: 121.42 | batch generator: 6.29 | data loader: 1.10
Tencent01: [2020-11-25 12:00:27,077] [INFO] [timer.py:157:stop] 0/74900, SamplesPerSec=55.97375459268427
Tencent01: [2020-11-25 12:04:13,228] [INFO] [timer.py:157:stop] 0/75000, SamplesPerSec=55.974629233690706
Tencent01: [2020-11-25 12:07:49,531] [INFO] [timer.py:157:stop] 0/75100, SamplesPerSec=55.97871409655795
Tencent01: [2020-11-25 12:10:51,046] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 12:10:51,046] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 12:10:51,047] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 12:10:51,047] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 12:10:51,047] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 12:10:51,047] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 12:10:51,047] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 12:10:51,047] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 12:10:51,048] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 12:10:51,047] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 12:10:51,048] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 12:10:51,047] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 12:10:51,048] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 12:10:51,047] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 12:10:51,047] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 12:10:51,048] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 12:10:51,047] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 12:10:51,048] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 12:10:51,048] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 12:10:51,048] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 12:10:51,047] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 12:10:51,048] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 12:10:51,047] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 12:10:51,048] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 12:10:51,047] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 12:10:51,048] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 12:10:51,047] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 12:10:51,048] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 12:10:51,048] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 12:10:51,048] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 12:10:51,048] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 12:10:51,048] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 12:11:25,682] [INFO] [logging.py:60:log_dist] [Rank 0] step=88800, skipped=88, lr=[0.00012755507179023064, 0.00012755507179023064], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 12:11:25,734] [INFO] [timer.py:157:stop] 0/75200, SamplesPerSec=55.982819874942145
Tencent01:  iteration    88800/  320000 | elapsed time per iteration (ms): 9047.2 | learning rate 1.276E-04 | lm loss 2.937814E+00 | loss scale 524288.0 |
Tencent01: time (ms) | forward: 2359.42 | backward: 6565.81 | optimizer: 121.59 | batch generator: 5.70 | data loader: 1.06
Tencent01: [2020-11-25 12:11:42,908] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 88801
Tencent01: [2020-11-25 12:11:42,908] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 12:11:42,908] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 88801
Tencent01: [2020-11-25 12:11:42,908] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 12:11:42,908] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 88801
Tencent01: [2020-11-25 12:11:42,908] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 88801
Tencent01: [2020-11-25 12:11:42,908] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 12:11:42,908] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 12:11:42,908] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 88801
Tencent01: [2020-11-25 12:11:42,908] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 12:11:42,909] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 88801
Tencent02: [2020-11-25 12:11:42,909] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 88801
Tencent02: [2020-11-25 12:11:42,909] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 88801
Tencent02: [2020-11-25 12:11:42,909] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 88801
Tencent02: [2020-11-25 12:11:42,909] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 12:11:42,909] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 88801
Tencent02: [2020-11-25 12:11:42,909] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 12:11:42,909] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 12:11:42,909] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 12:11:42,909] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 12:11:42,909] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 88801
Tencent02: [2020-11-25 12:11:42,909] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 88801
Tencent02: [2020-11-25 12:11:42,909] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 12:11:42,909] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 12:11:42,909] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 88801
Tencent02: [2020-11-25 12:11:42,910] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 12:11:42,909] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 88801
Tencent01: [2020-11-25 12:11:42,909] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 12:11:42,909] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
Tencent01: [2020-11-25 12:11:42,909] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 88801
Tencent01: [2020-11-25 12:11:42,909] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 88801
Tencent01: [2020-11-25 12:11:42,909] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 12:11:42,909] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 12:15:12,279] [INFO] [timer.py:157:stop] 0/75300, SamplesPerSec=55.98354602739482
Tencent01: [2020-11-25 12:18:57,820] [INFO] [timer.py:157:stop] 0/75400, SamplesPerSec=55.984599352349456
Tencent01: [2020-11-25 12:23:23,044] [INFO] [timer.py:157:stop] 0/75500, SamplesPerSec=55.972784289047496
Tencent01: [2020-11-25 12:27:26,675] [INFO] [logging.py:60:log_dist] [Rank 0] step=88900, skipped=89, lr=[0.00012750619914886872, 0.00012750619914886872], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 12:27:26,726] [INFO] [timer.py:157:stop] 0/75600, SamplesPerSec=55.96797688098868
Tencent01:  iteration    88900/  320000 | elapsed time per iteration (ms): 9609.9 | learning rate 1.275E-04 | lm loss 2.947128E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2464.73 | backward: 7024.00 | optimizer: 120.79 | batch generator: 5.30 | data loader: 1.05
Tencent01: [2020-11-25 12:31:13,754] [INFO] [timer.py:157:stop] 0/75700, SamplesPerSec=55.968571229161725
Tencent01: [2020-11-25 12:34:58,412] [INFO] [timer.py:157:stop] 0/75800, SamplesPerSec=55.969937948443146
Tencent01: [2020-11-25 12:38:34,598] [INFO] [timer.py:157:stop] 0/75900, SamplesPerSec=55.97402843409455
Tencent01: [2020-11-25 12:42:20,396] [INFO] [logging.py:60:log_dist] [Rank 0] step=89000, skipped=89, lr=[0.00012745678968325547, 0.00012745678968325547], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 12:42:20,449] [INFO] [timer.py:157:stop] 0/76000, SamplesPerSec=55.974987021052854
Tencent01:  iteration    89000/  320000 | elapsed time per iteration (ms): 8937.2 | learning rate 1.275E-04 | lm loss 2.921686E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2359.57 | backward: 6455.66 | optimizer: 121.59 | batch generator: 6.21 | data loader: 1.16
Tencent01: ----------------------------------------------------------------------------------------------------
Tencent01: -----------------------------------------------------------------------------------
Tencent01:  validation loss at iteration 89000 | LM loss: 2.909004E+00 | LM PPL: 1.833853E+01
Tencent01: -----------------------------------------------------------------------------------
Tencent01: [2020-11-25 12:47:03,140] [INFO] [timer.py:157:stop] 0/76100, SamplesPerSec=55.97534043849947
Tencent01: [2020-11-25 12:51:28,594] [INFO] [timer.py:157:stop] 0/76200, SamplesPerSec=55.9635894059611
Tencent01: [2020-11-25 12:55:37,026] [INFO] [timer.py:157:stop] 0/76300, SamplesPerSec=55.95732495088789
Tencent01: [2020-11-25 12:59:32,077] [INFO] [logging.py:60:log_dist] [Rank 0] step=89100, skipped=89, lr=[0.00012740733688700804, 0.00012740733688700804], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 12:59:32,129] [INFO] [timer.py:157:stop] 0/76400, SamplesPerSec=55.955340356215835
Tencent01:  iteration    89100/  320000 | elapsed time per iteration (ms): 10316.8 | learning rate 1.274E-04 | lm loss 2.936562E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2359.67 | backward: 7285.37 | optimizer: 121.80 | batch generator: 7.35 | data loader: 1.28
Tencent01: [2020-11-25 13:03:16,907] [INFO] [timer.py:157:stop] 0/76500, SamplesPerSec=55.95666160354931
Tencent01: [2020-11-25 13:06:53,109] [INFO] [timer.py:157:stop] 0/76600, SamplesPerSec=55.96072183373713
Tencent01: [2020-11-25 13:10:29,335] [INFO] [timer.py:157:stop] 0/76700, SamplesPerSec=55.96476471383967
Tencent01: [2020-11-25 13:14:22,838] [INFO] [logging.py:60:log_dist] [Rank 0] step=89200, skipped=89, lr=[0.0001273578408077905, 0.0001273578408077905], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 13:14:22,890] [INFO] [timer.py:157:stop] 0/76800, SamplesPerSec=55.96327288376292
Tencent01:  iteration    89200/  320000 | elapsed time per iteration (ms): 8907.6 | learning rate 1.274E-04 | lm loss 2.928706E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2358.42 | backward: 6427.68 | optimizer: 121.16 | batch generator: 5.95 | data loader: 1.01
Tencent01: [2020-11-25 13:18:18,015] [INFO] [timer.py:157:stop] 0/76900, SamplesPerSec=55.96129065032304
Tencent01: [2020-11-25 13:22:43,131] [INFO] [timer.py:157:stop] 0/77000, SamplesPerSec=55.949788235695145
Tencent01: [2020-11-25 13:26:48,789] [INFO] [timer.py:157:stop] 0/77100, SamplesPerSec=55.94449580090552
Tencent01: [2020-11-25 13:30:33,556] [INFO] [logging.py:60:log_dist] [Rank 0] step=89300, skipped=89, lr=[0.00012730830149330856, 0.00012730830149330856], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 13:30:33,609] [INFO] [timer.py:157:stop] 0/77200, SamplesPerSec=55.945806611394666
Tencent01:  iteration    89300/  320000 | elapsed time per iteration (ms): 9707.2 | learning rate 1.273E-04 | lm loss 2.947059E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2359.57 | backward: 7225.91 | optimizer: 121.31 | batch generator: 6.50 | data loader: 1.14
Tencent01: [2020-11-25 13:34:09,876] [INFO] [timer.py:157:stop] 0/77300, SamplesPerSec=55.949825419586716
Tencent01: [2020-11-25 13:37:46,004] [INFO] [timer.py:157:stop] 0/77400, SamplesPerSec=55.9538719343929
Tencent01: [2020-11-25 13:41:33,218] [INFO] [timer.py:157:stop] 0/77500, SamplesPerSec=55.95442362899331
Tencent01: [2020-11-25 13:45:17,651] [INFO] [logging.py:60:log_dist] [Rank 0] step=89400, skipped=89, lr=[0.0001272587189913096, 0.0001272587189913096], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 13:45:17,704] [INFO] [timer.py:157:stop] 0/77600, SamplesPerSec=55.95582193649319
Tencent01:  iteration    89400/  320000 | elapsed time per iteration (ms): 8841.0 | learning rate 1.273E-04 | lm loss 2.972472E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2359.20 | backward: 6359.84 | optimizer: 121.50 | batch generator: 6.42 | data loader: 1.17
Tencent01: [2020-11-25 13:49:59,269] [INFO] [timer.py:157:stop] 0/77700, SamplesPerSec=55.93925728872617
Tencent01: [2020-11-25 13:54:05,635] [INFO] [timer.py:157:stop] 0/77800, SamplesPerSec=55.93378856832472
Tencent01: [2020-11-25 13:57:52,256] [INFO] [timer.py:157:stop] 0/77900, SamplesPerSec=55.934533813090596
Tencent01: [2020-11-25 14:01:28,553] [INFO] [logging.py:60:log_dist] [Rank 0] step=89500, skipped=89, lr=[0.00012720909334958268, 0.00012720909334958268], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 14:01:28,605] [INFO] [timer.py:157:stop] 0/78000, SamplesPerSec=55.938500822057634
Tencent01:  iteration    89500/  320000 | elapsed time per iteration (ms): 9709.0 | learning rate 1.272E-04 | lm loss 3.000121E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2359.93 | backward: 7227.39 | optimizer: 121.33 | batch generator: 5.92 | data loader: 1.03
Tencent01: [2020-11-25 14:05:04,771] [INFO] [timer.py:157:stop] 0/78100, SamplesPerSec=55.94251658810469
Tencent01: [2020-11-25 14:08:50,699] [INFO] [timer.py:157:stop] 0/78200, SamplesPerSec=55.94346784608758
Tencent01: [2020-11-25 14:12:37,251] [INFO] [timer.py:157:stop] 0/78300, SamplesPerSec=55.94422514596906
Tencent01: [2020-11-25 14:17:00,985] [INFO] [logging.py:60:log_dist] [Rank 0] step=89600, skipped=89, lr=[0.0001271594246159584, 0.0001271594246159584], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 14:17:01,037] [INFO] [timer.py:157:stop] 0/78400, SamplesPerSec=55.933367161982055
Tencent01:  iteration    89600/  320000 | elapsed time per iteration (ms): 9324.3 | learning rate 1.272E-04 | lm loss 2.923517E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2358.54 | backward: 6844.21 | optimizer: 121.19 | batch generator: 6.20 | data loader: 1.10
Tencent01: [2020-11-25 14:21:07,137] [INFO] [timer.py:157:stop] 0/78500, SamplesPerSec=55.92804595709952
Tencent01: [2020-11-25 14:24:54,087] [INFO] [timer.py:157:stop] 0/78600, SamplesPerSec=55.92868876209646
Tencent01: [2020-11-25 14:28:40,364] [INFO] [timer.py:157:stop] 0/78700, SamplesPerSec=55.929551140203706
Tencent01: [2020-11-25 14:32:16,492] [INFO] [logging.py:60:log_dist] [Rank 0] step=89700, skipped=89, lr=[0.00012710971283830887, 0.00012710971283830887], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 14:32:16,545] [INFO] [timer.py:157:stop] 0/78800, SamplesPerSec=55.93354010059846
Tencent01:  iteration    89700/  320000 | elapsed time per iteration (ms): 9155.1 | learning rate 1.271E-04 | lm loss 2.899451E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2360.28 | backward: 6672.72 | optimizer: 121.68 | batch generator: 6.31 | data loader: 1.16
Tencent01: [2020-11-25 14:36:00,635] [INFO] [timer.py:157:stop] 0/78900, SamplesPerSec=55.93506561694886
Tencent01: [2020-11-25 14:39:46,810] [INFO] [timer.py:157:stop] 0/79000, SamplesPerSec=55.935943955079466
Tencent01: [2020-11-25 14:44:01,390] [INFO] [timer.py:157:stop] 0/79100, SamplesPerSec=55.928041296257305
Tencent01: [2020-11-25 14:48:06,112] [INFO] [logging.py:60:log_dist] [Rank 0] step=89800, skipped=89, lr=[0.00012705995806454776, 0.00012705995806454776], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 14:48:06,165] [INFO] [timer.py:157:stop] 0/79200, SamplesPerSec=55.92318096815478
Tencent01:  iteration    89800/  320000 | elapsed time per iteration (ms): 9496.2 | learning rate 1.271E-04 | lm loss 2.943244E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2359.40 | backward: 7015.38 | optimizer: 121.06 | batch generator: 6.22 | data loader: 1.04
Tencent01: [2020-11-25 14:48:41,010] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 14:48:41,010] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 14:48:41,011] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 14:48:41,011] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 14:48:41,011] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 14:48:41,011] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 14:48:41,011] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 14:48:41,011] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 14:48:41,011] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 14:48:41,011] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 14:48:41,011] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 14:48:41,012] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 14:48:41,011] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 14:48:41,012] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 14:48:41,012] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent01: [2020-11-25 14:48:41,012] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 14:48:41,011] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 14:48:41,011] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 14:48:41,011] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 14:48:41,012] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 14:48:41,011] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 14:48:41,011] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 14:48:41,011] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 14:48:41,013] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 14:48:41,013] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 14:48:41,013] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 14:48:41,013] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 14:48:41,013] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 14:48:41,013] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 14:48:41,013] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent02: [2020-11-25 14:48:41,014] [INFO] [fused_optimizer.py:325:_update_scale] No Grad overflow for 1000 iterations
Tencent02: [2020-11-25 14:48:41,015] [INFO] [fused_optimizer.py:327:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
Tencent01: [2020-11-25 14:49:25,799] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 89806
Tencent01: [2020-11-25 14:49:25,799] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 14:49:25,799] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
Tencent01: [2020-11-25 14:49:25,800] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 89806
Tencent01: [2020-11-25 14:49:25,800] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 14:49:25,800] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 89806
Tencent01: [2020-11-25 14:49:25,800] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 14:49:25,800] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 89806
Tencent02: [2020-11-25 14:49:25,800] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 89806
Tencent02: [2020-11-25 14:49:25,800] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 89806
Tencent02: [2020-11-25 14:49:25,800] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 14:49:25,800] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 14:49:25,800] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 14:49:25,800] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 89806
Tencent01: [2020-11-25 14:49:25,800] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 14:49:25,800] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 89806
Tencent01: [2020-11-25 14:49:25,800] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 14:49:25,800] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 89806
Tencent02: [2020-11-25 14:49:25,800] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 89806
Tencent02: [2020-11-25 14:49:25,800] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 14:49:25,800] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 14:49:25,800] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 89806
Tencent02: [2020-11-25 14:49:25,801] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 14:49:25,801] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 89806
Tencent02: [2020-11-25 14:49:25,801] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent02: [2020-11-25 14:49:25,801] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent02: Grad overflow on iteration 89806
Tencent02: [2020-11-25 14:49:25,801] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 14:49:25,801] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 89806
Tencent01: [2020-11-25 14:49:25,801] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 14:49:25,801] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 89806
Tencent01: [2020-11-25 14:49:25,801] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 14:49:25,802] [INFO] [fused_optimizer.py:314:_update_scale] 
Tencent01: Grad overflow on iteration 89806
Tencent01: [2020-11-25 14:49:25,802] [INFO] [fused_optimizer.py:316:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
Tencent01: [2020-11-25 14:52:01,739] [INFO] [timer.py:157:stop] 0/79300, SamplesPerSec=55.92117631172014
Tencent01: [2020-11-25 14:55:47,297] [INFO] [timer.py:157:stop] 0/79400, SamplesPerSec=55.92226003433775
Tencent01: [2020-11-25 14:59:23,548] [INFO] [timer.py:157:stop] 0/79500, SamplesPerSec=55.92620444058722
Tencent01: [2020-11-25 15:03:10,708] [INFO] [logging.py:60:log_dist] [Rank 0] step=89900, skipped=90, lr=[0.0001270106585322851, 0.0001270106585322851], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 15:03:10,759] [INFO] [timer.py:157:stop] 0/79600, SamplesPerSec=55.926778000196265
Tencent01:  iteration    89900/  320000 | elapsed time per iteration (ms): 9045.9 | learning rate 1.270E-04 | lm loss 2.891248E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2360.95 | backward: 6564.29 | optimizer: 120.31 | batch generator: 7.11 | data loader: 1.23
Tencent01: [2020-11-25 15:06:57,599] [INFO] [timer.py:157:stop] 0/79700, SamplesPerSec=55.92745450671753
Tencent01: [2020-11-25 15:10:52,666] [INFO] [timer.py:157:stop] 0/79800, SamplesPerSec=55.92560314197657
Tencent01: [2020-11-25 15:15:23,966] [INFO] [timer.py:157:stop] 0/79900, SamplesPerSec=55.91269216219281
Tencent01: [2020-11-25 15:19:20,008] [INFO] [logging.py:60:log_dist] [Rank 0] step=90000, skipped=90, lr=[0.00012696081833897123, 0.00012696081833897123], mom=[(0.9, 0.999), (0.9, 0.999)]
Tencent01: [2020-11-25 15:19:20,061] [INFO] [timer.py:157:stop] 0/80000, SamplesPerSec=55.91055403278457
Tencent01:  iteration    90000/  320000 | elapsed time per iteration (ms): 9693.0 | learning rate 1.270E-04 | lm loss 2.949759E+00 | loss scale 262144.0 |
Tencent01: time (ms) | forward: 2360.49 | backward: 7210.19 | optimizer: 121.95 | batch generator: 6.32 | data loader: 1.13
Tencent01: [2020-11-25 15:19:20,065] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: /root/data/checkpoints/gpt-345M11-14-13-21/90000/mp_rank_00_model_states.pt
Tencent01: Traceback (most recent call last):
Tencent01:   File "pretrain_gpt2.py", line 802, in <module>
Tencent01:     main()
Tencent01:   File "pretrain_gpt2.py", line 779, in main
Tencent01:     timers, args, summary_writer=summary_writer)
Tencent01:   File "pretrain_gpt2.py", line 533, in train
Tencent01:     save_checkpoint(args.iteration, model, optimizer, lr_scheduler, args)
Tencent01:   File "/root/code/Megatron-LM/utils.py", line 194, in save_checkpoint
Tencent01:     save_ds_checkpoint(iteration, model, lr_scheduler, args)
Tencent01:   File "/root/code/Megatron-LM/utils.py", line 254, in save_ds_checkpoint
Tencent01:     model.save_checkpoint(args.save, iteration, client_state=sd)
Tencent01:   File "/opt/conda/lib/python3.7/site-packages/deepspeed/runtime/engine.py", line 1347, in save_checkpoint
Tencent01:     self._save_checkpoint(save_dir, tag, client_state=client_state)
Tencent01:   File "/opt/conda/lib/python3.7/site-packages/deepspeed/runtime/engine.py", line 1413, in _save_checkpoint
Tencent01:     torch.save(state, save_path)
Tencent01:   File "/opt/conda/lib/python3.7/site-packages/torch/serialization.py", line 364, in save
Tencent01:     _save(obj, opened_zipfile, pickle_module, pickle_protocol)
Tencent01:   File "/opt/conda/lib/python3.7/site-packages/torch/serialization.py", line 481, in _save
Tencent01:     storage._write_file(buf, _should_read_directly(buf))
Tencent01:   File "/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
Tencent01:     _error_if_any_worker_fails()
Tencent01: RuntimeError: DataLoader worker (pid 599) is killed by signal: Killed. 
